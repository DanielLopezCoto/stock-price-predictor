{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: BT CEO hails telecom networks as 'UK's real digital backbone'\n",
      "Enlace: https://uk.finance.yahoo.com/news/bt-ceo-london-tech-week-uk-infrastructure-050059140.html?.tsrc=rss\n",
      "Resumen: Allison Kirkby, CEO of BT, highlighted how the company is playing a key role in developing the infrastructure that acts as the 'UK's real digital backbone'.\n",
      "------\n",
      "Título: Super Micro Computer (SMCI) Debuts the Industry’s Broadest AI Solution Stack for NVIDIA Blackwell\n",
      "Enlace: https://finance.yahoo.com/news/super-micro-computer-smci-debuts-031505879.html?.tsrc=rss\n",
      "Resumen: Super Micro Computer, Inc. (NASDAQ:SMCI) is one of the 15 AI Stocks Making Waves on Wall Street. On June 11, the company announced an expansion of the industry’s broadest portfolio of solutions designed for NVIDIA Blackwell Architecture to the European market. The 30+ solutions offer the most comprehensive and efficient solution stack for NVIDIA HGX B200, GB200 […]\n",
      "------\n",
      "Título: CrowdStrike (CRWD) Boosts AI Security With NVIDIA—Here’s What It Means\n",
      "Enlace: https://finance.yahoo.com/news/crowdstrike-crwd-boosts-ai-security-031336515.html?.tsrc=rss\n",
      "Resumen: CrowdStrike Holdings, Inc. (NASDAQ:CRWD) is one of the 15 AI Stocks Making Waves on Wall Street. On June 11, the company announced that it will be integrating Falcon® Cloud Security with NVIDIA universal LLM NIM microservices and NeMo Safety in order to provide full lifecycle protection for AI and over 100,000 large language models (LLMs) in collaboration […]\n",
      "------\n",
      "Título: Europe is Getting Its First Industrial AI Cloud—Thanks to NVIDIA (NVDA)\n",
      "Enlace: https://finance.yahoo.com/news/europe-getting-first-industrial-ai-031117437.html?.tsrc=rss\n",
      "Resumen: NVIDIA Corporation (NASDAQ:NVDA) is one of the 15 AI Stocks Making Waves on Wall Street. On June 11, the company announced that it is building the world’s first industrial AI cloud for European manufacturers at the NVIDIA GTC Paris. Featuring 10,000 GPUs, the AI factory will empower Europe’s industrial leaders to accelerate every manufacturing application, from design, […]\n",
      "------\n",
      "Título: Chinese AI Companies Dodge U.S. Chip Curbs by Flying Suitcases of Hard Drives Abroad\n",
      "Enlace: https://finance.yahoo.com/m/6bcc81ad-abbb-342b-8cad-f3c3d98b8e44/chinese-ai-companies-dodge.html?.tsrc=rss\n",
      "Resumen: Engineers carried data to countries where Nvidia chips are available, frustrating Washington’s efforts to restrict China’s access to the technology.\n",
      "------\n",
      "Título: Are Robots The Answer To China's Aged Care Crisis? Beijing Launches Massive Trial To Test If Machines Can Cook, Clean And Console The Elderly\n",
      "Enlace: https://finance.yahoo.com/news/robots-answer-chinas-aged-care-213004889.html?.tsrc=rss\n",
      "Resumen: Are robots the solution to China's growing aged care problem? The Chinese Ministry of Industry and Information Technology has announced a large-scale trial program to test whether intelligent machines can provide not just physical assistance, but also emotional support for its expanding senior population. What Happened: The ministry has invited private companies with proven track records in elder care to apply for the initiative. Firms must have robust environmental, credit, and safety records t\n",
      "------\n",
      "Título: AMD Says New Chips Can Top Nvidia’s in Booming AI Chip Field\n",
      "Enlace: https://finance.yahoo.com/news/amd-says-chips-top-nvidia-212923249.html?.tsrc=rss\n",
      "Resumen: (Bloomberg) -- Advanced Micro Devices Inc. Chief Executive Officer Lisa Su said her company’s latest AI processors can challenge Nvidia Corp. chips in a market she now expects to soar past $500 billion in the next three years. Most Read from BloombergShuttered NY College Has Alumni Fighting Over Its FutureTrump’s Military Parade Has Washington Bracing for Tanks and WeaponryNYC Renters Brace for Price Hikes After Broker-Fee BanDo World’s Fairs Still Matter?NY Long Island Rail Service Resumes Afte\n",
      "------\n",
      "Título: AMD Bolsters AI Data Center Pitch With Full-Rack Systems\n",
      "Enlace: https://finance.yahoo.com/m/740cc1fc-708b-3743-869a-0a17aab1bd12/amd-bolsters-ai-data-center.html?.tsrc=rss\n",
      "Resumen: Advanced Micro Devices on Thursday announced full-rack AI data center systems to better compete with Nvidia. But AMD stock fell.\n",
      "------\n",
      "Título: AMD gains on Nvidia? Lisa Su reveals new chips in heated AI inference race\n",
      "Enlace: https://finance.yahoo.com/news/amd-gains-nvidia-lisa-su-201435056.html?.tsrc=rss\n",
      "Resumen: Investing.com -- Advanced Micro Devices Inc (NASDAQ:AMD) made an aggressive bid for dominance in AI inference at its Advancing AI event Thursday, unveiling new chips that directly challenge NVIDIA Corporation’s (NASDAQ:NVDA) supremacy in the data center GPU market. AMD claims its latest Instinct MI355X accelerators surpass Nvidia’s most advanced Blackwell GPUs in inference performance while offering a significant cost advantage, a critical selling point as hyperscalers look to scale generative A\n",
      "------\n",
      "Título: Here's Why Shares in Navitas Powered Higher Again This Week\n",
      "Enlace: https://www.fool.com/investing/2025/06/12/heres-why-shares-in-navitas-powered-higher-again-t/?.tsrc=rss\n",
      "Resumen: A recent presentation by Navitas' management shed some light on the potential of its collobaration with Nvidia.  The next generation of data centers is expected to drive significant growth for many technology companies, including Navitas.  The move comes as investors continue to digest the news that Nvidia is collaborating with Navitas to develop data center architecture for the next generation of data centers, due to launch in 2027.\n",
      "------\n",
      "Título: AMD Introduces AI-Focused MI350 Series, Part of ‘Vision for an Open AI’ Ecosystem\n",
      "Enlace: https://finance.yahoo.com/m/ca9d8622-50ac-360a-94a0-8b776ac938d7/amd-introduces-ai-focused.html?.tsrc=rss\n",
      "Resumen: The chip maker said it partnered with prominent companies such as Meta Platforms, OpenAI, Oracle and Microsoft as it seeks an open AI ecosystem.\n",
      "------\n",
      "Título: America's Chip Crackdown Just Handed China a $8B AI Opportunity\n",
      "Enlace: https://finance.yahoo.com/news/americas-chip-crackdown-just-handed-191818431.html?.tsrc=rss\n",
      "Resumen: Arm and Nvidia warn export bans are fueling China's tech ascent and shrinking U.S. market share\n",
      "------\n",
      "Título: Why Buy QBTS & IONQ Stocks After NVIDIA CEO's Remarks?\n",
      "Enlace: https://finance.yahoo.com/news/why-buy-qbts-ionq-stocks-190000864.html?.tsrc=rss\n",
      "Resumen: NVDA CEO Jensen Huang's upbeat take on quantum computing sparks investor interest in QBTS and IONQ stocks.\n",
      "------\n",
      "Título: Exclusive-'Neocloud' Crusoe to buy $400 million worth of AMD chips for AI data centers\n",
      "Enlace: https://ca.finance.yahoo.com/news/exclusive-neocloud-crusoe-buy-400-183908177.html?.tsrc=rss\n",
      "Resumen: SAN FRANCISCO (Reuters) -Crusoe CEO Chase Lochmiller said on Thursday the artificial intelligence data center builder planned to purchase roughly $400 million worth of AI chips from Advanced Micro Devices to add to its portfolio of computing power.  The cloud computing startup is aiming to build a data center - or cluster - to house the AMD AI chips in the U.S. and will rent them to customers for building AI models and running applications.  Crusoe's data centers are purpose-built to house AI chips, which it says allows it to offer superior performance compared with older designs.\n",
      "------\n",
      "Título: Nvidia to exclude China from forecasts amid US chip export curbs, CNN reports\n",
      "Enlace: https://finance.yahoo.com/news/nvidia-stop-including-china-forecasts-183120979.html?.tsrc=rss\n",
      "Resumen: (Reuters) -Nvidia  will no longer include the China market in its revenue and profit forecasts following stringent U.S. trade restrictions on chip sales to the region, CEO Jensen Huang told CNN on Thursday.  When asked if the United States would lift the export controls after the trade discussions with China in London this week, Huang said he was not counting on it.  Huang reiterated his criticism of U.S. chip export curbs during his CNN interview, building on his earlier remarks about the restrictions from April that prevented Nvidia from selling its H20 chip made for China.\n",
      "------\n",
      "Título: AMD launches MI350 AI chip line to rival Nvidia's Blackwell processors, debuts AI cloud service\n",
      "Enlace: https://finance.yahoo.com/news/amd-launches-mi350-ai-chip-line-to-rival-nvidias-blackwell-processors-debuts-ai-cloud-service-183046317.html?.tsrc=rss\n",
      "Resumen: AMD launched its MI350 line of AI chips on Thursday.\n",
      "------\n",
      "Título: AMD Unveils New AI Chips to Take On Nvidia\n",
      "Enlace: https://finance.yahoo.com/m/777cf37c-11b0-3a9b-9d26-5a646d5f2a42/amd-unveils-new-ai-chips-to.html?.tsrc=rss\n",
      "Resumen: FEATURE   Advanced Micro Devices is upping its game in the artificial-intelligence infrastructure space. On Thursday, at AMD’s Advancing AI event in San Jose, Calif., the company announced its latest AI chips to compete with Nvidia the market leader: the AMD Instinct MI350 series consisting of the MI350X and MI355X.\n",
      "------\n",
      "Título: Nvidia will stop including China in its forecasts amid US chip export controls, CEO says\n",
      "Enlace: https://www.cnn.com/2025/06/12/tech/nvidia-ceo-china-us-ai-chip-exports?.tsrc=rss\n",
      "Resumen: Chipmaker Nvidia will exclude the Chinese market from its revenue and profit forecasts following the imposition of tough US restrictions on chip sales to China, its CEO said Thursday.\n",
      "------\n",
      "Título: Find The Next Palantir, Nvidia Or Hot IPO. Here's How.\n",
      "Enlace: https://finance.yahoo.com/m/3ea1184c-5a73-3eb6-9e62-aab9dda34953/find-the-next-palantir%2C.html?.tsrc=rss\n",
      "Resumen: Use the IBD Stock Screener to quickly filter through thousands of companies to find stocks that meet your criteria.\n",
      "------\n",
      "Título: AMD unveils AI server as OpenAI taps its newest chips\n",
      "Enlace: https://finance.yahoo.com/news/amd-ceo-unveils-ai-chips-174705568.html?.tsrc=rss\n",
      "Resumen: SAN JOSE (Reuters) -Advanced Micro Devices CEO Lisa Su on Thursday unveiled a new artificial intelligence server for 2026 that aims to challenge Nvidia's flagship offerings as OpenAI's CEO said the ChatGPT creator would adopt AMD's latest chips.  The MI400 series of chips will be the basis of a new server called \"Helios\" that AMD plans to release next year.  The move comes as the competition between Nvidia and other AI chip firms has shifted away from selling individual chips to selling servers packed with scores or even hundreds of processors, woven together with networking chips from the same company.\n",
      "------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': \"Allison Kirkby, CEO of BT, highlighted how the company is playing a key role in developing the infrastructure that acts as the 'UK's real digital backbone'.\",\n",
       " 'summary_detail': {'type': 'text/html',\n",
       "  'language': None,\n",
       "  'base': 'https://feeds.finance.yahoo.com/rss/2.0/headline?s=NVDA&region=US&lang=en-US',\n",
       "  'value': \"Allison Kirkby, CEO of BT, highlighted how the company is playing a key role in developing the infrastructure that acts as the 'UK's real digital backbone'.\"},\n",
       " 'id': '0cdfdba7-aae7-4286-b236-5923a87c4ad5',\n",
       " 'guidislink': False,\n",
       " 'links': [{'rel': 'alternate',\n",
       "   'type': 'text/html',\n",
       "   'href': 'https://uk.finance.yahoo.com/news/bt-ceo-london-tech-week-uk-infrastructure-050059140.html?.tsrc=rss'}],\n",
       " 'link': 'https://uk.finance.yahoo.com/news/bt-ceo-london-tech-week-uk-infrastructure-050059140.html?.tsrc=rss',\n",
       " 'published': 'Fri, 13 Jun 2025 05:00:59 +0000',\n",
       " 'published_parsed': time.struct_time(tm_year=2025, tm_mon=6, tm_mday=13, tm_hour=5, tm_min=0, tm_sec=59, tm_wday=4, tm_yday=164, tm_isdst=0),\n",
       " 'title': \"BT CEO hails telecom networks as 'UK's real digital backbone'\",\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': 'https://feeds.finance.yahoo.com/rss/2.0/headline?s=NVDA&region=US&lang=en-US',\n",
       "  'value': \"BT CEO hails telecom networks as 'UK's real digital backbone'\"}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "# URL del feed de Yahoo Finance para NVIDIA\n",
    "url = \"https://feeds.finance.yahoo.com/rss/2.0/headline?s=NVDA&region=US&lang=en-US\"\n",
    "\n",
    "# Leer el feed\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "# Mostrar los títulos\n",
    "for entry in feed.entries:\n",
    "    print(\"Título:\", entry.title)\n",
    "    print(\"Enlace:\", entry.link)\n",
    "    print(\"Resumen:\", entry.summary)\n",
    "    print(\"------\")\n",
    "\n",
    "feed.keys()\n",
    "feed.entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dd345834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aadea1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Año mínimo en el que hay noticias es 2009\n",
    "# Las categorías disponibles son \"press_release\" y \"blog_post\"\n",
    "min_year = 2009\n",
    "categories = {\n",
    "    \"press_release\": 21926,\n",
    "    \"blog_post\": 21924}\n",
    "\n",
    "\n",
    "year = 2023\n",
    "category = categories[\"press_release\"]\n",
    "\n",
    "BASE_URL = \"https://nvidianews.nvidia.com\"\n",
    "HEADERS={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "url = BASE_URL+\"/news?q=&year={year}&c={category}\".format(\n",
    "    year=year, category=category, page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ae0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de páginas: ['1', '2', '3', '4', '5', '6']\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    pages = get_page_numbers(soup)\n",
    "    n_pages = len(pages)\n",
    "    print(\"Número de páginas:\", pages)\n",
    "    \n",
    "    for page in pages:\n",
    "        print(f\"Obteniendo página: {page} de {n_pages}\")\n",
    "        page_url = f\"{url}&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(page_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                articles = soup.find_all(\"article\")\n",
    "                n_articles = len(articles)\n",
    "\n",
    "                for id, article in enumerate(articles, start=1):\n",
    "                    print(f\"Obteniendo artículo {id} de {n_articles}\")\n",
    "                    c_article = get_article_info(article)\n",
    "                    \n",
    "                    response = requests.get(c_article[\"link\"], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    soup_article = BeautifulSoup(response.content, \"html.parser\")\n",
    "                    \n",
    "                    c_article[\"content\"] = get_content_page(soup_article)\n",
    "                    lista.append(c_article)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error al obtener la página {page}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# result = requests.get(\"https://finance.yahoo.com/quote/NVDA?p=NVDA&.tsrc=fin-srch\", headers={\"User-Agent\": \"Mozilla/5.0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9d11f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'NVIDIA Brings Business Intelligence to Chatbots, Copilots and Summarization Tools With Enterprise-Grade Generative AI Microservice',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice',\n",
       "  'publish_date': '2023-11-28',\n",
       "  'summary': 'AWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.',\n",
       "  'content': 'AWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.\\nNVIDIA NeMo™ Retriever— a new offering in theNVIDIA NeMofamily of frameworks and tools for building, customizing and deploying generative AI models — helps organizations enhance their generative AI applications with enterprise-graderetrieval-augmented generation(RAG) capabilities.\\nAs a semantic-retrieval microservice, NeMo Retriever helps generative AI applications provide more accurate responses through NVIDIA-optimized algorithms. Developers using the microservice can connect their AI applications to business data wherever it resides across clouds and data centers. It adds NVIDIA-optimized RAG capabilities toAI foundriesand is part of theNVIDIA AI Enterprisesoftware platform, available inAWS Marketplace.\\nCadence, Dropbox, SAP and ServiceNow are among the pioneers working with NVIDIA to build production-ready RAG capabilities into their custom generative AI applications and services.\\n“Generative AI applications with RAG capabilities are the next killer app of the enterprise,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NeMo Retriever, developers can create customized generative AI chatbots, copilots and summarization tools that can access their business data to transform productivity with accurate and valuable generative AI intelligence.”\\nGlobal Leaders Enhance LLM Accuracy With NeMo RetrieverElectronic systems design leader Cadence serves companies across hyperscale computing, 5G communications, automotive, mobile, aerospace, consumer and healthcare markets. It is working with NVIDIA to develop RAG features for generative AI applications in industrial electronics design.\\n“Generative AI introduces innovative approaches to address customer needs, such as tools to uncover potential flaws early in the design process,” said Anirudh Devgan, president and CEO of Cadence. “Our researchers are working with NVIDIA to use NeMo Retriever to further boost the accuracy and relevance of generative AI applications to reveal issues and help customers get high-quality products to market faster.”\\nCracking the Code for Accurate Generative AI ApplicationsUnlike open-source RAG toolkits, NeMo Retriever supports production-ready generative AI with commercially viable models, API stability, security patches and enterprise support.\\nNVIDIA-optimized algorithms power the highest accuracy results in Retriever’s embedding models. The optimized embedding models capture relationships between words, enabling LLMs to process and analyze textual data.\\nUsing NeMo Retriever, enterprises can connect their LLMs to multiple data sources and knowledge bases, so that users can easily interact with data and receive accurate, up-to-date answers using simple, conversational prompts. Businesses using Retriever-powered applications can allow users to securely gain access to information spanning numerous data modalities, such as text, PDFs, images and videos.\\nEnterprises can use NeMo Retriever to achieve more accurate results with less training, speeding time to market and supporting energy efficiency in the development of generative AI applications.\\nReliable, Simple, Secure Deployment With NVIDIA AI EnterpriseCompanies can deploy NeMo Retriever-powered applications to run during inference on NVIDIA-accelerated computing on virtually any data center or cloud. NVIDIA AI Enterprise supports accelerated, high-performance inference with NVIDIA NeMo,NVIDIA Triton Inference Server™,NVIDIA TensorRT™,NVIDIA TensorRT-LLMand otherNVIDIA AIsoftware.\\nTo maximize inference performance, developers can run their models onNVIDIA GH200 Grace Hopper Superchips with TensorRT-LLM software.\\nAvailabilityDevelopers can sign up forearly access to NVIDIA NeMo Retriever.'},\n",
       " {'title': 'NVIDIA Announces Upcoming Events for Financial Community',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-upcoming-events-for-financial-community-6897247',\n",
       "  'publish_date': '2023-11-22',\n",
       "  'summary': 'SANTA CLARA, Calif., Nov. 22, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following events for the financial community: UBS Global Technology ConferenceTuesday, Nov. 28, 11:35 a.m. ...',\n",
       "  'content': 'SANTA CLARA, Calif., Nov.  22, 2023  (GLOBE NEWSWIRE) -- NVIDIA will present at the following events for the financial community:\\nUBS Global Technology ConferenceTuesday, Nov. 28, 11:35 a.m. Pacific time\\nWells Fargo 7thAnnual TMT SummitWednesday, Nov. 29, 8 a.m. Pacific time\\nArete Technology ConferenceThursday, Dec. 7, 10:45 a.m. Pacific time\\nJ.P. Morgan Healthcare ConferenceMonday, Jan. 8, 2024, 10:30 a.m. Pacific time\\nInterested parties can listen to the live audio webcast of NVIDIA presentations at financial events atinvestor.nvidia.com. Replays of the webcasts will be available for 90 days afterward.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling\\xa0industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.\\nFor further information, contact:\\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries.'},\n",
       " {'title': 'NVIDIA Announces Financial Results for Third Quarter Fiscal 2024',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-third-quarter-fiscal-2024',\n",
       "  'publish_date': '2023-11-21',\n",
       "  'summary': 'NVIDIA (NASDAQ: NVDA) today reported revenue for the third quarter ended October 29, 2023, of $18.12 billion, up 206% from a year ago and up 34% from the previous quarter.',\n",
       "  'content': 'NVIDIA (NASDAQ: NVDA) today reported revenue for the third quarter ended October 29, 2023, of $18.12 billion, up 206% from a year ago and up 34% from the previous quarter.\\nGAAP earnings per diluted share for the quarter were $3.71, up more than 12x from a year ago and up 50% from the previous quarter. Non-GAAP earnings per diluted share were $4.02, up nearly 6x from a year ago and up 49% from the previous quarter.\\n“Our strong growth reflects the broad industry platform transition from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\\n“Large language model startups, consumer internet companies and global cloud service providers were the first movers, and the next waves are starting to build. Nations and regional CSPs are investing in AI clouds to serve local demand, enterprise software companies are adding AI copilots and assistants to their platforms, and enterprises are creating custom AI to automate the world’s largest industries.\\n“NVIDIA GPUs, CPUs, networking, AI foundry services and NVIDIA AI Enterprise software are all growth engines in full throttle. The era of generative AI is taking off,” he said.\\nNVIDIA will pay its next quarterly cash dividend of $0.04 per share on December 28, 2023, to all shareholders of record on December 6, 2023.\\nQ3 Fiscal 2024 Summary\\n\\nOutlookNVIDIA’s outlook for the fourth quarter of fiscal 2024 is as follows:\\nHighlights\\nNVIDIA achieved progress since its previous earnings announcement in these areas:\\nData Center\\nGaming\\nProfessional Visualization\\nAutomotive\\nCFO CommentaryCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available athttps://investor.nvidia.com.\\nConference Call and Webcast InformationNVIDIA will conduct a conference call with analysts and investors to discuss its third quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website,https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its fourth quarter and fiscal 2024.\\nNon-GAAP MeasuresTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'title': 'NVIDIA’s New Ethernet Networking Platform for AI Available Soon From Dell Technologies, Hewlett Packard Enterprise, Lenovo',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidias-new-ethernet-networking-platform-for-ai-available-soon-from-dell-technologies-hewlett-packard-enterprise-lenovo',\n",
       "  'publish_date': '2023-11-20',\n",
       "  'summary': 'NVIDIA today announced that Dell Technologies, Hewlett Packard Enterprise and Lenovo will be the first to integrate NVIDIA Spectrum-X™ Ethernet networking technologies for AI into their server lineups to help enterprise customers speed up generative ...',\n",
       "  'content': 'NVIDIA today announced thatDell Technologies, Hewlett Packard Enterprise andLenovowill be the first to integrateNVIDIA Spectrum-X™Ethernetnetworking technologiesfor AI into their server lineups to help enterprise customers speed up generative AI workloads.\\nPurpose-built for generative AI, Spectrum-X offers enterprises a new class of Ethernet networking that can achieve 1.6x higher networking performance for AI communication versus traditional Ethernet offerings.\\nThe new systems coming from three of the top system makers bring together Spectrum-X with NVIDIA Tensor Core GPUs,NVIDIA AI Enterprisesoftware andNVIDIA AI Workbenchsoftware to provide enterprises the building blocks to transform their businesses with generative AI.\\n“Generative AI and accelerated computing are driving a generational transition as enterprises upgrade their data centers to serve these workloads,” said Jensen Huang, founder and CEO of NVIDIA. “Accelerated networking is the catalyst for a new wave of systems from NVIDIA’s leading server manufacturer partners to speed the shift to the era of generative AI.”\\n“Accelerated computing and networking are key to building systems to meet the demands of large language models and generative AI applications,” said Michael Dell, chairman and CEO of Dell Technologies. “Through our collaboration, Dell Technologies and NVIDIA are providing customers with the infrastructure and software needed to quickly and securely extract intelligence from their data.”\\n“Generative AI will undoubtedly drive innovation across multiple industries,” said Antonio Neri, president and CEO of HPE. “These powerful new applications will require a fundamentally different architecture to support a variety of dynamic workloads. To enable customers to realize the full potential of generative AI, HPE is partnering with NVIDIA to build systems with the required power, efficiency and scalability to support these applications.”\\n“Generative AI can power unprecedented transformation but places unprecedented demands on enterprise infrastructure,” said Yuanqing Yang, chairman and CEO of Lenovo. “Working closely with NVIDIA, Lenovo is building efficient, accelerated systems with the networking, computing and software needed to power modern AI applications.”\\nNetworking Purpose-Built to Accelerate AIFor peak AI workload efficiency, Spectrum-X combines the extreme performance of theSpectrum-4Ethernet switch; theNVIDIA BlueField®-3 SuperNIC, a new class of network accelerators for supercharging hyperscale AI workloads; as well asacceleration software. Spectrum-X complements BlueField-3 DPUs, the world’s most advanced infrastructure computing platform.\\nSpectrum-4 is the world’s first 51Tb/sec Ethernet switch for AI, providing highly effective data throughput at scale and under load while minimizing network congestion for multi-tenant, AI cloud workloads. Its intelligent, fine-tuned routing technology enables maximum utilization of network infrastructure at all times.\\nBlueField-3 SuperNICs are designed for network-intensive, massively parallel computing, offering up to 400Gb/s RDMA over Converged Ethernet (RoCE) network connectivity between GPU servers and boosting performance for AI training and inference traffic on the east-west network inside the cluster. They also enable secure, multi-tenant data center environments, ensuring deterministic and isolated performance between tenant jobs. Boasting a power-efficient, half-height, half-length PCIe form factor, BlueField-3 SuperNICs are ideal for enterprise-class servers.\\nAcceleration software powering Spectrum-X features NVIDIA software development kits such asCumulus Linux, PureSONiCandNetQ— which together drive the platform’s breakthrough performance — and theNVIDIA DOCA™ software framework, which is at the heart of BlueField.\\nNVIDIA AI Enterprise provides frameworks, pretrained models and development tools for secure, stable and supported production AI. NVIDIA AI Workbench allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation — then scale them to virtually any data center or cloud.\\nNVIDIA Israel-1 Supercomputer Powered by Spectrum-XSpectrum-X also enables the NVIDIA Israel-1 supercomputer, a reference architecture for next-generation AI systems. Israel-1 is a collaboration with Dell Technologies, using Dell PowerEdge XE9680 servers powered by the NVIDIA HGX™ H100 eight-GPU platform and BlueField-3 DPUs and SuperNICs with Spectrum-4 switches.\\nAvailabilityNew systems from Dell, HPE and Lenovo featuring the complete NVIDIA AI stack are expected in the first quarter of next year.'},\n",
       " {'title': 'NVIDIA Introduces Generative AI Foundry Service on Microsoft Azure for Enterprises and Startups Worldwide',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-introduces-generative-ai-foundry-service-on-microsoft-azure-for-enterprises-and-startups-worldwide',\n",
       "  'publish_date': '2023-11-15',\n",
       "  'summary': 'NVIDIA today introduced an AI foundry service to supercharge the development and tuning of custom generative AI applications for enterprises and startups deploying on Microsoft Azure.',\n",
       "  'content': \"Microsoft Ignite—NVIDIA today introduced an AI foundry service to supercharge the development and tuning of custom generative AI applications for enterprises and startups deploying on Microsoft Azure.\\nThe NVIDIA AI foundry service pulls together three elements — a collection ofNVIDIA AI Foundation Models,NVIDIA NeMo™ framework and tools, andNVIDIA DGX™ CloudAI supercomputing services — that give enterprises an end-to-end solution for creating custom generative AI models. Businesses can then deploy their customized models withNVIDIA AI Enterprisesoftware to power generative AI applications, including intelligent search, summarization and content generation.\\nIndustry leaders SAP SE, Amdocs and Getty Images are among the pioneers building custom models using the service.\\n“Enterprises need custom models to perform specialized skills trained on the proprietary DNA of their company — their data,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA’s AI foundry service combines our generative AI model technologies, LLM training expertise and giant-scale AI factory. We built this in Microsoft Azure so enterprises worldwide can connect their custom model with Microsoft’s world-leading cloud services.”\\n“Our partnership with NVIDIA spans every layer of the Copilot stack — from silicon to software — as we innovate together for this new age of AI,” said Satya Nadella, chairman and CEO of Microsoft. “With NVIDIA’s generative AI foundry service on Microsoft Azure, we’re providing new capabilities for enterprises and startups to build and deploy AI applications on our cloud.”\\nIndustry Leaders Building Tailored, Timely LLMsNVIDIA’s AI foundry service can be used to customize models for generative AI-powered applications across industries, including enterprise software, telecommunications and media. Once ready to deploy, enterprises can use a technique calledretrieval-augmented generation(RAG) to connect their models with their enterprise data and access new insights.\\nAs the first customer of NVIDIA DGX Cloud on Microsoft Azure, SAP plans to use the service and optimized RAG workflow withNVIDIA DGX CloudandNVIDIA AI Enterprisesoftware running on Azure to help customize and deploy Joule®, its new natural language generative AI copilot.\\n“Joule draws on SAP’s unique position at the nexus of business and technology, and builds on our relevant, reliable and responsible approach to Business AI,” said Christian Klein, CEO and member of the Executive Board of SAP SE. “In partnership with NVIDIA, Joule can help customers unlock the potential of generative AI for their business by automating time-consuming tasks and quickly analyzing data to deliver more intelligent, personalized experiences.”\\nAmdocs, a leading provider of software and services to communications and media companies, is optimizing models for the Amdocs amAIz framework to speed adoption of generative AI applications and services for telcos globally.\\n“Generative AI technology presents an incredible opportunity for service providers to reinvent the way they engage with customers,” said Shuky Sheffer, president and CEO at Amdocs. “Leveraging NVIDIA’s and Microsoft's technology to power the Amdocs amAlz framework will bring new GenAI-powered applications to customers faster and enable them to benefit from the immense potential of generative AI, while also providing enterprise-grade security, reliability and performance.”\\nCurated, Optimized Models for Custom Generative AICustomers using the NVIDIA foundry service can choose from several NVIDIA AI Foundation models, including a new family ofNVIDIA Nemotron-3 8B modelshosted in the Azure AI model catalog. Developers can also access the Nemotron-3 8B models on the NVIDIA NGC™ catalog, as well as community models such as Meta’s Llama 2 models optimized for NVIDIA for accelerated computing, which are also coming soon to the Azure AI model catalog.\\nOptimized with 8 billion parameters, the Nemotron-3 8B family includes versions tuned for different use cases and have multilingual capabilities for building custom enterprise generative AI applications.\\nNVIDIA DGX Cloud Now Available on Microsoft Azure MarketplaceNVIDIA DGX Cloud AI supercomputing is available today onAzure Marketplace. It features instances customers can rent, scaling to thousands of NVIDIA Tensor Core GPUs, and comes with NVIDIA AI Enterprise software, including NeMo, to speed LLM customization.\\nThe addition of DGX Cloud on the Azure Marketplace enables Azure customers to use their existing Microsoft Azure Consumption Commitment credits to speed model development with NVIDIA AI supercomputing and software.\\nNVIDIA AI Enterprise software is now integrated into Azure Machine Learning, adding NVIDIA’s platform of secure, stable and supported AI and data science software. This brings NeMo andNVIDIA Triton Inference Server™ to Azure’s enterprise-grade AI service.\\nNVIDIA AI Enterprise is also available onAzure Marketplace, providing businesses worldwide with broad options for production-ready AI development and deployment of custom generative AI applications.\"},\n",
       " {'title': 'NVIDIA Supercharges Hopper, the World’s Leading AI Computing Platform',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-supercharges-hopper-the-worlds-leading-ai-computing-platform',\n",
       "  'publish_date': '2023-11-13',\n",
       "  'summary': 'NVIDIA today announced it has supercharged the world’s leading AI computing platform with the introduction of the NVIDIA HGX™ H200.',\n",
       "  'content': 'SC23—NVIDIA today announced it has supercharged the world’s leading AI computing platform with the introduction of the NVIDIA HGX™ H200. Based on NVIDIA Hopper™ architecture, the platform features the NVIDIA H200 Tensor Core GPU with advanced memory to handle massive amounts of data for generative AI and high performance computing workloads.\\nThe NVIDIA H200 is the first GPU to offer HBM3e — faster, larger memory to fuel the acceleration of generative AI and large language models, while advancing scientific computing for HPC workloads. With HBM3e, the NVIDIA H200 delivers 141GB of memory at 4.8 terabytes per second, nearly double the capacity and 2.4x more bandwidth compared with its predecessor, the NVIDIA A100.\\nH200-powered systems from the world’s leading server manufacturers and cloud service providers are expected to begin shipping in the second quarter of 2024.\\n“To create intelligence with generative AI and HPC applications, vast amounts of data must be efficiently processed at high speed using large, fast GPU memory,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “With NVIDIA H200, the industry’s leading end-to-end AI supercomputing platform just got faster to solve some of the world’s most important challenges.”\\nPerpetual Innovation, Perpetual Performance LeapsThe NVIDIA Hopper architecture delivers an unprecedented performance leap over its predecessor and continues to raise the bar through ongoing software enhancements with H100, including the recent release of powerful open-source libraries likeNVIDIA TensorRT™-LLM.\\nThe introduction of H200 will lead to further performance leaps, including nearly doubling inference speed on Llama 2, a 70 billion-parameter LLM, compared to the H100. Additional performance leadership and improvements with H200 are expected with future software updates.\\nNVIDIA H200 Form FactorsNVIDIA H200 will be available in NVIDIA HGX H200 server boards with four- and eight-way configurations, which are compatible with both the hardware and software of HGX H100 systems. It is also available in theNVIDIA GH200 Grace Hopper™ Superchip with HBM3e, announced in August.With these options, H200 can be deployed in every type of data center, including on premises, cloud, hybrid-cloud and edge. NVIDIA’s global ecosystem of partner server makers — includingASRock Rack,ASUS, Dell Technologies, Eviden,GIGABYTE, Hewlett Packard Enterprise,Ingrasys, Lenovo,QCT, Supermicro, Wistron and Wiwynn — can update their existing systems with an H200.\\nAmazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure will be among the first cloud service providers to deploy H200-based instances starting next year, in addition toCoreWeave,Lambdaand Vultr.\\nPowered by NVIDIA NVLink™ and NVSwitch™ high-speed interconnects, HGX H200 provides the highest performance on various application workloads, including LLM training and inference for the largest models beyond 175 billion parameters.\\nAn eight-way HGX H200 provides over 32 petaflops of FP8 deep learning compute and 1.1TB of aggregate high-bandwidth memory for the highest performance in generative AI and HPC applications.\\nWhen paired with NVIDIA Grace™ CPUs with an ultra-fast NVLink-C2C interconnect, the H200 creates the GH200 Grace Hopper Superchip with HBM3e — an integrated module designed to serve giant-scale HPC and AI applications.\\nAccelerate AI With NVIDIA Full-Stack SoftwareNVIDIA’s accelerated computing platform is supported by powerful software tools that enable developers and enterprises to build and accelerate production-ready applications from AI to HPC. This includes theNVIDIA AI Enterprisesuite of software for workloads such as speech, recommender systems and hyperscale inference.\\nAvailabilityThe NVIDIA H200 will be available from global system manufacturers and cloud service providers starting in the second quarter of 2024.\\nWatch Buck’sSC23 special addresson Nov. 13 at 6 a.m. PT to learn more about the NVIDIA H200 Tensor Core GPU.'},\n",
       " {'title': 'NVIDIA Grace Hopper Superchip Powers JUPITER, Defining a New Class of Supercomputers to Propel AI for Scientific Discovery',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchip-powers-jupiter-defining-a-new-class-of-supercomputers-to-propel-ai-for-scientific-discovery',\n",
       "  'publish_date': '2023-11-13',\n",
       "  'summary': 'NVIDIA today announced that JUPITER — which launches a new class of supercomputers for AI-driven scientific breakthroughs — will be powered by the NVIDIA Grace Hopper™ accelerated computing architecture to deliver extreme-scale computing power for ...',\n",
       "  'content': 'SC23—NVIDIA today announced that JUPITER — which launches a new class of supercomputers for AI-driven scientific breakthroughs — will be powered by the NVIDIA Grace Hopper™ accelerated computing architecture to deliver extreme-scale computing power for AI and simulation workloads.\\nHosted at the Forschungszentrum Jülich facility in Germany, JUPITER — which is owned by the EuroHPC Joint Undertaking and contracted to Eviden and ParTec — is being built in collaboration with NVIDIA, ParTec, Eviden and SiPearl to accelerate the creation of foundational AI models in climate and weather research, material science, drug discovery, industrial engineering and quantum computing.\\nJUPITER marks the debut of a quadNVIDIA GH200 Grace Hopper Superchipnode configuration, based on Eviden’s BullSequana XH3000 liquid-cooled architecture, with a booster module comprising close to 24,000 NVIDIA GH200 Superchips interconnected with theNVIDIA Quantum-2 InfiniBandnetworking platform. Being the world’s most powerful AI system, JUPITER can deliver over 90 exaflops of performance for AI training — 45x more than Jülich’s previous JUWELS Booster system — and 1 exaflop for high performance computing (HPC) applications, while consuming only 18.2 megawatts of power.\\nThe quad GH200 features an innovative node architecture with 288 Arm Neoverse cores capable of achieving 16 petaflops of AI performance using up to 2.3 terabytes of high-speed memory. Four GH200 processors are networked through a high-speed NVIDIA NVLink®connection.\\n“The JUPITER supercomputer powered by NVIDIA GH200 and using our advanced AI software will deliver exascale AI and HPC performance to tackle the greatest scientific challenges of our time,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “Our work with Jülich, Eviden and ParTec on this groundbreaking system will usher in a new era of AI supercomputing to advance the frontiers of science and technology.”\\n“At the heart of JUPITER is NVIDIA’s accelerated computing platform, making it a groundbreaking system that will revolutionize scientific research,” said Thomas Lippert, director of the Jülich Supercomputing Centre. “JUPITER combines exascale AI and exascale HPC with the world’s best AI software ecosystem to boost the training of foundational models to new heights.”\\n“Jülich Supercomputing Centre’s JUPITER system is the latest example of the significant strides Eviden’s making with the NVIDIA GH200,” said Emmanuel Le Roux, group senior vice president and global head of HPC, AI and quantum at Eviden. “Collaborating with NVIDIA to integrate the revolutionary GH200 into the BullSequana XH3000 supercomputer will empower the research and scientific community to push the boundaries of simulations, tackle scientific challenges and accelerate uncharted discoveries.”\\nThe JUPITER supercomputer defines a new class of supercomputers by combining NVIDIA’s full stack of software solutions to solve some of the world’s toughest challenges, including in the areas of:\\nInstallation of the JUPITER system is expected in 2024.\\nWatch NVIDIA’sSC23 special addressfeaturing Buck and professor Kristel Michielsen from the Jülich Supercomputing Centre to learn more about the JUPITER supercomputer.'},\n",
       " {'title': 'NVIDIA Sets Conference Call for Third-Quarter Financial Results',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-sets-conference-call-for-third-quarter-financial-results-6896799',\n",
       "  'publish_date': '2023-11-01',\n",
       "  'summary': 'CFO Commentary to Be Provided in Writing Ahead of CallSANTA CLARA, Calif., Nov. 01, 2023 (GLOBE NEWSWIRE) -- NVIDIA will host a conference call on Tuesday, Nov. 21, at 2 p.m. PT (5 p.m. ET), ...',\n",
       "  'content': 'CFO Commentary to Be Provided in Writing Ahead of Call\\nSANTA CLARA, Calif., Nov.  01, 2023  (GLOBE NEWSWIRE) -- NVIDIA will host a conference call on Tuesday, Nov. 21, at 2 p.m.\\xa0PT (5 p.m. ET), to discuss its financial results for the third quarter of fiscal year 2024, which ended Oct. 29, 2023.\\nThe call will be webcast live (in listen-only mode) oninvestor.nvidia.com. The company’s prepared remarks will be followed by a question-and-answer session, which will be limited to questions from financial analysts and institutional investors.\\nAhead of the call, NVIDIA will provide written commentary on its third-quarter results from its CFO, Colette Kress. This material will be posted toinvestor.nvidia.comimmediately after the company’s results are publicly announced at approximately 1:20 p.m. PT.\\nThe webcast will be recorded and available for replay until the company’s conference call to discuss financial results for its fourth quarter and fiscal year 2024.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling\\xa0industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.'},\n",
       " {'title': 'NVIDIA Partners With Foxconn to Build Factories and Systems\\xa0for the AI Industrial Revolution',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-partners-with-foxconn-to-build-factories-and-systemsfor-the-ai-industrial-revolution',\n",
       "  'publish_date': '2023-10-17',\n",
       "  'summary': 'NVIDIA today announced that it is collaborating with Hon Hai Technology Group (Foxconn) to accelerate the AI industrial revolution.',\n",
       "  'content': 'NVIDIA today announced that it is collaborating with Hon Hai Technology Group (Foxconn) to accelerate the AI industrial revolution.\\nFoxconn will integrate NVIDIA technology to develop a new class of data centers powering a wide range of applications — including digitalization of manufacturing and inspection workflows, development of AI-powered electric vehicle and robotics platforms, and a growing number of language-based generative AI services.\\nAnnounced in a fireside chat with NVIDIA founder and CEO Jensen Huang and Foxconn Chairman and CEO Young Liu at Hon Hai Tech Day, in Taipei, the collaboration starts with the creation of AI factories — an NVIDIA® GPU computing infrastructure specially built for processing, refining and transforming vast amounts of data into valuable AI models and tokens — based on the NVIDIA accelerated computing platform, including the latest NVIDIA GH200 Grace Hopper™ Superchip and NVIDIA AI Enterprise software.\\nFoxconn is also developing its smart solution platforms based on NVIDIA technologies:\\n“Most importantly, NVIDIA and Foxconn are building these factories together. We will be helping the whole industry move much faster into the new AI era,” said Foxconn Chairman and CEO Young Liu.\\n“A new type of manufacturing has emerged — the production of intelligence. And the data centers that produce it are AI factories,” said Huang. “Foxconn, the world’s largest manufacturer, has the expertise and scale to build AI factories globally. We are delighted to expand our decade-long partnership with Foxconn to accelerate the AI industrial revolution.”\\nEnabling Foxconn Customers to Build AI Data FactoriesWorking closely with NVIDIA, Foxconn is expected to build a large number of systems based on NVIDIA CPUs, GPUs and networking for its global customer base, which is looking to create and operate their own AI factories, optimized withNVIDIA AI Enterprise software.\\nAmong the key NVIDIA technologies Foxconn is using to create these custom designs are NVIDIA HGX™ reference designs featuring eight NVIDIA H100 Tensor Core GPUs per system, NVIDIA GH200 Superchips, NVIDIA OVX™ reference designs and NVIDIA networking.\\nWith these systems, Foxconn customers can leverage NVIDIA accelerated computing to deliver generative AI services as well as use simulation to speed up the training of autonomous machines, including industrial robots and self-driving cars.\\nFoxconn Eyes Potential AI FactoryIn addition to equipping its customers with NVIDIA technology-powered AI factories, Foxconn is eyeing its own that will tap into the NVIDIA Omniverse™ platform and Isaac and Metropolis frameworks to meet the strict production and quality standards of the electronics industry.\\nAdvances in edge AI and simulation are enabling deployment of autonomous mobile robots that can travel several miles a day and industrial robots for assembling components, applying coatings, packaging and performing quality inspections.\\nAn AI factory with these NVIDIA platforms can give Foxconn the ability to accomplish AI training and inference, enhance factory workflows and run simulations in the virtual world before deployment in the physical world. Simulating the entire robotics and automation pipeline from end to end provides Foxconn with a path to operational efficiency gains, saving time and costs.\\nDeveloping Safe, AI-Powered EVsFoxconn will also deliver a range of NVIDIA DRIVE™ solutions to global automakers, serving as a tier-one manufacturer of NVIDIA DRIVE Orin™-based electronic control units (ECUs) today and scaling to NVIDIA DRIVE Thor-based ECUs in the future.\\nAs a contract manufacturer, Foxconn will offer highly automated and autonomous, AI-rich EVs featuring the upcoming NVIDIA DRIVE Hyperion 9 platform, which includes DRIVE Thor and a state-of-the-art sensor architecture. This will enable Foxconn and its automotive customers to realize a new era of functionally safe and secure software-defined cars.'},\n",
       " {'title': 'Infosys and NVIDIA Team to Help World’s Enterprises Boost Productivity With Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/infosys-and-nvidia',\n",
       "  'publish_date': '2023-09-20',\n",
       "  'summary': 'Infosys, a global leader in next-generation digital services and consulting, and NVIDIA today announced that they have expanded their strategic collaboration with the aim to help enterprises worldwide drive productivity gains with generative AI ...',\n",
       "  'content': 'Infosys(NSE, BSE, NYSE: INFY),a global leader in next-generation digital services and consulting, andNVIDIA(NASDAQ: NVDA) today announced that they have expanded their strategic collaboration with the aim to help enterprises worldwide drive productivity gains with generative AI applications and solutions.\\nThe broadened alliance will bring theNVIDIA AI Enterpriseecosystem of models, tools, runtimes and GPU systems toInfosys Topaz—an AI-first set of services, solutions and platforms that use generative AI technologies. Through the integration, Infosys will create offerings customers can adopt to easily integrate generative AI into their businesses.\\nAdditionally, Infosys plans to set up an NVIDIA Center of Excellence, where it will train and certify 50,000 of its employees on NVIDIA AI technology to provide generative AI expertise to its vast network of customers across\\xa0industries.\\n“Infosys is transforming into an AI-first company to better provide AI-based services to our clients worldwide. Our clients are also looking at complex AI use cases that can drive significant business value across their entire value chain,” said Nandan Nilekani, co-founder and chairman, Infosys. “Infosys Topaz offerings and solutions are complementary to NVIDIA’s core stack. By combining our strengths and training 50,000 of our workforce on NVIDIA AI technology, we are creating end-to-end industry leading AI solutions that will help enterprises on their journey to become AI-first.”\\n“Generative AI will drive the next wave of enterprise productivity gains,” said Jensen Huang, founder and CEO, NVIDIA. “The NVIDIA AI Enterprise ecosystem is ramping quickly to provide the platform for generative AI. Together, NVIDIA and Infosys will create an expert workforce to help businesses use this platform to build custom applications and solutions.”\\n\\nFull-Stack NVIDIA Integration Powers Advanced Infosys Solutions\\nInfosys uses the full-stack NVIDIAgenerative AI platform, including hardware and enterprise-grade software to innovate across its business operations, and it is helping customers create generative AI applications for business operations, sales and marketing.\\nWithNVIDIA AI Enterprise frameworks, pretrained models and toolkits — including theNVIDIA NeMo™LLM framework,NVIDIA Metropolisfor computer vision andNVIDIA Rivafor speech AI — Infosys has already developed a number of offerings to multiple AI-first enterprise solutions across industries. These include:\\nIntegrating theInfosys Video Analyticsplatform with NVIDIA Metropolis, which brings the power of computer vision to address retail industry challenges, including creating frictionless shopping experiences, improving merchandising and planogram compliance, reducing shrinkage, monitoring inventory, extracting real-time intelligence, checking compliances for health, safety and more, as well as for logistics, manufacturing and utilities.\\nPairingInfosys Generative AI Labswith the NVIDIA NeMo framework, which enables organizations to fine-tune and fast-track deployments of large language models tailored for a wide range of enterprise use cases, providing a cost-effective and easily scalable platform. UsingNVIDIA NeMo Guardrails, Infosys is augmenting its Responsible AI Toolkit to build powerful intelligent practices to safeguard against the potential risks of generative AI, such as IP infringement, bias and toxicity, hallucinations and security threats.\\nUsingInfosys CortexwithNVIDIA Rivaspeech and translation AI, Infosys is developing AI-driven next-generation contact center solutions. These include language neutralization features for seamless multilingual support, as well as equipping contact center representatives with real-time customer intent and sentiment analysis tools to enhance customer satisfaction and foster brand loyalty.\\nThe collaboration extends to digitalization applications, with a focus on developing solutions for enterprise use cases across 3D workflows, design collaboration, digital twin, world simulation and others.\\nInfosys and NVIDIA are also co-developing AI-powered solutions in areas like 5G, cybersecurity and energy transition.'},\n",
       " {'title': 'Tata Partners With NVIDIA to Build Large-Scale AI Infrastructure',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/tata-partners-with-nvidia-to-build-large-scale-ai-infrastructure',\n",
       "  'publish_date': '2023-09-08',\n",
       "  'summary': 'NVIDIA today announced an extensive collaboration with Tata Group to deliver AI computing infrastructure and platforms for developing AI solutions. The collaboration will bring state-of-the-art AI capabilities within reach to thousands of ...',\n",
       "  'content': 'NVIDIA today announced an extensive collaboration with Tata Group to deliver AI computing infrastructure and platforms for developing AI solutions. The collaboration will bring state-of-the-art AI capabilities within reach to thousands of organizations, businesses and AI researchers, and hundreds of startups in India.\\nThe companies will work together to build an AI supercomputer powered by the next-generationNVIDIA®GH200 Grace Hopper Superchipto achieve performance that is best in class.\\n“The global generative AI race is in full steam,” said Jensen Huang, founder and CEO of NVIDIA. “Data centers worldwide are shifting to GPU computing to build energy-efficient infrastructure to support the exponential demand for generative AI.\\n“We are delighted to partner with Tata as they expand their cloud infrastructure service with NVIDIA AI supercomputing to support the exponential demand of generative AI startups and processing of large language models.” Huang said.\\nTata Communications and NVIDIA will develop an AI cloud in India aimed at providing critical infrastructure that enables computing’s next lifecycle. Tata Communications’ robust global network combined with the AI cloud will empower enterprises to transfer data across the AI cloud at high speeds, enabling them to effectively bring the AI cloud to the doorstep of every enterprise.\\nTCS will utilize the AI infrastructure and capabilities to build and process generative AI applications. The NVIDIA partnership will further enable TCS in collaborating with its customers to drive reimagination with an AI-first approach. Additionally, TCS will upskill its 600,000-strong workforce leveraging the partnership.\\nThis partnership will also catalyze the AI-led transformation across Tata Group companies ranging from manufacturing to consumer businesses.\\nCommenting on the collaboration with NVIDIA, N. Chandrasekaran, chairman of Tata Sons, said: “The advancements in AI have made focus on AI a central priority in governments, industries and society at large. The impact of AI and machine learning is going to be profound across industries and every aspect of our lives. This is a key transformational trend of the decade and every company must prepare to make this AI transition. Our partnership with NVIDIA will democratize access to AI infrastructure, accelerate build-out of AI solutions and enable upgradation of AI talent at scale. Tata Group’s presence across sectors coupled with NVIDIA’s deep capabilities offers numerous opportunities for collaboration to advance India’s AI ambition.”'},\n",
       " {'title': 'Reliance and NVIDIA Partner to Advance AI in India, for India',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/reliance-and-nvidia-partner-to-advance-ai-in-india-for-india',\n",
       "  'publish_date': '2023-09-08',\n",
       "  'summary': 'In a major step to support India’s industrial sector, NVIDIA and Reliance Industries today announced a collaboration to develop India’s own foundation large language model trained on the nation’s diverse languages and tailored for generative AI ...',\n",
       "  'content': 'In a major step to support India’s industrial sector, NVIDIA and Reliance Industries today announced a collaboration to develop India’s own foundation large language model trained on the nation’s diverse languages and tailored for generative AI applications to serve the world’s most populous nation.\\nThe companies will work together to build AI infrastructure that is over an order of magnitude more powerful than the fastest supercomputer in India today. NVIDIA will provide access to the most advancedNVIDIA® GH200 Grace Hopper SuperchipandNVIDIA DGX™ Cloud, an AI supercomputing service in the cloud. GH200 marks a fundamental shift in computing architecture that provides exceptional performance and massive memory bandwidth.\\nThe NVIDIA-powered AI infrastructure is the foundation of the new frontier into AI for Reliance Jio Infocomm, Reliance Industries’ telecom arm. The global AI revolution is transforming industries and daily life. To serve India’s vast potential in AI, Reliance will create AI applications and services for their 450 million Jio customers and provide energy-efficient AI infrastructure to scientists, developers and startups across India.\\nAI can help rural farmers interact via cell phones in their local language to get weather information and crop prices. It can help provide, at massive scale, expert diagnosis of medical symptoms and imaging scans where doctors may not be immediately available. AI can better predict cyclonic storms using decades of atmospheric data, enabling those at risk to evacuate and find shelter.\\nThe AI infrastructure will be hosted in AI-ready computing data centers that will eventually expand to 2,000 MW. Execution and implementation will be managed by Jio, which has extensive offerings and experience across mobile telephony, 5G spectrum, fiber networks and more.\\n“We are delighted to partner with Reliance to build state-of-the-art AI supercomputers in India,” said Jensen Huang, founder and CEO of NVIDIA. “India has scale, data and talent. With the most advanced AI computing infrastructure, Reliance can build its own large language models that power generative AI applications made in India, for the people of India.”\\nJio has broad expertise, infrastructure and engineering skill to roll out and manage the new AI computing infrastructure. The collaboration with NVIDIA also aligns with its strategy of serving as a large, comprehensive digital, cloud and networking platform for both consumers and business customers.\\n“As India advances from a country of data proliferation to creating technology infrastructure for widespread and accelerated growth, computing and technology super centres like the one we envisage with NVIDIA will provide the catalytic growth just like Jio did to our nation’s digital march,” said Mukesh Ambani, chairman and managing director of Reliance Industries. “I am delighted with the partnership with NVIDIA and looking forward to a purposeful journey together.”\\n“At Jio, we are committed to fueling India’s technology renaissance by democratizing access to cutting-edge technologies, and our collaboration with NVIDIA is a significant step in this direction,” said Akash Ambani, chairman of Reliance Jio Infocomm. “Together, we will develop a state-of-the-art AI cloud infrastructure that is secure, sustainable and deeply relevant across India, accelerating the nation’s journey towards becoming an AI powerhouse.”'},\n",
       " {'title': 'Google Cloud and NVIDIA Expand Partnership to Advance AI Computing, Software and Services',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/google-cloud-and-nvidia-expand-partnership-to-advance-ai-computing-software-and-services',\n",
       "  'publish_date': '2023-08-29',\n",
       "  'summary': 'Google Cloud and NVIDIA today announced new AI infrastructure and software for customers to build and deploy massive models for generative AI and speed data science workloads.',\n",
       "  'content': 'Google Cloud Next —\\xa0Google Cloud and NVIDIA today announced new AI infrastructure and software for customers to build and deploy massive models for generative AI and speed data science workloads.\\nIn a fireside chat at Google Cloud Next, Google Cloud CEO Thomas Kurian and NVIDIA founder and CEO Jensen Huang discussed how the partnership is bringing end-to-end machine learning services to some of the largest AI customers in the world — including by making it easy to run AI supercomputers with Google Cloud offerings built on NVIDIA technologies. The new hardware and software integrations utilize the same NVIDIA technologies employed over the past two years by Google DeepMind and Google research teams.\\n“We’re at an inflection point where accelerated computing and generative AI have come together to speed innovation at an unprecedented pace,” Huang said. “Our expanded collaboration with Google Cloud will help developers accelerate their work with infrastructure, software and services that supercharge energy efficiency and reduce costs.”\\n“Google Cloud has a long history of innovating in AI to foster and speed innovation for our customers,” Kurian said. “Many of Google’s products are built and served on NVIDIA GPUs, and many of our customers are seeking out NVIDIA accelerated computing to power efficient development of LLMs to advance generative AI.”\\nNVIDIA Integrations to Speed AI and Data Science DevelopmentGoogle’s framework for building massive large language models (LLMs), PaxML, is now optimized for NVIDIA accelerated computing.\\nOriginally built to span multiple Google TPU accelerator slices, PaxML now enables developers to useNVIDIA® H100andA100Tensor Core GPUs for advanced and fully configurable experimentation and scale. A GPU-optimized PaxML container is available immediately in theNVIDIA NGC™ software catalog. In addition, PaxML runs on JAX, which has been optimized for GPUs leveraging the OpenXLA compiler.\\nGoogle DeepMind and other Google researchers are among the first to use PaxML with NVIDIA GPUs for exploratory research.\\nThe NVIDIA-optimized container for PaxML will be available immediately on the NVIDIA NGC container registry to researchers, startups and enterprises worldwide that are building the next generation of AI-powered applications.\\nAdditionally, the companies announced Google’s integration ofserverless Sparkwith NVIDIA GPUs throughGoogle’s Dataprocservice. This will help data scientists speed Apache Spark workloads to prepare data for AI development.\\nThese new integrations are the latest in NVIDIA and Google’s extensive history of collaboration. They cross hardware and software announcements, including:'},\n",
       " {'title': 'NVIDIA Announces Upcoming Events for Financial Community',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-upcoming-events-for-financial-community-6894934',\n",
       "  'publish_date': '2023-08-24',\n",
       "  'summary': 'SANTA CLARA, Calif., Aug. 24, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following events for the financial community: Goldman Sachs Communacopia & Technology ConferenceTuesday, Sept. ...',\n",
       "  'content': \"SANTA CLARA, Calif., Aug.  24, 2023  (GLOBE NEWSWIRE) -- NVIDIA will present at the following events for the financial community:\\nGoldman Sachs Communacopia & Technology ConferenceTuesday, Sept. 5, 12:25 p.m. Pacific time\\nEvercore ISI 2023 Semiconductor & Semiconductor Equipment ConferenceWednesday, Sept. 6, 6 a.m. Pacific time\\nCiti's 2023 Global Technology ConferenceThursday, Sept. 7, 9:15 a.m. Pacific time\\nBank of America Global A.I. Conference 2023Monday, Sept. 11, 9 a.m. Pacific time\\nInterested parties can listen to the live audio webcast of NVIDIA presentations at financial events atinvestor.nvidia.com. Replays of the webcasts will be available for 90 days afterward.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling\\xa0industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.\\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\"},\n",
       " {'title': 'NVIDIA Announces Financial Results for Second Quarter Fiscal 2024',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-second-quarter-fiscal-2024',\n",
       "  'publish_date': '2023-08-23',\n",
       "  'summary': 'NVIDIA today reported revenue for the second quarter ended July 30, 2023, of $13.51 billion, up 101% from a year ago and up 88% from the previous quarter.',\n",
       "  'content': 'NVIDIA (NASDAQ: NVDA) today reported revenue for the second quarter ended July 30, 2023, of $13.51 billion, up 101% from a year ago and up 88% from the previous quarter.\\nGAAP earnings per diluted share for the quarter were $2.48, up 854% from a year ago and up 202% from the previous quarter. Non-GAAP earnings per diluted share were $2.70, up 429% from a year ago and up 148% from the previous quarter.\\n“A new computing era has begun. Companies worldwide are transitioning from general-purpose to accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\\n“NVIDIA GPUs connected by our Mellanox networking and switch technologies and running our CUDA AI software stack make up the computing infrastructure of generative AI.\\n“During the quarter, major cloud service providers announced massive NVIDIA H100 AI infrastructures. Leading enterprise IT system and software providers announced partnerships to bring NVIDIA AI to every industry. The race is on to adopt generative AI,” he said.\\nDuring the second quarter of fiscal 2024, NVIDIA returned $3.38 billion to shareholders in the form of 7.5 million shares repurchased for $3.28 billion, and cash dividends. As of the end of the second quarter, the company had $3.95 billion remaining under its share repurchase authorization. On August 21, 2023, the Board of Directors approved an additional $25.00 billion in share repurchases, without expiration. NVIDIA plans to continue share repurchases this fiscal year.\\nNVIDIA will pay its next quarterly cash dividend of $0.04 per share on September 28, 2023, to all shareholders of record on September 7, 2023.\\nQ2 Fiscal 2024 Summary\\n\\nOutlookNVIDIA’s outlook for the third quarter of fiscal 2024 is as follows:\\nHighlights\\nNVIDIA achieved progress since its previous earnings announcement in these areas:\\nData Center\\nGaming\\nProfessional Visualization\\nAutomotive\\nCFO CommentaryCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available athttps://investor.nvidia.com.\\nConference Call and Webcast InformationNVIDIA will conduct a conference call with analysts and investors to discuss its second quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website,https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its third quarter of fiscal 2024.\\nNon-GAAP MeasuresTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, legal settlement costs, contributions, other, gains and losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\\n\\nNVIDIA CORPORATIONCONDENSED CONSOLIDATED STATEMENTS OF INCOME(In millions, except per share data)(Unaudited)\\n\\n\\nNVIDIA CORPORATIONCONDENSED CONSOLIDATED BALANCE SHEETS(In millions)(Unaudited)\\n\\n\\nNVIDIA CORPORATIONCONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS(In millions)(Unaudited)\\n\\n\\n\\nNVIDIA CORPORATIONRECONCILIATION OF GAAP TO NON-GAAP FINANCIAL MEASURES(In millions, except per share data)(Unaudited)\\n\\n\\n\\n\\nNVIDIA CORPORATIONRECONCILIATION OF GAAP TO NON-GAAP OUTLOOK\\n\\n'},\n",
       " {'title': 'VMware and NVIDIA Unlock Generative AI for Enterprises',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/vmware-and-nvidia-unlock-generative-ai-for-enterprises',\n",
       "  'publish_date': '2023-08-22',\n",
       "  'summary': 'VMware Inc. and NVIDIA today announced the expansion of their strategic partnership to ready the hundreds of thousands of enterprises that run on VMware’s cloud infrastructure for the era of generative AI.',\n",
       "  'content': 'VMware Explore—VMware Inc. (NYSE: VMW) and NVIDIA (NASDAQ: NVDA) today announced the expansion of their strategic partnership to ready the hundreds of thousands of enterprises that run on VMware’s cloud infrastructure for the era of generative AI.\\nVMware Private AI Foundation with NVIDIA will enable enterprises to customize models and run generative AI applications, including intelligent chatbots, assistants, search and summarization. The platform will be a fully integrated solution featuringgenerative AI software and accelerated computing from NVIDIA, built on VMware Cloud Foundation and optimized for AI.\\n“Generative AI and multi-cloud are the perfect match,” said Raghu Raghuram, CEO, VMware. “Customer data is everywhere — in their data centers, at the edge, and in their clouds. Together with NVIDIA, we’ll empower enterprises to run their generative AI workloads adjacent to their data with confidence while addressing their corporate data privacy, security and control concerns.”\\n“Enterprises everywhere are racing to integrate generative AI into their businesses,” said Jensen Huang, founder and CEO, NVIDIA. “Our expanded collaboration with VMware will offer hundreds of thousands of customers — across financial services, healthcare, manufacturing and more — the full-stack software and computing they need to unlock the potential of generative AI using custom applications built with their own data.”\\nFull-Stack Computing to Supercharge Generative AITo achieve business benefits faster, enterprises are seeking to streamline development, testing and deployment of generative AI applications. McKinsey estimates that generative AI could add up to $4.4 trillion annually to the global economy.(1)\\nVMware Private AI Foundation with NVIDIA will enable enterprises to harness this capability, customizing large language models; producing more secure and private models for their internal usage; and offering generative AI as a service to their users; and, more securely running inference workloads at scale.\\nThe platform is expected to include integrated AI tools to empower enterprises to run proven models trained on their private data in a cost-efficient manner. To be built onVMware Cloud Foundation and NVIDIA AI Enterprise software, the platform’s expected benefits will include:\\nThe platform will featureNVIDIA NeMo, an end-to-end, cloud-native framework included in NVIDIA AI Enterprise — the operating system of the NVIDIA AI platform — that allows enterprises to build, customize and deploy generative AI models virtually anywhere. NeMo combines customization frameworks, guardrail toolkits, data curation tools and pretrained models to offer enterprises an easy, cost-effective and fast way to adopt generative AI.\\nFor deploying generative AI in production, NeMo uses TensorRT for Large Language Models (TRT-LLM), which accelerates and optimizes inference performance on the latest LLMs on NVIDIA GPUs. With NeMo, VMware Private AI Foundation with NVIDIA will enable enterprises to pull in their own data to build and run custom generative AI models on VMware’s hybrid cloud infrastructure.\\nAt VMware Explore 2023, NVIDIA and VMware will highlight how developers within enterprises can use the newNVIDIA AI Workbenchto pull community models, like Llama 2,available on Hugging Face, customize them remotely and deploy production-grade generative AI in VMware environments.\\nBroad Ecosystem Support for VMware Private AI Foundation With NVIDIAVMware Private AI Foundation with NVIDIA will be supported by Dell Technologies, Hewlett Packard Enterprise and Lenovo — which will be among the first to offer systems that supercharge enterprise LLM customization and inference workloads withNVIDIA L40S GPUs,NVIDIA BlueField®-3 DPUsandNVIDIA ConnectX®-7 SmartNICs.\\nThe NVIDIA L40S GPU enables up to 1.2x more generative AI inference performance and up to 1.7x more training performance compared with the NVIDIA A100 Tensor Core GPU.\\nNVIDIA BlueField-3 DPUs accelerate, offload and isolate the tremendous compute load of virtualization, networking, storage, security and other cloud-native AI services from the GPU or CPU.\\nNVIDIA ConnectX-7 SmartNICs deliver smart, accelerated networking for data center infrastructure to boost some of the world’s most demanding AI workloads.\\nVMware Private AI Foundation with NVIDIA builds on the companies’ decade-long partnership. Their co-engineering work optimized VMware’s cloud infrastructure to run NVIDIA AI Enterprise with performance comparable to bare metal. Mutual customers further benefit from the resource and infrastructure management and flexibility enabled by VMware Cloud Foundation.\\nAvailabilityVMware intends to release VMware Private AI Foundation with NVIDIA in early 2024.'},\n",
       " {'title': 'NVIDIA AI-Ready Servers From World’s Leading System Manufacturers to Supercharge Generative AI for Enterprises',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/ai-enterprise-ready-servers',\n",
       "  'publish_date': '2023-08-22',\n",
       "  'summary': 'NVIDIA today announced the world’s leading system manufacturers will deliver AI-ready servers that support VMware Private AI Foundation with NVIDIA, announced separately today, to help companies customize and deploy generative AI applications using ...',\n",
       "  'content': 'VMware Explore--\\xa0NVIDIA today announced the world’s leading system manufacturers will deliver AI-ready servers that supportVMware Private AI Foundation with NVIDIA, announced separately today, to help companies customize and deploy generative AI applications using their proprietary business data.\\nNVIDIA AI-ready servers will includeNVIDIA®L40S GPUs,NVIDIA BlueField®-3 DPUsandNVIDIA AI Enterprise softwareto enable enterprises to fine-tune generative AI foundation models and deploy generative AI applications like intelligent chatbots, search and summarization tools. These servers also provide NVIDIA-accelerated infrastructure and software to power VMware Private AI Foundation with NVIDIA.\\nNVIDIA L40S-powered servers from leading global system manufacturers — Dell Technologies, Hewlett Packard Enterprise and Lenovo — will be available by year-end to accelerate enterprise AI.\\n“A new computing era has begun,” said Jensen Huang, founder and CEO of NVIDIA. “Companies in every industry are racing to adopt generative AI. With our ecosystem of world-leading software and system partners, we are bringing generative AI to the world’s enterprises.”\\nNVIDIA AI-ready servers are an ideal platform for businesses that will deploy VMware Private AI Foundation with NVIDIA.\\n“Generative AI is supercharging digital transformation, and enterprises need a fully integrated solution to more securely build applications that enable them to advance their business,” said Raghu Raghuram, CEO of VMware. “Through the combined expertise of VMware, NVIDIA and our server manufacturer partners, businesses will be able to develop and deploy AI with data privacy, security and control.”\\nPowering Generative AI Transformation in the EnterpriseNVIDIA AI-ready servers are designed to provide full-stack accelerated infrastructure and software for industries racing to adopt generative AI for a broad range of applications, including drug discovery, retail product descriptions, intelligent virtual assistants, manufacturing simulation and fraud detection.\\nThe servers featureNVIDIA AI Enterprise, the operating system of the NVIDIA AI platform. The software provides production-ready enterprise support and security for over 100 frameworks, pretrained models, toolkits and software, includingNVIDIA NeMo™ for LLMs,NVIDIA Modulusfor simulations,NVIDIA RAPIDS™ for data science andNVIDIA Triton™ Inference Serverfor production AI.\\nBuilt to handle complex AI workloads with billions of parameters, L40S GPUs include fourth-generation Tensor Cores and an FP8 Transformer Engine, delivering over 1.45 petaflops of tensor processing power and up to 1.7x training performance compared with the NVIDIA A100 Tensor Core GPU.\\nFor generative AI applications such as intelligent chatbots, assistants, search and summarization, the NVIDIA L40S enables up to 1.2x more generative AI inference performance than the NVIDIA A100 GPU.\\nIntegrating NVIDIA BlueField DPUs drives further speedups by accelerating, offloading and isolating the tremendous compute load of virtualization, networking, storage, security and other cloud-native AI services.\\nNVIDIA ConnectX®-7 SmartNICs offer advanced hardware offloads and ultra-low latency, delivering best-in-class, scalable performance for data-intensive generative AI workloads.\\nBroad Ecosystem to Speed Enterprise Generative AI DeploymentsThe world’s leading computer makers are building NVIDIA AI-ready servers, including the Dell PowerEdge R760xa, HPE ProLiant Gen11 servers for VMware Private AI Foundation with NVIDIA, and Lenovo ThinkSystem SR675 V3.\\n\"Generative AI is a catalyst for innovation, helping to solve some of the world’s most pressing challenges,” said Michael Dell, chairman and chief executive officer, Dell Technologies. “Dell Generative AI Solutions with NVIDIA AI-ready servers will play a critical role in advancing human progress by driving unprecedented levels of productivity and revolutionizing the way industries operate.\"\\n“Generative AI will usher in a new scale of productivity for enterprises, from powering chatbots and digital assistants to helping with the design and development of new solutions,” said Antonio Neri, president and CEO of HPE. “We are pleased to continue working closely with NVIDIA to feature its GPUs and software in a range of enterprise tuning and inference workload solutions that will accelerate deployments of generative AI.”\\n“Businesses are eager to adopt generative AI to power intelligent transformation,” said Yang Yuanqing, chairman and CEO of Lenovo. “In collaboration with NVIDIA and VMware, Lenovo is further extending our leadership in generative AI and solidifying our unique position in helping customers in their AI journey.”\\nAvailabilityNVIDIA AI-ready servers with L40S GPUs and BlueField DPUs will be available by year-end, with instances available from cloud service providers expected in the coming months.'},\n",
       " {'title': 'NVIDIA Releases Major Omniverse Upgrade With Generative AI and OpenUSD',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-releases-major-omniverse-upgrade-with-generative-ai-and-openusd',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA today announced a major release of its NVIDIA Omniverse™ platform, offering new foundation applications and services for developers and industrial enterprises to optimize and enhance their 3D pipelines with the OpenUSD framework and ...',\n",
       "  'content': 'SIGGRAPH—NVIDIA today announced a major release of itsNVIDIA Omniverse™ platform, offering new foundation applications and services for developers and industrial enterprises to optimize and enhance their 3D pipelines with theOpenUSDframework andgenerative AI.\\nThe update to Omniverse, an OpenUSD-native software platform for connecting, describing and simulating across 3D tools and applications, accelerates the creation of virtual worlds and advanced workflows for industrial digitalization. Cesium, Convai, Move AI, SideFX Houdini and Wonder Dynamics are now connected to Omniverse via OpenUSD.\\nKey highlights from the platform update includeadvancements to Omniverse Kit— the engine for developing native OpenUSD applications and extensions — as well as to the NVIDIA Omniverse Audio2Face™ foundation app andspatial-computing capabilities.\\n“Industrial enterprises are racing to digitalize their workflows, increasing the demand for OpenUSD-enabled, connected, interoperable, 3D software ecosystems,” said Rev Lebaredian, vice president of Omniverse and simulation technology at NVIDIA. “The latest Omniverse update lets developers tap generative AI through OpenUSD to enhance their tools, and it allows enterprises to build larger, more complex world-scale simulations as digital testing grounds for their industrial applications.”\\nKey ImprovementsUpdates to Omniverse Kit include:\\nThese platform updates are showcased in Omniverse foundation applications, which are fully customizable reference applications thatcreators,enterprisesanddeveloperscan copy, extend or enhance. Upgraded applications include:\\nOpenUSD Expands Omniverse EcosystemNVIDIA alsoannounceda broad range of frameworks, resources and services for developers and companies to accelerate the adoption of Universal Scene Description, known as OpenUSD.\\nIn addition, the company announced newOmniverse Cloud APIs, built by NVIDIA, for developers to more seamlessly implement and deploy OpenUSD pipelines and applications. For example, ChatUSD is a large language model copilot for developers that can answer USD knowledge questions or generate Python-USD code scripts.\\nNew Omniverse connections enabled by OpenUSD are now available, opening more opportunities for industrial enterprises to break data silos in their complex production pipelines.\\nExpanding their collaboration across Adobe Substance 3D, generative AI and OpenUSD initiatives, Adobe and NVIDIA announced plans to make Adobe Firefly — Adobe’s family of creative generative AI models — available as APIs in Omniverse, enabling developers and creators to enhance their design processes.\\nWonder Dynamics is connected to Omniverse with new OpenUSD export support through its AI platform Wonder Studio, which automatically animates, lights and composes computer-generated characters into live-action scenes. New OpenUSD export support will enable artists to generate and export a complete 3D scene — all from a single camera video.\\nLuma AI’s reality-capture models in USDZ format can be readily imported to Omniverse. Tools from avatar company Convai and character-engine company Inworld AI are connected to Omniverse. With AI tools like Convai, creators can add characters in their digital twin environments that can provide relevant information about the environment and objects, be a tour guide or be a virtual robot. Move AI enables single-camera motion capture with the Move One app, which can be used to generate 3D character animations that can then be exported to OpenUSD and used in Omniverse.\\nOmniverse users can nowbuild content, experiences and applicationsthat are compatible with other OpenUSD-based spatial computing platforms such as ARKit and RealityKit. Plus, new support for the Khronos Group’sOpenXRopen standard expands Omniverse use to more headsets from manufacturers such asHTC VIVE,Magic LeapandVarjo.\\nUsers of SideFX Houdini can also now load Houdini Digital Assets directly into the Omniverse viewport, making Houdini-based connected workflows more seamless. The Cesium extension for Omniverse, called Cesium for Omniverse, enables 3D Tiles, an open standard for streaming massive geospatial datasets in virtual worlds, including those supported by OpenUSD. CGI.Backgrounds now has several ultra-high-definition HDRi maps available in USD Composer. The Cadence DataCenter Design Software™, now available via Omniverse, helps users see computational fluid dynamics simulations in the complete context of their digital twin. With the Cadence data center extension, users can plan, test and validate design and operational considerations before implementing them. And the Blackshark.AI world digital twin platform is now connected to Omniverse.\\nCustomers Using Omniverse for DigitalizationCustomers are using Omniverse for tasks ranging from simulating robots to training AI models and improving animation.\\nBoston Dynamics AI Institute is using Omniverse to simulate robots and their interactions to enable the design of novel robotics and control systems. Continental, one of the leading companies in automotive and industrialization of autonomous systems, is using Omniverse in its mobile robots business to generate physically accuratesynthetic dataat scale to train computer-vision AI models and perform system-integration testing.\\nVolvo Cars has transitioned its digital twin to be OpenUSD-based, using Omniverse to create immersive visualizations to help customers make online purchasing decisions.\\nMarks Design, a brand design and experience agency, is using Omniverse and OpenUSD to streamline collaboration and improve its animation, visualization and rendering workflows.\\nNew Omniverse Systems and PartnersNVIDIA iscollaboratingwith global systems manufacturers to bring RTX workstations optimally configured for Omniverse to millions of designers, architects and engineers. The new systems feature up to four NVIDIA RTX 6000 Ada Generation GPUs, bundled withNVIDIA Omniverse Enterprisesoftware, to accelerate OpenUSD world-building, generative AI-enhanced collaborative design and other industrial digitalization applications.\\nOmniverse users can also take advantage of the newNVIDIA L40S GPU, a powerful, universal data center GPU that accelerates the most graphics-intensive workloads.\\nAvailabilityThe latest Omniverse release is now available in betato download for freeand coming soon to Omniverse Enterprise.\\nLearn more aboutNVIDIA Omniverseand watch NVIDIA CEO Jensen Huang’sSIGGRAPH keynoteaddress on demand.'},\n",
       " {'title': 'NVIDIA Omniverse Opens Portals to Vast Worlds of OpenUSD',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-omniverse-opens-portals-to-vast-worlds-of-openusd',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'New Omniverse Cloud APIs Help Developers Adopt OpenUSD; Generative AI Model ChatUSD LLM Converses in USD; RunUSD Translates USD to Interactive Graphics, DeepSearch LLM Enables Semantic 3D ...',\n",
       "  'content': 'SIGGRAPH—NVIDIA today announced a broad range of frameworks, resources and services for developers and companies to accelerate the adoption of Universal Scene Description, known asOpenUSD.\\nNVIDIA is advancing the development of OpenUSD — a 3D framework enabling interoperability between software tools and data types for the building of virtual worlds — throughNVIDIA Omniverse™ and a new portfolio of technologies and cloud application programming interfaces (APIs) — including ChatUSD and RunUSD — along with a newNVIDIA OpenUSD Developer Program.\\nThese investments in OpenUSD expand on NVIDIA’s co-founding of theAlliance for OpenUSD (AOUSD)— an organization announced last week that will standardize OpenUSD specifications — along with Pixar, Adobe, Apple and Autodesk.\\n“Just as HTML ignited a major computing revolution of the 2D internet, OpenUSD will spark the era of collaborative 3D and industrial digitalization,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA is putting our full force behind the advancement and adoption of OpenUSD through our development of NVIDIA Omniverse and generative AI.”\\nOpenUSD Goes to the CloudNVIDIA announced four newOmniverse CloudAPIs built by NVIDIA for developers to more seamlessly implement and deploy OpenUSD pipelines and applications.\\nEvolving OpenUSD FunctionalityOpenUSD was invented to better connect film and animation pipelines. Industrial applications — such as building interoperable manufacturing design pipelines, creating physically accurate real-timedigital twinsof factories, or training and validating autonomous vehicles — require different demands of the 3D framework.\\nTo enable these highly complex industrial and perception AI workloads, NVIDIA is developing NVIDIA Omniverse, the OpenUSD-native software platform for developing applications, as well as technologies that include geospatial data models, metrics assembly and simulation-ready, orSimReady, specifications for OpenUSD.\\nGeospatial data models for OpenUSD let users develop simulations and calculations for true-to-reality digital twins of factories, warehouses, cities and evenEarth. For extreme-scale projects, it accounts for the planet’s curvature to ensure the simulations are physically accurate.\\nIndustrial applications combine datasets from many tools and sources, each represented in different units. NVIDIA is developing a metrics assembly for OpenUSD that enables users to combine diverse datasets with complete accuracy.\\nNVIDIA is also developing a structure for new SimReady 3D models. These will include true-to-reality material and physical properties, which are critical to accurately training autonomous robots and vehicles. For example, an autonomous robot tasked with sorting packages needs to be trained in simulation on 3D packages that move and react to physical contact just as they would in the real world.\\nGet early access to OpenUSD services, resources and tools through theNVIDIA OpenUSD Developer Program. This includes two new fully distributable OpenUSD sample scenes built by NVIDIA designers and artists — called Da Vinci’s Workshop and Riverfront Tower.\\nLearn more aboutOpenUSD, itsecosystem of connections,AOUSDand theNVIDIA Omniverseplatform. Watch Huang’sSIGGRAPH keynote.'},\n",
       " {'title': 'NVIDIA, Global Data Center System Manufacturers to Supercharge Generative AI and Industrial Digitalization',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-global-data-center-system-manufacturers-to-supercharge-generative-ai-and-industrial-digitalization',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA today announced NVIDIA OVX™ servers featuring the new NVIDIA® L40S GPU, a powerful, universal data center processor designed to accelerate the most compute-intensive, complex applications, including AI training and inference, 3D design and ...',\n",
       "  'content': 'SIGGRAPH—NVIDIA today announcedNVIDIA OVX™ serversfeaturing the new NVIDIA®L40S GPU, a powerful, universal data center processor designed to accelerate the most compute-intensive, complex applications, including AI training and inference, 3D design and visualization, video processing and industrial digitalization with theNVIDIA Omniverse™ platform.\\nThe new GPU powers accelerated computing workloads for generative AI, which is transforming workflows and services across industries, including text, image and video generation, chatbots, game development, product design and healthcare.\\n“As generative AI transforms every industry, enterprises are increasingly seeking large-scale compute resources in the data center,” said Bob Pette, vice president of professional visualization at NVIDIA. “OVX systems with NVIDIA L40S GPUs accelerate AI, graphics and video processing workloads, and meet the demanding performance requirements of an ever-increasing set of complex and diverse applications.”\\nPowerful Performance for AI and GraphicsNVIDIA OVX systems will enable up to eight NVIDIA L40S GPUs per server, each equipped with 48GB of memory. Based on the NVIDIA Ada Lovelace GPU architecture, the L40S includes fourth-generation Tensor Cores and an FP8 Transformer Engine, delivering over 1.45 petaflops of tensor processing power. For complex AI workloads with billions of parameters and multiple data modalities — such as text and video — L40S enables up to 1.2x more generative AI inference performance and up to 1.7x training performance compared with the NVIDIA A100 Tensor Core GPU.\\nTo power high-fidelity professional visualization workflows like real-time rendering, product design and 3D content creation, the NVIDIA L40S GPU includes 142 third-generation RT Cores that deliver 212 teraflops of ray-tracing performance. This enables creative professionals to create immersive visual experiences and photorealistic content.\\nFor computationally demanding workflows, such as engineering and scientific simulations, the NVIDIA L40S includes 18,176 CUDA®cores, delivering nearly 5x the single-precision floating-point (FP32) performance of the NVIDIA A100 GPU to accelerate complex calculations and data-intensive analyses.\\nEarly AdoptionAmong the first cloud service providers to offer L40S instances is CoreWeave, which specializes in large-scale, GPU-accelerated workloads.\\n“With the explosion of generative AI, our customers across industries are seeking powerful compute offerings and scale to match the complexity of any workload — from interactive video to AI design and automation,” said Brian Venturo, chief technology officer at CoreWeave. “NVIDIA L40S GPUs will further expand our broad portfolio of NVIDIA solutions, making CoreWeave the first specialized cloud provider to offer these new resources for fast, efficient and cost-effective accelerated computing to power the next wave of generative AI applications.”\\nSoftware to Boost AIEnterprises deploying L40S GPUs can benefit fromNVIDIA AI Enterprisesoftware, which announced a major update today. The software provides production-ready enterprise support and security for over 100 frameworks, pretrained models, toolkits and software, includingNVIDIA Modulusfor simulations,NVIDIA RAPIDS™ for data science andNVIDIA Triton™ Inference Serverfor production AI.\\nOmniverse ExpandsNVIDIA also announced major updates to theOmniverseplatform, introducing capabilities and platform enhancements that enable developers to accelerate and advance OpenUSD pipelines and industrial digitalization applications with the power of generative AI. The next generation ofNVIDIA OVXsystems powering Omniverse Cloud will feature L40S GPUs to deliver the AI and graphics performance needed to supercharge generative AI pipelines and Omniverse workloads.\\nAvailabilityThe NVIDIA L40S will be available starting this fall. Global system builders, including ASUS, Dell Technologies, GIGABYTE, HPE, Lenovo,QCTand Supermicro, will soon offer OVX systems that include the NVIDIA L40S GPUs. These servers will help professionals worldwide advance AI and bring generative AI applications like intelligent chatbots, search and summarization tools to users across industries.'},\n",
       " {'title': 'NVIDIA, Global Workstation Manufacturers to Launch Powerful Systems for Generative AI and LLM Development, Content Creation, Data Science',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-global-workstation-manufacturers-to-launch-powerful-systems-for-generative-ai-and-llm-development-content-creation-data-science',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA and global manufacturers today announced powerful new NVIDIA RTX™ workstations designed for development and content creation in the age of generative AI and digitalization.',\n",
       "  'content': 'SIGGRAPH—NVIDIA and global manufacturers today announced powerful new NVIDIA RTX™ workstations designed for development and content creation in the age of generative AI and digitalization.\\nThe systems, including those from BOXX, Dell Technologies, HP and Lenovo, are based onNVIDIA RTX 6000 Ada Generation GPUsand incorporateNVIDIA AI EnterpriseandNVIDIA Omniverse™ Enterprisesoftware.\\nSeparately, NVIDIA also released three new desktop workstation Ada Generation GPUs — theNVIDIA RTX 5000,RTX 4500andRTX 4000— to deliver the latest AI, graphics and real-time rendering technology to professionals worldwide.\\n“Few workloads are as challenging as generative AI and digitalization applications, which require a full-stack approach to computing,” said Bob Pette, vice president of professional visualization at NVIDIA. “Professionals can now tackle these on a desktop with the latest NVIDIA-powered RTX workstations, enabling them to build vast, digitalized worlds in the new age of generative AI.”\\nThe new RTX workstations offer up to four NVIDIA RTX 6000 Ada GPUs, each equipped with 48GB of memory, and a single desktop workstation can provide up to 5,828 TFLOPS of AI performance and 192GB of GPU memory. Depending on user needs, systems can be configured with NVIDIA AI Enterprise or Omniverse Enterprise to power a breadth of demanding generative AI and graphics-intensive workloads.\\nNVIDIA AI Enterprise 4.0, announced separately today, now includesNVIDIA NeMo™, an end-to-end framework for building and customizing foundation models for generative AI,NVIDIA RAPIDS™ libraries for data science, as well as frameworks, pretrained models and tools for building common enterprise AI use cases, including recommenders, virtual assistants and cybersecurity solutions.\\nOmniverse Enterprise is a platform for industrial digitalization that enables teams to develop interoperable 3D workflows andOpenUSDapplications. As an OpenUSD-native platform, Omniverse enables globally distributed teams to collaborate on full-design-fidelity datasets from hundreds of 3D applications.\\n“Yurts provides a full-stack generative AI solution aligning with multiple form factors, deployment models and budgets of our customers. We’ve achieved this by leveraging LLMs for various natural language processing tasks and incorporating the RTX 6000 Ada. From private data centers to workstation-sized solutions that fit under a desk, Yurts remains committed to scaling our platform and offering alongside NVIDIA,” said Jason Schnitzer, chief technology officer at Yurts.\\nWorkstation users can also take advantage of the newNVIDIA AI Workbench, available soon in early access, which provides developers with a unified, easy-to-use\\xa0toolkit for creating, fine-tuning and running generative AI models with just a few clicks. Users of any skill level can quickly create, test and customize pretrained generative AI models on a PC or workstation and then scale them to virtually any data center, public cloud or NVIDIA DGX Cloud.\\nNext-Generation RTX TechnologyThe new NVIDIA RTX 5000, RTX 4500 and RTX 4000 desktop GPUs feature the latest NVIDIA Ada Lovelace architecture technologies, including:\\n“The NVIDIA RTX 5000 Ada GPU demonstrates NVIDIA’s impressive generational performance improvements — it has significantly increased our efficiency in creating stereo panoramas using Enscape,” said Dan Stine, director of design technology at architectural firm Lake|Flato. “With the performance boost and large frame buffer of RTX 5000 GPUs, our large, complex models look great in virtual reality, which gives our clients a more comfortable and contextual experience.”\\nAvailabilityRTX workstationsfeaturing up to four RTX 6000 Ada GPUs, NVIDIA AI Enterprise and NVIDIA Omniverse Enterprise will be available from system builders starting in the fall.\\nThe new NVIDIA RTX 5000 GPU is now available and shipping from HP and through global distribution partners such as Leadtek, PNY and Ryoyo Electro starting today. The NVIDIA RTX 4500 and RTX 4000 GPUs will be available in the fall from BOXX, Dell Technologies, HP and Lenovo and through global distribution partners.'},\n",
       " {'title': 'NVIDIA AI Workbench Speeds Adoption of Custom Generative AI for World’s Enterprises',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-ai-workbench-speeds-adoption-of-custom-generative-ai-for-worlds-enterprises',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA today announced NVIDIA AI Workbench, a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation — then scale them to virtually any data center, public ...',\n",
       "  'content': 'SIGGRAPH—NVIDIA today announcedNVIDIA AI Workbench, a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models on a PC or workstation — then scale them to virtually any data center, public cloud orNVIDIA DGX™ Cloud.\\nAI Workbench removes the complexity of getting started with an enterprise AI project. Accessed through a simplified interface running on a local system, it allows developers to customize models from popular repositories like Hugging Face, GitHub andNVIDIA NGC™ using custom data. The models can then be shared easily across multiple platforms.\\n“Enterprises around the world are racing to find the right infrastructure and build generative AI models and applications,” said Manuvir Das, vice president of enterprise computing at NVIDIA. “NVIDIA AI Workbench provides a simplified path for cross-organizational teams to create the AI-based applications that are increasingly becoming essential in modern business.”\\nA New Era for AI DevelopersWhile hundreds of thousands of pretrained models are now available, customizing them with the many open-source tools can require hunting through multiple online repositories for the right framework, tools and containers, and employing the right skills to customize a model for a specific use case.\\nWith NVIDIA AI Workbench, developers can customize and run generative AI in just a few clicks. It allows them to pull together all necessary enterprise-grade models, frameworks, software development kits and libraries from open-source repositories and the NVIDIA AI platform into a unified developer toolkit.Leading AI infrastructure providers — including Dell Technologies, Hewlett Packard Enterprise, HP Inc., Lambda, Lenovo and Supermicro — are embracing AI Workbench for its ability to augment their latest generation of multi-GPU-capable desktop workstations, high-end mobile workstations and virtual workstations.\\nDevelopers with a Windows or Linux-based NVIDIA RTX™ PC or workstation will also be able to initiate, test and fine-tune enterprise-grade generative AI projects on their local RTX systems, and easily access data center and cloud computing resources to scale as needed.\\nNew NVIDIA AI Enterprise 4.0 Software Advances AI DeploymentTo further accelerate the adoption of generative AI, NVIDIA announced the latest version of its enterprise software platform,NVIDIA AI Enterprise 4.0. It gives businesses the tools needed to adopt generative AI, while also offering the security and API stability required for reliable production deployments.\\nNewly supported software and tools in NVIDIA AI Enterprise that help streamline generative AI deployment include:\\nNVIDIA AI Enterprise software — which lets users build and run NVIDIA AI-enabled solutions across the cloud, data center and edge — is certified to run on mainstream NVIDIA-Certified Systems™, NVIDIA DGX systems, all major cloud platforms and newly announced NVIDIA RTX workstations.\\nLeading software companiesServiceNowandSnowflake, as well as infrastructure provider Dell Technologies, which offersDell Generative AI Solutions, recently announced they are collaborating with NVIDIA to enable new generative AI solutions and services on their platforms. The integration of NVIDIA AI Enterprise 4.0 and NVIDIA NeMo provides a foundation for production-ready generative AI for customers.\\nNVIDIA AI Enterprise 4.0 will be integrated into partner marketplaces, includingAWS Marketplace,Google CloudandMicrosoft Azure, as well as through NVIDIA cloud partner Oracle Cloud Infrastructure.\\nAdditionally, MLOps providers, including Azure Machine Learning, ClearML,Domino Data Lab, Run:AI, and Weights & Biases, are adding seamless integration with the NVIDIA AI platform to simplify production-grade generative AI model development.\\nBroad Partner Support“Dell Technologies and NVIDIA are committed to helping enterprises build purpose-built AI models to access the immense opportunity of generative AI. With NVIDIA AI Workbench, developers can take advantage of the full Dell Generative AI Solutions portfolio to customize models on PCs, workstations and data center infrastructure.”— Meghana Patwardhan, vice president of commercial client products at Dell Technologies\\n“Most enterprises do not have the expertise, budget and data center resources to manage the high complexity of AI software and systems. We look forward to NVIDIA AI Workbench’s potential to simplify generative AI project creation with one-click training and deployment on the HPE GreenLake edge-to-cloud platform.”—Evan Sparks, chief product officer for AI at HPE\\n“As a workstation market leader offering the performance and efficiency needed for the most demanding data science and AI models, we have a long history collaborating with NVIDIA. HP is embracing the next generation of high-performance systems, coupled with NVIDIA RTX Ada Generation GPUs and NVIDIA AI Workbench, and bringing the power of generative AI to our enterprise customers and helping move AI workloads between the cloud and locally.”— Jim Nottingham, senior vice president of advanced computing solutions at HP Inc.\\n“Lenovo and NVIDIA are helping customers overcome deployment complexities and more easily implement generative AI to deliver transformative services and products to the market. NVIDIA AI Workbench and the Lenovo AI-ready portfolio enable developers to leverage the power of their smart devices and scale across edge-to-cloud infrastructure.”—Rob Herman, vice president and general manager of Lenovo Workstation & Client AI\\n“The longstanding VMware and NVIDIA partnership has helped unlock the power of AI for every business by delivering an end-to-end enterprise platform optimized for AI workloads. Together, we are making generative AI more accessible and easier to implement in the enterprise. With AI Workbench, NVIDIA is giving developers a set of powerful tools to help enterprises accelerate gen AI adoption. With the new NVIDIA AI Workbench, development teams can seamlessly move AI workloads from the desktop to production.”— Chris Wolf, vice president of VMware AI Labs\\nWatch NVIDIA founder and CEO Jensen Huang’sSIGGRAPH keynoteaddress on demand to learn more aboutNVIDIA AI WorkbenchandNVIDIA AI Enterprise 4.0.\\nAI Workbench is coming soon in early access.Sign upto get notified when it is available.'},\n",
       " {'title': 'NVIDIA and Hugging Face to Connect Millions of Developers to Generative AI Supercomputing',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA and Hugging Face today announced a partnership that will put generative AI supercomputing at the fingertips of millions of developers building large language models (LLMs) and other advanced AI applications.',\n",
       "  'content': 'SIGGRAPH—NVIDIA and Hugging Face today announced a partnership that will put generative AI supercomputing at the fingertips of millions of developers building large language models (LLMs) and other advanced AI applications.\\nBy giving developers access toNVIDIA DGX™ CloudAI supercomputing within the Hugging Face platform to train and tune advanced AI models, the combination will help supercharge industry adoption of generative AI using LLMs that are custom-tailored with business data for industry-specific applications, including intelligent chatbots, search and summarization.\\n“Researchers and developers are at the heart of generative AI that is transforming every industry,” said Jensen Huang, founder and CEO of NVIDIA. “Hugging Face and NVIDIA are connecting the world’s largest AI community with NVIDIA’s AI computing platform in the world’s leading clouds. Together, NVIDIA AI computing is just a click away for the Hugging Face community.”\\nAs part of the collaboration, Hugging Face will offer a new service — called Training Cluster as a Service — to simplify the creation of new and custom generative AI models for the enterprise. Powered by NVIDIA DGX Cloud, the service will be available in the coming months.\\n“People around the world are making new connections and discoveries with generative AI tools, and we’re still only in the early days of this technology shift,” said Clément Delangue, co-founder and CEO of Hugging Face. “Our collaboration will bring NVIDIA’s most advanced AI supercomputing to Hugging Face to enable companies to take their AI destiny into their own hands with open source and with speed they need to contribute to what’s coming next.”\\nSupercharging LLM Customization and Training Within Hugging FaceThe Hugging Face platform lets developers build, train and deploy state-of-the-art AI models using open-source resources. Over 15,000 organizations use Hugging Face, and its community has shared over 250,000 models and 50,000 datasets.\\nSupercharging LLM Customization and Training Within Hugging FaceThe Hugging Face platform lets developers build, train and deploy state-of-the-art AI models using open-source resources. Over 15,000 organizations use Hugging Face, and its community has shared over 250,000 models and 50,000 datasets.\\nThe DGX Cloud integration with Hugging Face will bring one-click access to NVIDIA’s multi-node AI supercomputing platform. With DGX Cloud, Hugging Face users will be able to connect to NVIDIA AI supercomputing, providing the software and infrastructure needed to rapidly train and tune foundation models with unique data to drive a new wave of enterprise LLM development. With Training Cluster as a Service, powered by DGX Cloud, companies will be able to leverage their unique data for Hugging Face to create uniquely efficient models in record time.\\nDGX Cloud Speeds Development and Customization for Massive ModelsEach instance of DGX Cloud features eightNVIDIA H100orA10080GB Tensor Core GPUs for a total of 640GB of GPU memory per node.NVIDIA Networkingprovides a high-performance, low-latency fabric that ensures workloads can scale across clusters of interconnected systems to meet the performance requirements of advanced AI workloads.\\nSupport from NVIDIA experts is included with DGX Cloud to help customers optimize their models and quickly resolve development challenges.\\nDGX Cloud infrastructure ishosted by leading NVIDIA cloud service provider partners.\\nAvailabilityThe NVIDIA DGX Cloud integration with Hugging Face is expected to be available in the coming months.\\nWatch Huang’sSIGGRAPH keynote addresson demand to learn more about NVIDIA DGX Cloud.'},\n",
       " {'title': 'NVIDIA Unveils Next-Generation GH200 Grace Hopper Superchip Platform for Era of Accelerated Computing and Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/gh200-grace-hopper-superchip-with-hbm3e-memory',\n",
       "  'publish_date': '2023-08-08',\n",
       "  'summary': 'NVIDIA today announced the next-generation NVIDIA GH200 Grace Hopper™ platform — based on a new Grace Hopper Superchip with the world’s first HBM3e processor — built for the era of accelerated computing and generative AI.',\n",
       "  'content': 'SIGGRAPH—NVIDIA today announced the next-generation NVIDIA GH200 Grace Hopper™ platform — based on a new Grace Hopper Superchip with the world’s first HBM3e processor — built for the era of accelerated computing and generative AI.\\nCreated to handle the world’s most complex generative AI workloads, spanning large language models, recommender systems and vector databases, the new platform will be available in a wide range of configurations.\\nThe dual configuration — which delivers up to 3.5x more memory capacity and 3x more bandwidth than the current generation offering — comprises a single server with 144 Arm Neoverse cores, eight petaflops of AI performance and 282GB of the latest HBM3e memory technology.\\n“To meet surging demand for generative AI, data centers require accelerated computing platforms with specialized needs,” said Jensen Huang, founder and CEO of NVIDIA. “The new GH200 Grace Hopper Superchip platform delivers this with exceptional memory technology and bandwidth to improve throughput, the ability to connect GPUs to aggregate performance without compromise, and a server design that can be easily deployed across the entire data center.”\\nThe new platform uses the Grace Hopper Superchip, which can be connected with additional Superchips byNVIDIA NVLink™, allowing them to work together to deploy the giant models used for generative AI. This high-speed, coherent technology gives the GPU full access to the CPU memory, providing a combined 1.2TB of fast memory when in dual configuration.\\nHBM3e memory, which is 50% faster than current HBM3, delivers a total of 10TB/sec of combined bandwidth, allowing the new platform to run models 3.5x larger than the previous version, while improving performance with 3x faster memory bandwidth.\\nGrowing Demand for Grace HopperLeading manufacturers are already offering systems based on the previously announced Grace Hopper Superchip. To drive broad adoption of the technology, the next-generation Grace Hopper Superchip platform with HBM3e is fully compatible with theNVIDIA MGX™server specification unveiled at COMPUTEX earlier this year. With MGX, any system manufacturer can quickly and cost-effectively add Grace Hopper into over 100 server variations.\\nAvailabilityLeading system manufacturers are expected to deliver systems based on the platform in Q2 of calendar year 2024.\\nWatch Huang’sSIGGRAPH keynoteaddress on demand to learn more about Grace Hopper.\\n'},\n",
       " {'title': 'NVIDIA Sets Conference Call for Second-Quarter Financial Results',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-sets-conference-call-for-second-quarter-financial-results-6894429',\n",
       "  'publish_date': '2023-08-02',\n",
       "  'summary': 'CFO Commentary to Be Provided in Writing Ahead of CallSANTA CLARA, Calif., Aug. 02, 2023 (GLOBE NEWSWIRE) -- NVIDIA will host a conference call on Wednesday, Aug. 23, at 2 p.m. PT (5 p.m. ET), ...',\n",
       "  'content': 'CFO Commentary to Be Provided in Writing Ahead of Call\\nSANTA CLARA, Calif., Aug.  02, 2023  (GLOBE NEWSWIRE) -- NVIDIA will host a conference call on Wednesday, Aug. 23, at 2 p.m.\\xa0PT (5 p.m. ET), to discuss its financial results for the second quarter of fiscal year 2024, which ended July 30, 2023.\\nThe call will be webcast live (in listen-only mode) oninvestor.nvidia.com. The company’s prepared remarks will be followed by a question-and-answer session, which will be limited to questions from financial analysts and institutional investors.\\nAhead of the call, NVIDIA will provide written commentary on its second-quarter results from its CFO, Colette Kress. This material will be posted toinvestor.nvidia.comimmediately after the company’s results are publicly announced at approximately 1:20 p.m. PT.\\nThe webcast will be recorded and available for replay until the company’s conference call to discuss financial results for its third quarter of fiscal year 2024.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling\\xa0industrial digitalization across markets. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.\\n'},\n",
       " {'title': 'Pixar, Adobe, Apple, Autodesk, and NVIDIA Form Alliance for OpenUSD to Drive Open Standards for 3D Content',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/aousd-to-drive-open-standards-for-3d-content',\n",
       "  'publish_date': '2023-08-01',\n",
       "  'summary': 'Pixar, Adobe, Apple, Autodesk, and NVIDIA, together with the Joint Development Foundation (JDF), an affiliate of the Linux Foundation, today announced the Alliance for OpenUSD (AOUSD) to promote the standardization, development, evolution, and ...',\n",
       "  'content': 'Pixar,Adobe,Apple,Autodesk, andNVIDIA, together with theJoint Development Foundation(JDF), an affiliate of the Linux Foundation, today announced the Alliance for OpenUSD (AOUSD) to promote the standardization, development, evolution, and growth of Pixar’s Universal Scene Description technology.\\nThe alliance seeks to standardize the 3D ecosystem by advancing the capabilities ofOpen Universal Scene Description (OpenUSD). By promoting greater interoperability of 3D tools and data, the alliance will enable developers and content creators to describe, compose, and simulate large-scale 3D projects and build an ever-widening range of 3D-enabled products and services.\\nCreated by Pixar Animation Studios, OpenUSD is a high-performance 3D scene description technology that offers robust interoperability across tools, data, and workflows. Already known for its ability to collaboratively capture artistic expression and streamline cinematic content production, OpenUSD’s power and flexibility make it an ideal content platform to embrace the needs of new industries and applications.\\nThe alliance will develop written specifications detailing the features of OpenUSD. This will enable greater compatibility and wider adoption, integration, and implementation, and allows inclusion by other standards bodies into their specifications. The Linux Foundation’s JDF was chosen to house the project, as it will enable open, efficient, and effective development of OpenUSD specifications, while providing a path to recognition through the International Organization for Standardization (ISO).\\nAOUSD will also provide the primary forum for the collaborative definition of enhancements to the technology by the greater industry. The alliance invites a broad range of companies and organizations to join and participate in shaping the future of OpenUSD.\\n“Universal Scene Description was invented at Pixar and is the technological foundation of our state-of-the-art animation pipeline,” said Steve May, Chief Technology Officer at Pixar and Chairperson of AOUSD. “OpenUSD is based on years of research and application in Pixar filmmaking. We open-sourced the project in 2016, and the influence of OpenUSD now expands beyond film, visual effects, and animation and into other industries that increasingly rely on 3D data for media interchange. With the announcement of AOUSD, we signal the exciting next step: the continued evolution of OpenUSD as a technology and its position as an international standard.”\\n“At Adobe, we believe in providing artists a set of flexible and powerful solutions running on a variety of devices,” said Guido Quaroni, Senior Director of Engineering, 3D&I at Adobe. “Leveraging a common 3D data representation during the creative process multiplies the value brought by each package and device. OpenUSD was created to be one of these ‘multipliers’ and we are excited to see a diverse group of companies joining together to support this innovative and open technology.”\\n“OpenUSD will help accelerate the next generation of AR experiences, from artistic creation to content delivery, and produce an ever-widening array of spatial computing applications,” said Mike Rockwell, Apple’s vice president of the Vision Products Group. “Apple has been an active contributor to the development of USD, and it is an essential technology for the groundbreaking visionOS platform, as well as the new Reality Composer Pro developer tool. We look forward to fostering its growth into a broadly adopted standard.”\\n“Whether you’re building CG worlds or digital twins or looking ahead to the 3D web, content creators need a cohesive way to collaborate and share data across tools, services, and platforms,” said Gordon Bradley, Fellow, Media & Entertainment, Autodesk. “Autodesk is excited to support the Alliance for OpenUSD as it drives 3D interoperability for visual effects, animation, and beyond, and supports our vision to help customers design and make a better world.”\\n“OpenUSD gives 3D developers, artists, and designers the complete foundation to tackle large-scale industrial, digital content creation, and simulation workloads with broad multi-app interoperability,” said Guy Martin, Director of Open Source and Standards at NVIDIA. “This alliance is a unique opportunity to accelerate OpenUSD collaboration globally by building formal standards across industries and initiatives to realize 3D worlds and industrial digitalization.”\\nAOUSD steering committee members will be speaking at both the Academy Software Foundation’s Open Source Days on Aug. 6 and at the SIGGRAPH conference at theAutodesk Vision Serieson Aug. 8 at 1 p.m. PT in Room 404A.\\nTo learn more about AOUSD and how to get involved, visitwww.aousd.org. To tune into the Academy Software Foundation panel on USD on Aug. 6, 2023, visitthe website.'},\n",
       " {'title': 'ServiceNow, NVIDIA, and Accenture Team to Accelerate Generative AI Adoption for Enterprises',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/servicenow-nvidia-accenture-ai-lighthouse',\n",
       "  'publish_date': '2023-07-26',\n",
       "  'summary': 'ServiceNow (NYSE: NOW), NVIDIA (NASDAQ: NVDA), and Accenture (NYSE: ACN) today announced the launch of AI Lighthouse, a first-of-its-kind program designed to fast-track the development and adoption of enterprise generative AI capabilities.',\n",
       "  'content': 'ServiceNow(NYSE: NOW), NVIDIA (NASDAQ: NVDA), and Accenture (NYSE: ACN) today announced the launch of AI Lighthouse, a first-of-its-kind program designed to fast-track the development and adoption of enterprise generative AI capabilities.\\nExpanding on existing strategic partnerships among ServiceNow, NVIDIA, and Accenture, AI Lighthouse will assist pioneering customers across industries in the design, development, and implementation of new generative AI use cases.\\nAI Lighthouse unites the ServiceNow enterprise automation platform and engine, NVIDIA AI supercomputing and software, and Accenture consulting and deployment services. The comprehensive offering will let customers collaborate as design partners in architecting custom generative AI large language models and applications to advance their businesses.\\n“This is a transformational moment for business to revolutionize how work gets done,” said ServiceNow Chairman and CEO Bill McDermott. “In collaboration with our visionary partners, ServiceNow, NVIDIA, and Accenture are forming the market-leading blueprint for AI-first enterprise innovation. We expect the AI Lighthouse customer program to inspire breakthrough ideas with massive ROI: ‘return on intelligence.’”\\n“Industries are racing to add generative AI tools to their operations at a faster pace than in any previous technology shift,” said Jensen Huang, founder and CEO, NVIDIA. “NVIDIA, ServiceNow, and Accenture are partnering to help customers lead their industries by deploying generative AI tools that harness their own invaluable knowledge to transform the applications they use every day.”\\n“Generative AI holds enormous potential for enterprises—it can help them reinvent how they work, strengthen their services, differentiate themselves, and reach new levels of performance,” said Julie Sweet, chair and CEO, Accenture. “Our expanded partnership with ServiceNow and NVIDIA will apply our combined experience, expertise, and insights to helping our clients create the most powerful, relevant, and responsible generative AI use cases and more quickly realize the value of this transformative technology.”\\nThe power of AI Lighthouse program will come to life for enterprise customers by:\\nReducing tedious manual work for customer service professionals, with overviews and insights to help them solve problems faster.\\nDeflecting cases by promoting self-service options, empowering people, and delivering engaging experiences with natural human language.\\nGenerating content automatically, including intelligent search results, work notes, and knowledge base articles.\\nBoosting developer productivity with intelligent recommendations for code.\\nSince May, ServiceNow has launched a slate of powerful generative AI capabilities, purpose-built for the Now Platform, and engaged with large pharmaceutical, financial services, manufacturing, and health care companies to test them in enterprise environments. The AI Lighthouse program will build on that early progress to collaborate on designing, developing, and implementing new generative AI use cases with a select group of customers across IT service management (ITSM), customer service management (CSM), and employee experience.\\nNVIDIA accelerated computing and software, includingNVIDIA DGX™ AI supercomputing andNVIDIA DGX Cloud, as well asNVIDIA NeMo™ LLM software, will provide full-stack computing for model training and tuning; ServiceNow will be the front-end workflow automation and intelligence platform; and Accenture will leverage its deep functional and industry knowledge and generative AI strategy, design and delivery experience to bring use cases to life for customers.\\nThe ServiceNow platform automates workflows across the entire enterprise by connecting disparate departments, systems, and silos and automating processes to increase productivity and enable seamless work experiences. Now Assist is ServiceNow’s generative AI experience—purpose-built within the ServiceNow platform—designed to enable intelligent automation and accelerate productivity by simplifying repetitive tasks, increasing agility, and transforming the user experience.\\nNVIDIA AI software and accelerated computing provide the platform for generative AI deployments across industries. Enterprises can use NeMo and NVIDIA frameworks, optimized inference engines, and APIs to add intelligence to generative AI applications such as drug discovery, intelligent chatbots, search, and summarization.\\nBuilding on Accenture’s recently announced $3 billion investment in AI, this collaboration will tap into the Accenture Center for Advanced AI, with its deep focus on generative AI and large language models. Accenture will accelerate the design and engineering of domain-specific LLMs and generative AI capabilities within the ServiceNow platform to make functional and industry workflows more intelligent—from elevating agent productivity and impact with summarization of service history and recommended actions, to improving self-service quality and speed with AI-powered virtual agents. Accenture will use its infrastructure and IT service operations experience across the cloud continuum, coupled with its vast experience in helping clients across industries leverage generative AI, to accelerate value across the enterprise.\\n\\nFor more information about the AI Lighthouse program as well as additional generative AI solutions ServiceNow strategic partners are offering, visithttps://www.servicenow.com/now‑platform/generative‑ai.html.\\nTo learn more about previously announced ServiceNow generative AI advancements, visit:\\nPartnerships withNVIDIAandCognizant\\nBigCode collaboration withHugging Face\\nServiceNow Generative AI Controller, which serves as the foundation for all generative AI functionality on the Now Platform\\nNow Assist for Search, which brings the power of generative AI to Portal Search, Next Experience, andVirtual Agent\\nNow Assist for Virtual Agent, a generative AI integration that helps virtual agents deliver conversational responses to questions from both customers and employees\\nCase summarization and text-to-code, powered by a proprietary ServiceNow LLM, to alleviate repetitive work and significantly improve productivity'},\n",
       " {'title': 'NVIDIA Names Melissa Lora to Board of Directors',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-names-melissa-lora-to-board-of-directors',\n",
       "  'publish_date': '2023-07-24',\n",
       "  'summary': 'NVIDIA today announced that it has named to its board of directors Melissa Lora, who spent three decades as an executive at Taco Bell Corp., a subsidiary of Yum! Brands, Inc., before retiring in 2018 as president of Taco Bell International.',\n",
       "  'content': 'NVIDIA today announced that it has named to its board of directors Melissa Lora, who spent three decades as an executive at Taco Bell Corp., a subsidiary of Yum! Brands, Inc., before retiring in 2018 as president of Taco Bell International.\\nShe has also been appointed to the board’s Audit Committee.\\n“Melissa is a great addition to our board of directors,” said Jensen Huang, founder and CEO of NVIDIA. “She brings senior management and operating experience, as well as extensive finance expertise, gained in a large corporate setting. We will benefit immensely from her guidance.”\\nLora, 61, holds a B.S. in finance from California State University, Long Beach, and an M.B.A. in corporate finance from University of Southern California. She also serves on the boards of directors of KB Home, where she is lead independent director, and Conagra Brands, Inc., where she is chairperson of the Audit & Finance Committee.\\nLora’s appointment expands NVIDIA’s board to 14 members.'},\n",
       " {'title': 'NVIDIA Announces Upcoming Event for Financial Community',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-upcoming-event-for-financial-community-6893775',\n",
       "  'publish_date': '2023-06-27',\n",
       "  'summary': 'SANTA CLARA, Calif., June 27, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following event for the financial community: Piper Sandler Webinar: Networks for AIWednesday, June 28, 9 a.m. ...',\n",
       "  'content': 'SANTA CLARA, Calif., June 27, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following event for the financial community:\\nPiper Sandler Webinar: Networks for AIWednesday, June 28, 9 a.m. Pacific time\\nInterested parties can listen to the live audio webcast of NVIDIA presentations at financial events atinvestor.nvidia.com. Replays of the webcasts will be available for 90 days afterward.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the industrial metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.\\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\n\\n'},\n",
       " {'title': 'Snowflake and NVIDIA Team to Help Businesses Harness Their Data for Generative AI in the Data Cloud',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/snowflake-and-nvidia-team-to-help-businesses-harness-their-data-for-generative-ai-in-the-data-cloud',\n",
       "  'publish_date': '2023-06-26',\n",
       "  'summary': 'Snowflake, the Data Cloud Company, and NVIDIA today announced at Snowflake Summit 2023 that they are partnering to provide businesses of all sizes with an accelerated path to create customized generative AI applications using their own proprietary ...',\n",
       "  'content': 'Snowflake Summit—Snowflake(NYSE: SNOW), the Data Cloud Company, andNVIDIA today announced atSnowflake Summit 2023that they are partnering to provide businesses of all sizes with an accelerated path to create customized generative AI applications using their own proprietary data, all securely within theSnowflake Data Cloud.\\nWith theNVIDIA NeMo™ platform for developing large language models (LLMs) and NVIDIA GPU-accelerated computing, Snowflake will enable enterprises to use data in their Snowflake accounts to make custom LLMs for advanced generative AI services, including chatbots, search and summarization. The ability to customize LLMs without moving data enables proprietary information to remain fully secured and governed within the Snowflake platform.\\n“Snowflake’s partnership with NVIDIA will bring high-performance machine learning and artificial intelligence to our vast volumes of proprietary and structured enterprise data, a new frontier to bringing unprecedented insights, predictions and prescriptions to the global world of business,” said Frank Slootman, chairman and CEO of Snowflake.\\n“Data is essential to creating generative AI applications that understand the complex operations and unique voice of every company,” said Jensen Huang, founder and CEO of NVIDIA. “Together, NVIDIA and Snowflake will create an AI factory that helps enterprises turn their own valuable data into custom generative AI models to power groundbreaking new applications\\xa0— right from the cloud platform that they use to run their businesses.”\\nNVIDIA and Snowflake’s collaboration represents a new opportunity for enterprises. It will enable them to use their proprietary data — which can range from hundreds of terabytes to petabytes of raw and curated business information — to create and fine-tune custom LLMs that power business-specific applications and services.\\nBy integrating AI technology from Snowflake and NVIDIA, customers can quickly and easily build, deploy and manage customized applications that bring the power of generative AI to all parts of their business across a variety of use cases. In addition, expanding AI capabilities in the Data Cloud enables these customers to create generative AI applications where their governed data already resides, a benefit that significantly reduces cost and latency while maintaining the security of their data.\\n“More enterprises than we expected are training or at least fine-tuning their own AI models, as they increasingly appreciate the value of their own data assets,” said Alexander Harrowell, principal analyst for advanced computing for AI at technology research group Omdia. “Similarly, enterprises are beginning to operate more diverse fleets of AI models for business-specific applications. Supporting them in this trend is one of the biggest open opportunities in the sector.”\\nCustom Models for Healthcare, Retail, Financial Services and More\\nWith over 8,000 customers worldwide (as of April 30, 2023), the Snowflake Data Cloud gives enterprises the ability to unify, integrate, analyze and share data across their organizations, as well as with customers, partners, suppliers and others. In addition, customers can build and share leading data applications at scale with the Data Cloud.\\nSnowflake’s unified platform offers industry-specific Data Clouds to help deliver innovative solutions across multiple verticals and lines of business spanningadvertising,media and entertainment,financial services,healthcare and life sciences,manufacturing,retail and consumer-packaged goods,technologyandtelecom. Most recently, Snowflake launched theGovernment and Education Data Cloudto enable data-driven decision making for the public sector.\\nSnowflake and NVIDIA’s collaboration will further enable customers to transform these industries by bringing customized generative AI applications to different verticals with the Data Cloud. For example, a healthcare insurance model could answer complex questions about procedures that are covered under various plans. A financial services model could share details about specific lending opportunities available to retail and business customers based on specific circumstances.\\nNVIDIA NeMo is a cloud-native enterprise platform for building, customizing and deploying generative AI models with billions of parameters. Snowflake plans to host and run NeMo in the Data Cloud, enabling customers to build, customize and deploy custom LLMs used for generative AI applications, such as chatbots and intelligent search.\\nWithNeMo Guardrailssoftware, developers can ensure their applications are aligned with business-specific topics, safety and security requirements.\\nTo learn more about the latest product integrations and trends in generative AI, watch Slootman and Huang’s fireside chat atSnowflake Summit 2023.\\nResources:\\nRead more aboutSnowflake and NVIDIA’s collaboration.\\nRegister to attend Snowflake Summit and learn more about the latestproduct integrations from Snowflake and NVIDIA.'},\n",
       " {'title': 'NVIDIA Stockholder Meeting Set for June 22; Individuals Can Participate Online',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-stockholder-meeting-set-for-june-22-individuals-can-participate-online',\n",
       "  'publish_date': '2023-06-08',\n",
       "  'summary': 'SANTA CLARA, Calif., June 08, 2023 (GLOBE NEWSWIRE) -- NVIDIA today announced it will hold its 2023 Annual Meeting of Stockholders online on Thursday, June 22, at 11 a.m. PT. The meeting will ...',\n",
       "  'content': 'SANTA CLARA, Calif., June 08, 2023 (GLOBE NEWSWIRE) -- NVIDIA today announced it will hold its 2023 Annual Meeting of Stockholders online on Thursday, June 22, at 11 a.m. PT. The meeting will take place virtually athttp://www.virtualshareholdermeeting.com/NVDA2023.\\nStockholders will need their control number included in their notice or proxy card to access the meeting, and may vote and submit questions while attending the meeting. Non-stockholders are welcome to attend by going to the above link and registering under “Guest Login.”\\nThe matters to be voted on at the meeting are set forth in the company’s Proxy Statement filed on May 8, 2023, with the U.S. Securities and Exchange Commission. The Proxy Statement is available atwww.nvidia.com/proxy.\\nA replay of the 2023 annual meeting webcast will be available until June 21, 2024, atwww.nvidia.com/proxy.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the industrial metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/\\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\n\\n'},\n",
       " {'title': 'NVIDIA Announces Upcoming Event for Financial Community',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-upcoming-event-for-financial-community-6893211',\n",
       "  'publish_date': '2023-06-08',\n",
       "  'summary': 'SANTA CLARA, Calif., June 08, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following event for the financial community: Nasdaq Investor Conference in Partnership with ...',\n",
       "  'content': 'SANTA CLARA, Calif., June 08, 2023 (GLOBE NEWSWIRE) -- NVIDIA will present at the following event for the financial community:\\nNasdaq Investor Conference in Partnership with JefferiesWednesday, June 14, 8:30 a.m. Pacific time\\nInterested parties can listen to the live audio webcast of NVIDIA presentations at financial events atinvestor.nvidia.com. Replays of the webcasts will be available for 90 days afterward.\\nAbout NVIDIASince its founding in 1993,NVIDIA(NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the industrial metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information athttps://nvidianews.nvidia.com/.\\n© 2023 NVIDIA Corporation. All rights reserved. NVIDIA and the NVIDIA logo are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries.\\n'},\n",
       " {'title': 'World’s Leading Electronics Manufacturers Adopt NVIDIA Generative AI and Omniverse to Digitalize State-of-the-Art Factories',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/worlds-leading-electronics-manufacturers-adopt-nvidia-generative-ai-and-omniverse-to-digitalize-state-of-the-art-factories',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA today announced that electronics manufacturers worldwide are advancing their industrial digitalization efforts using a new, comprehensive reference workflow that combines NVIDIA technologies for generative AI, 3D collaboration, simulation and ...',\n",
       "  'content': '\\nCOMPUTEX—NVIDIA today announced that electronics manufacturers worldwide are advancing their industrial digitalization efforts using a new, comprehensive reference workflow that combines NVIDIA technologies for generative AI, 3D collaboration, simulation and autonomous machines.\\nSupported by an expansive partner network, the workflow helps manufacturers plan, build, operate and optimize their factories with an array of NVIDIA technologies. These include:NVIDIA Omniverse™, which connects top computer-aided design apps, as well as APIs and cutting-edge frameworks for generative AI; theNVIDIA Isaac Sim™ application for simulating and testing robots; and theNVIDIA Metropolisvision AI framework, now enabled for automated optical inspection.\\nIn hiskeynote address at COMPUTEX, NVIDIA founder and CEO Jensen Huang showcased ademo of an entirely digitalized smart factory— an industry first for electronics makers.\\n“The world’s largest industries make physical things. Building them digitally first can save enormous costs,” said Huang. “NVIDIA makes it easy for electronics makers to build and operate virtual factories, digitalize their manufacturing and inspection workflows, and greatly improve quality and safety while reducing costly last-minute surprises and delays.”\\nWorld’s Leading Electronics Makers Embrace Digitalization With NVIDIAThe new reference workflow is being used by Foxconn Industrial Internet, Innodisk, Pegatron, Quanta and Wistron as they work to optimize their workcell and assembly line operations while lowering production costs.\\nFoxconn Industrial Internet, a service arm of the world’s largest technology manufacturer, is working with NVIDIA Metropolis ecosystem partners to automate significant portions of its circuit-board quality-assurance inspection points.\\n“NVIDIA’s strength in AI and its strong ecosystem of application partners are providing Foxconn Industrial Internet with a path to significant operational efficiency gains,” said Tai-Yu Chou, CTO at Foxconn Industrial Internet. “The combination of NVIDIA Metropolis for factories and Isaac Sim for robotics is helping us realize industrial automation goals faster than ever imagined.”\\nInnodisk is deploying NVIDIA Metropolis to automate optical inspection processes on its production lines, saving cost and improving production efficiency.\\nPegatron, a leading electronics manufacturer and service provider, is using thereference workflowto digitalize its circuit-board factories with simulation, robotics and automated production inspection.\\n“NVIDIA Omniverse, Isaac Sim and Metropolis give us the ability to accomplish AI training, enhance factory workflows and run numerous simulations in the virtual world before we commit to an idea in the physical world,” said Andrew Hsiao, associate vice president of the software R&D division at Pegatron. “Digitalizing our entire factory enables us to simulate the robotics and automation pipeline from end to end, and lets us try things out in a simulated environment, which saves time and greatly reduces costs.”\\nQuanta, a major manufacturer of laptops and other electronic hardware, is using AI robots from its subsidiaryTechman Robotto inspect the quality of manufactured products. Techman is leveraging Isaac Sim to simulate, test and optimize its state-of-the-art collaborative robots while using NVIDIA AI and GPUs for inference on the robots themselves.\\nWistron, one of the world’s largest suppliers of information and communications products, is tapping NVIDIA Omniverse to builddigital twinsof its automated receiving lines and operations buildings using inputs from Autodesk AutoCAD, Autodesk Revit and FlexSim. Wistron also uses NVIDIA Metropolis to automate portions of its circuit-board optical inspection using AI-enabled computer vision.\\nIndustrial Ecosystem Swarms NVIDIA TechnologiesNVIDIA is working with several leading manufacturing-tools and service providers to build a full-stack, single architecture with each at every workflow level.\\nAt the systems level,NVIDIA IGX Orin™ provides an all-in-one edge AI platform, combining industrial-grade hardware with enterprise-level software and support. IGX meets the unique durability and low-power-consumption requirements of edge computing, while delivering the high performance needed for developing and running AI applications.\\nManufacturer partnersADLINK,Advantech,Aetina,Dedicated Computing,Onyx,Prodrive TechnologiesandYuanare developing IGX-powered systems to serve the industrial and medical markets. These systems allow the benefits of digitalization to be realized during physical production.\\nAt the platform level, Omniverse connects the world’s leading 3D, simulation and generative AI providers. The open development platform, for example, lets teams build interoperability between their favorite applications — such as those from Adobe, Autodesk and Siemens.\\nA demo in the COMPUTEX keynote showcased Omniverse connected to various AI assistants, such as ChatGPT and Blender GPT, to simplify 3D workflows and Python-application development.NVIDIA Omniverse Cloud, a platform-as-a-service now available onMicrosoft Azure, gives enterprise customers access to the full-stack suite of Omniverse software applications, and NVIDIA OVX infrastructure, with the scale and security of Azure cloud services.\\nAnd at the application level, Isaac Sim allows companies to build and optimally deploy AI-based robots. Manufacturers can work with industrial automation companyREADY Roboticsto program their robot tasks in simulation before deploying in the real world. Simulation technology partners likeSoftServeandFS Studioshorten development timelines for customers by building digital twin-based simulations.\\nAlso at the application level, NVIDIA Metropolis includes a collection of factory-automation AI workflows that enable industrial solution providers and manufacturers to develop, deploy and manage customized quality-control solutions that save cost and improve production throughput. A large partner ecosystem — including ADLINK, Aetina, Deloitte, Quantiphi and Siemens — is helping to bring these solutions to market.\\nLearn more about Omniverse, Isaac Sim and Metropolis atCOMPUTEX.'},\n",
       " {'title': 'WPP Partners With NVIDIA to Build Generative AI-Enabled Content Engine for Digital Advertising',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/wpp-partners-with-nvidia-to-build-generative-ai-enabled-content-engine-for-digital-advertising',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA and WPP today announced they are developing a content engine that harnesses NVIDIA Omniverse™ and AI to enable creative teams to produce high-quality commercial content faster, more efficiently and at scale while staying fully aligned with a ...',\n",
       "  'content': '\\nCOMPUTEX—NVIDIA and WPP today announced they are developing a content engine that harnessesNVIDIA Omniverse™ and AI to enable creative teams to produce high-quality commercial content faster, more efficiently and at scale while staying fully aligned with a client’s brand.\\nThe new engine connects an ecosystem of 3D design, manufacturing and creative supply chain tools, including those from Adobe and Getty Images, letting WPP’s artists and designers integrate 3D content creation with generative AI. This enables their clients to reach consumers in highly personalized and engaging ways, while preserving the quality, accuracy and fidelity of their company’s brand identity, products and logos.\\nNVIDIA founder and CEO Jensen Huang unveiled the engine in ademoduring hisCOMPUTEX keynote address, illustrating how clients can work with teams at WPP, the world’s largest marketing services organization, to make large volumes of brand advertising content such as images or videos and experiences like 3D product configurators more tailored and immersive.\\n“The world’s industries, including the $700 billion digital advertising industry, are racing to realize the benefits of AI,” Huang said. “With Omniverse Cloud and generative AI tools, WPP is giving brands the ability to build and deploy product experiences and compelling content at a level of realism and scale never possible before.”\\n“Generative AI is changing the world of marketing at incredible speed,” said Mark Read, CEO of WPP. “Our partnership with NVIDIA gives WPP a unique competitive advantage through an AI solution that is available to clients nowhere else in the market today. This new technology will transform the way that brands create content for commercial use, and cements WPP’s position as the industry leader in the creative application of AI for the world’s top brands.”\\nAn Engine for CreativityThe new content engine has at its foundationOmniverse Cloud— a platform for connecting 3D tools, and developing and operating industrial digitalization applications. This allows WPP to seamlessly connect its supply chain of product-design data from software such as Adobe’sSubstance 3Dtools for 3D and immersive content creation, plus computer-aided design tools to create brand-accurate, photoreal digital twins of client products.\\nWPP uses responsibly trained generative AI tools and content from partners such as Adobe and Getty Images so its designers can create varied, high-fidelity images from text prompts and bring them into scenes. This includes Adobe Firefly, a family of creative generative AI models, and exclusive visual content from Getty Images created usingNVIDIA Picasso,a foundry for custom generative AI models for visual design.\\nWith the final scenes, creative teams can render large volumes of brand-accurate, 2D images and videos for classic advertising, or publish interactive 3D product configurators toNVIDIA Graphics Delivery Network, a worldwide, graphics streaming network, for consumers to experience on any web device.\\nIn addition to speed and efficiency, the new engine outperforms current methods, which require creatives to manually create hundreds of thousands of pieces of content using disparate data coming from disconnected tools and systems.\\nThe partnership with NVIDIA builds on WPP’s existing leadership position in emerging technologies and generative AI, with award-winning campaigns for major clients around the world.\\nThe new content engine will soon be available exclusively to WPP’s clients around the world.'},\n",
       " {'title': 'NVIDIA Collaborates With SoftBank Corp. to Power SoftBank’s Next-Gen Data Centers Using Grace Hopper Superchip for Generative AI and 5G/6G',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/softbank-telecom-data-centers-grace-hopper',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA and SoftBank Corp. today announced they are collaborating on a pioneering platform for generative AI and 5G/6G applications that is based on the NVIDIA GH200 Grace Hopper™ Superchip and which SoftBank plans to roll out at new, distributed AI ...',\n",
       "  'content': 'COMPUTEX—NVIDIA and SoftBank Corp. today announced they are collaborating on a pioneering platform for generative AI and 5G/6G applications that is based on theNVIDIA GH200 Grace Hopper™ Superchipand which SoftBank plans to roll out at new, distributed AI data centers across Japan.\\nPaving the way for the rapid, worldwide deployment of generative AI applications and services, SoftBank will build data centers that can, in collaboration with NVIDIA, host generative AI and wireless applications on a multi-tenant common server platform, which reduces costs and is more energy efficient.\\nThe platform will use the newNVIDIA MGX™ reference architecturewith Arm Neoverse-based GH200 Superchips and is expected to improve performance, scalability and resource utilization of application workloads.\\n“As we enter an era where society coexists with AI, the demand for data processing and electricity requirements will rapidly increase. SoftBank will provide next-generation social infrastructure to support the super-digitalized society in Japan,” said Junichi Miyakawa, president and CEO of SoftBank Corp. “Our collaboration with NVIDIA will help our infrastructure achieve a significantly higher performance with the utilization of AI, including optimization of the RAN. We expect it can also help us reduce energy consumption and create a network of interconnected data centers that can be used to share resources and host a range of generative AI applications.”\\n“Demand for accelerated computing and generative AI is driving a fundamental change in the architecture of data centers,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA Grace Hopper is a revolutionary computing platform designed to process and scale-out generative AI services. Like with other visionary initiatives in their past, SoftBank is leading the world to create a telecom network built to host generative AI services.”\\nThe new data centers will be more evenly distributed across its footprint than those used in the past, and handle both AI and 5G workloads. This will allow them to better operate at peak capacity with low latency and at substantially lower overall energy costs.\\nSoftBank is exploring creating 5G applications for autonomous driving, AI factories, augmented and virtual reality, computer vision and digital twins.\\nVirtual RAN With Record-Breaking ThroughputNVIDIA Grace Hopper andNVIDIA BlueField®-3 data processing unitswill accelerate the software-defined 5G vRAN, as well as generative AI applications, without bespoke hardware accelerators or specialized 5G CPUs. Additionally, theNVIDIA Spectrum Ethernet switchwith BlueField-3 will deliver a highly precise timing protocol for 5G.\\nThe solution achieves breakthrough 5G speed on an NVIDIA-accelerated 1U MGX-based server design, with industry-high throughput of 36Gbps downlink capacity, based on publicly available data on 5G accelerators. Operators have struggled to deliver such high downlink capacity using industry-standard servers.\\nNew Reference ArchitectureNVIDIA MGX is a modular reference architecture that enables system manufacturers and hyperscale customers to quickly and cost-effectively build over a hundred different server variations to suit a wide range of AI, HPC andNVIDIA Omniverse™ applications.\\nBy incorporatingNVIDIA Aerial™ softwarefor high-performance, software-defined, cloud-native 5G networks, these 5G base stations will allow operators to dynamically allocate compute resources, and achieve 2.5x power efficiency over competing products.\\n“The future of generative AI requires high-performance, energy-efficient compute like that of the Arm Neoverse-based Grace Hopper Superchip from NVIDIA,” said Rene Haas, CEO of Arm. “Combined with NVIDIA BlueField DPUs, Grace Hopper enables the new SoftBank 5G data centers to run the most demanding compute- and memory-intensive applications and bring exponential efficiency gains to software-defined 5G and AI on Arm.”'},\n",
       " {'title': 'NVIDIA Launches Accelerated Ethernet Platform for Hyperscale Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-launches-accelerated-ethernet-platform-for-hyperscale-generative-ai',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA today announced NVIDIA Spectrum-X™, an accelerated networking platform designed to improve the performance and efficiency of Ethernet-based AI clouds.',\n",
       "  'content': 'COMPUTEX—NVIDIA today announcedNVIDIA Spectrum-X™, an accelerated networking platform designed to improve the performance and efficiency of Ethernet-based AI clouds.\\nNVIDIA Spectrum-X is built on networking innovations powered by the tight coupling of theNVIDIA Spectrum-4 Ethernet switchwith theNVIDIA BlueField®-3 DPU, achieving 1.7x better overall AI performance and power efficiency, along with consistent, predictable performance in multi-tenant environments. Spectrum-X is supercharged by NVIDIA acceleration software and software development kits (SDKs), allowing developers to build software-defined, cloud-native AI applications.\\nThe delivery of end-to-end capabilities reduces run-times of massive transformer-based generative AI models. This allows network engineers, AI data scientists and cloud service providers to improve results and make informed decisions faster.\\nThe world’s top hyperscalers are adopting NVIDIA Spectrum-X, including industry-leading cloud innovators.\\nAs a blueprint and testbed for NVIDIA Spectrum-X reference designs, NVIDIA is building Israel-1, a hyperscale generative AI supercomputer to be deployed in its Israeli data center on Dell PowerEdge XE9680 servers based on theNVIDIA HGX™ H100 eight-GPU platform, BlueField-3 DPUs and Spectrum-4 switches.\\n“Transformative technologies such as generative AI are forcing every enterprise to push the boundaries of data center performance in pursuit of competitive advantage,” said Gilad Shainer, senior vice president of networking at NVIDIA. “NVIDIA Spectrum-X is a new class of Ethernet networking that removes barriers for next-generation AI workloads that have the potential to transform entire industries.”\\nThe NVIDIA Spectrum-X networking platform is highly versatile and can be used in various AI applications. It uses fully standards-based Ethernet and is interoperable with Ethernet-based stacks.\\nThe platform starts with Spectrum-4, the world’s first 51Tb/sec Ethernet switch built specifically for AI networks. Advanced RoCE extensions work in concert across the Spectrum-4 switches, BlueField-3 DPUs and NVIDIA LinkX optics to create an end-to-end 400GbE network that is optimized for AI clouds.\\nNVIDIA Spectrum-X enhances multi-tenancy with performance isolation to ensure tenants’ AI workloads perform optimally and consistently. It also offers better AI performance visibility, as it can identify performance bottlenecks and it features completely automated fabric validation.\\nAcceleration software driving Spectrum-X includes powerful NVIDIA SDKs such asCumulus Linux, pureSONiCandNetQ— which together enable the networking platform’s extreme performance. It also includes theNVIDIA DOCA™ software framework, which is at the heart of BlueField DPUs.\\nNVIDIA Spectrum-X enables unprecedented scale of 256 200Gb/s ports connected by a single switch, or 16,000 ports in a two-tier leaf-spine topology to support the growth and expansion of AI clouds while maintaining high levels of performance and minimizing network latency.\\nImmediate Ecosystem AdoptionCompanies offering NVIDIA Spectrum-X include Dell Technologies, Lenovo andSupermicro.\\nAvailabilityNVIDIASpectrum-X,Spectrum-4 switches,BlueField-3 DPUsand 400G LinkX optics are available now.\\nLearn more about NVIDIA Spectrum-X atCOMPUTEX.'},\n",
       " {'title': 'NVIDIA MGX Gives System Makers Modular Architecture to Meet Diverse Accelerated Computing Needs of World’s Data Centers',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-mgx-server-specification',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'To meet the diverse accelerated computing needs of the world’s data centers, NVIDIA today unveiled the NVIDIA MGX™ server specification, which provides system manufacturers with a modular reference architecture to quickly and cost-effectively build ...',\n",
       "  'content': 'COMPUTEX—To meet the diverse accelerated computing needs of the world’s data centers, NVIDIA today unveiled theNVIDIA MGX™server specification, which provides system manufacturers with a modular reference architecture to quickly and cost-effectively build more than 100 server variations to suit a wide range of AI, high performance computing and Omniverse applications.\\nASRock Rack, ASUS, GIGABYTE, Pegatron, QCT andSupermicrowill adopt MGX, which can slash development costs by up to three-quarters and reduce development time by two-thirds to just six months.\\n“Enterprises are seeking more accelerated computing options when architecting data centers that meet their specific business and application needs,” said Kaustubh Sanghani, vice president of GPU products at NVIDIA. “We created MGX to help organizations bootstrap enterprise AI, while saving them significant amounts of time and money.”\\nWith MGX, manufacturers start with a basic system architecture optimized for accelerated computing for their server chassis, and then select their GPU, DPU and CPU. Design variations can address unique workloads, such as HPC, data science, large language models, edge computing, graphics and video, enterprise AI, and design and simulation. Multiple tasks like AI training and 5G can be handled on a single machine, while upgrades to future hardware generations can be frictionless. MGX can also be easily integrated into cloud and enterprise data centers.\\nCollaboration With Industry LeadersQCT and Supermicro will be the first to market, with MGX designs appearing in August. Supermicro’s ARS-221GL-NR system, announced today, will include the NVIDIA Grace™ CPU Superchip, while QCT’s S74G-2U system, also announced today, will use theNVIDIA GH200 Grace Hopper Superchip.\\nAdditionally, SoftBank Corp. plans to roll out multiple hyperscale data centers across Japan and use MGX to dynamically allocate GPU resources between generative AI and 5G applications.\\n“As generative AI permeates across business and consumer lifestyles, building the right infrastructure for the right cost is one of network operators’ greatest challenges,” said Junichi Miyakawa, president and CEO at SoftBank Corp. “We expect that NVIDIA MGX can tackle such challenges and allow for multi-use AI, 5G and more depending on real-time workload requirements.”\\nDifferent Designs for Different NeedsData centers increasingly need to meet requirements for both growing compute capabilities and decreasing carbon emissions to combat climate change, while also keeping costs down.\\nAccelerated computing servers from NVIDIA have long provided exceptional computing performance and energy efficiency. Now, the modular design of MGX gives system manufacturers the ability to more effectively meet each customer’s unique budget, power delivery, thermal design and mechanical requirements.\\nMultiple Form Factors Offer Maximum FlexibilityMGX works with different form factors and is compatible with current and future generations of NVIDIA hardware, including:\\nMGX differs fromNVIDIA HGX™ in that it offers flexible, multi-generational compatibility with NVIDIA products to ensure that system builders can reuse existing designs and easily adopt next-generation products without expensive redesigns. In contrast, HGX is based on an NVLink®-connected, multi-GPU baseboard tailored to scale to create the ultimate in AI and HPC systems.\\nSoftware to Drive Acceleration FurtherIn addition to hardware, MGX is supported by NVIDIA’s full software stack, which enables developers and enterprises to build and accelerate AI, HPC and other applications. This includesNVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, which features over 100 frameworks, pretrained models and development tools to accelerate AI and data science for fully supported enterprise AI development and deployment.\\nMGX is compatible with the Open Compute Project and Electronic Industries Alliance server racks, for quick integration into enterprise and cloud data centers.\\nWatch NVIDIA founder and CEO Jensen Huang discuss the MGX server specification in his keynote address atCOMPUTEXand learn more in theNVIDIA MGX architecture white paper.'},\n",
       " {'title': 'NVIDIA Announces DGX GH200 AI Supercomputer',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-dgx-gh200-ai-supercomputer',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA today announced a new class of large-memory AI supercomputer — an NVIDIA DGX™ supercomputer powered by NVIDIA® GH200 Grace Hopper Superchips and the NVIDIA NVLink® Switch System — created to enable the development of giant, next-generation ...',\n",
       "  'content': '\\nCOMPUTEX—NVIDIA today announced a new class of large-memory AI supercomputer — anNVIDIA DGX™ supercomputer powered byNVIDIA®GH200 Grace Hopper Superchipsand theNVIDIA NVLink®Switch System— created to enable the development of giant, next-generation models for generative AI language applications, recommender systems and data analytics workloads.\\nTheNVIDIA DGX GH200’s massive shared memory space uses NVLink interconnect technology with the NVLink Switch System to combine 256 GH200 superchips, allowing them to perform as a single GPU. This provides 1 exaflop of performance and 144 terabytes of shared memory — nearly 500x more memory than the previous generation NVIDIA DGX A100, which was introduced in 2020.\\n“Generative AI, large language models and recommender systems are the digital engines of the modern economy,” said Jensen Huang, founder and CEO of NVIDIA. “DGX GH200 AI supercomputers integrate NVIDIA’s most advanced accelerated computing and networking technologies to expand the frontier of AI.”\\nNVIDIA NVLink Technology Expands AI at ScaleGH200 superchips eliminate the need for a traditional CPU-to-GPU PCIe connection by combining an Arm-basedNVIDIA Grace™ CPUwith anNVIDIA H100 Tensor Core GPUin the same package, usingNVIDIA NVLink-C2Cchip interconnects. This increases the bandwidth between GPU and CPU by 7x compared with the latest PCIe technology, slashes interconnect power consumption by more than 5x, and provides a 600GB Hopper architecture GPU building block for DGX GH200 supercomputers.\\nDGX GH200 is the first supercomputer to pair Grace Hopper Superchips with the NVIDIA NVLink Switch System, a new interconnect that enables all GPUs in a DGX GH200 system to work together as one. The previous-generation system only provided for eight GPUs to be combined with NVLink as one GPU without compromising performance.\\nThe DGX GH200 architecture provides 48x more NVLink bandwidth than the previous generation, delivering the power of a massive AI supercomputer with the simplicity of programming a single GPU.\\nA New Research Tool for AI PioneersGoogle Cloud, Meta and Microsoft are among the first expected to gain access to the DGX GH200 to explore its capabilities for generative AI workloads. NVIDIA also intends to provide the DGX GH200 design as a blueprint to cloud service providers and other hyperscalers so they can further customize it for their infrastructure.\\n“Building advanced generative models requires innovative approaches to AI infrastructure,” said Mark Lohmeyer, vice president of Compute at Google Cloud. “The new NVLink scale and shared memory of Grace Hopper Superchips address key bottlenecks in large-scale AI and we look forward to exploring its capabilities for Google Cloud and our generative AI initiatives.”\\n“As AI models grow larger, they need powerful infrastructure that can scale to meet increasing demands,” said Alexis Björlin, vice president of Infrastructure, AI Systems and Accelerated Platforms at Meta. “NVIDIA’s Grace Hopper design looks to provide researchers with the ability to explore new approaches to solve their greatest challenges.”\\n“Training large AI models is traditionally a resource- and time-intensive task,” said Girish Bablani, corporate vice president of Azure Infrastructure at Microsoft. “The potential for DGX GH200 to work with terabyte-sized datasets would allow developers to conduct advanced research at a larger scale and accelerated speeds.”\\nNew NVIDIA Helios Supercomputer to Advance Research and DevelopmentNVIDIA is building its own DGX GH200-based AI supercomputer to power the work of its researchers and development teams.\\nNamed NVIDIA Helios, the supercomputer will feature four DGX GH200 systems. Each will be interconnected withNVIDIA Quantum-2 InfiniBandnetworking to supercharge data throughput for training large AI models. Helios will include 1,024 Grace Hopper Superchips and is expected to come online by the end of the year.\\nFully Integrated and Purpose-Built for Giant ModelsDGX GH200 supercomputers include NVIDIA software to provide a turnkey, full-stack solution for the largest AI and data analytics workloads.NVIDIA Base Command™ software provides AI workflow management, enterprise-grade cluster management, libraries that accelerate compute, storage and network infrastructure, and system software optimized for running AI workloads.\\nAlso included isNVIDIA AI Enterprise, the software layer of the NVIDIA AI platform. It provides over 100 frameworks, pretrained models and development tools to streamline development and deployment of production AI including generative AI, computer vision, speech AI and more.\\nAvailabilityNVIDIA DGX GH200 supercomputers are expected to be available by the end of the year.\\nWatch Huang discuss NVIDIA DGX GH200 supercomputers during hiskeynote address at COMPUTEX.'},\n",
       " {'title': 'NVIDIA Grace Hopper Superchips Designed for Accelerated Generative AI Enter Full Production',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchips-designed-for-accelerated-generative-ai-enter-full-production',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA today announced that the NVIDIA® GH200 Grace Hopper Superchip is in full production, set to power systems coming online worldwide to run complex AI and HPC workloads.',\n",
       "  'content': 'COMPUTEX—NVIDIA today announced that theNVIDIA®GH200 Grace Hopper Superchipis in full production, set to power systems coming online worldwide to run complex AI and HPC workloads.\\nThe GH200-powered systems join more than 400 system configurations powered by\\xa0 different combinations of NVIDIA’s latest CPU, GPU and DPU architectures — includingNVIDIA Grace™,NVIDIA Hopper™,NVIDIA Ada LovelaceandNVIDIA BlueField®— created to help meet the surging demand for generative AI.\\nAt COMPUTEX, NVIDIA founder and CEO Jensen Huang revealed new systems, partners and additional details surrounding the GH200 Grace Hopper Superchip, which brings together the Arm-based NVIDIA Grace CPU and Hopper GPU architectures usingNVIDIA NVLink®-C2Cinterconnect technology. This delivers up to 900GB/s total bandwidth — 7x higher bandwidth than the standard PCIe Gen5 lanes found in traditional accelerated systems, providing incredible compute capability to address the most demanding generative AI and HPC applications.\\n“Generative AI is rapidly transforming businesses, unlocking new opportunities and accelerating discovery in healthcare, finance, business services and many more industries,” said Ian Buck, vice president of accelerated computing at NVIDIA. “With Grace Hopper Superchips in full production, manufacturers worldwide will soon provide the accelerated infrastructure enterprises need to build and deploy generative AI applications that leverage their unique proprietary data.”\\nGlobal hyperscalers and supercomputing centers in Europe and the U.S. are among several customers that will have access to GH200-powered systems.\\nHundreds of Accelerated Systems and Cloud InstancesTaiwan manufacturers are among the many system manufacturers worldwide bringing to market a wide variety of systems powered by different combinations of NVIDIA accelerators and processors. These include AAEON, Advantech, Aetina, ASRock Rack, ASUS, GIGABYTE, Ingrasys, Inventec, Pegatron, QCT, Tyan, Wistron and Wiwynn — all featured in Huang’sCOMPUTEX keynote addresstoday as key partners.\\nAdditionally, global server manufacturers Cisco, Dell Technologies, Hewlett Packard Enterprise, Lenovo, Supermicro and Eviden, an Atos company, offer a broad array of NVIDIA-accelerated systems.\\nCloud partners for NVIDIA H100 include Amazon Web Services (AWS), Cirrascale, CoreWeave, Google Cloud, Lambda, Microsoft Azure, Oracle Cloud Infrastructure, Paperspace and Vultr.\\nNVIDIA L4 GPUs are generally available on Google Cloud.\\nFull-Stack Computing Across Accelerated SystemsThe coming portfolio of systems accelerated by the NVIDIA Grace, Hopper and Ada Lovelace architectures provides broad support for the NVIDIA software stack, which includes NVIDIA AI, the NVIDIA Omniverse™ platform and NVIDIA RTX™ technology.\\nNVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, offers over 100 frameworks, pretrained models and development tools to streamline development and deployment of production AI, including generative AI, computer vision and speech AI.\\nTheNVIDIA Omniversedevelopment platform for building and operating metaverse applications enables individuals and teams to work across multiple software suites and collaborate in real time in a shared environment. The platform is based on theUniversal Scene Descriptionframework, an open, extensible 3D language for virtual worlds.\\nTheNVIDIA RTX platformfuses ray tracing, deep learning and rasterization to fundamentally transform the creative process for content creators and developers with support for industry-leading tools and APIs. Applications built on the RTX platform bring the power of real-time photorealistic rendering and AI-enhanced graphics, video and image processing to enable millions of designers and artists to create their best work.\\nAvailabilitySystems with GH200 Grace Hopper Superchips are expected to be available beginning later this year.\\nLearn more about the latest in AI, graphics and NVIDIA-powered systems atCOMPUTEX.'},\n",
       " {'title': 'NVIDIA ACE for Games Sparks Life Into Virtual Characters\\xa0With Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-ace-for-games-sparks-life-into-virtual-characters-with-generative-ai',\n",
       "  'publish_date': '2023-05-28',\n",
       "  'summary': 'NVIDIA today announced NVIDIA Avatar Cloud Engine (ACE) for Games, a custom AI model foundry service that transforms games by bringing intelligence to non-playable characters (NPCs) through AI-powered natural language interactions.',\n",
       "  'content': 'COMPUTEX—NVIDIA today announcedNVIDIA Avatar Cloud Engine (ACE)for Games, a custom AI model foundry service that transforms games by bringing intelligence to non-playable characters (NPCs) through AI-powered natural language interactions.\\nDevelopers of middleware, tools and games can use ACE for Games to build and deploy customized speech, conversation and animation AI models in their software and games.\\n“Generative AI has the potential to revolutionize the interactivity players can have with game characters and dramatically increase immersion in games,” said John Spitzer, vice president of developer and performance technology at NVIDIA. “Building on our expertise in AI and decades of experience working with game developers, NVIDIA is spearheading the use of generative AI in games.”\\nPioneering Generative AI in GamesBuilding onNVIDIA Omniverse™, ACE for Games delivers optimized AI foundation models for speech, conversation and character animation, including:\\nDevelopers can integrate the entire NVIDIA ACE for Games solution or use only the components they need.\\n‘Kairos’ Offers a Peek at the Future of GamesNVIDIA collaborated withConvai, anNVIDIA Inceptionstartup, to showcase how developers will soon be able to use NVIDIA ACE for Games to build NPCs. Convai, which is focused on developing cutting-edge conversational AI for virtual game worlds, integrated ACE modules into its end-to-end real-time avatar platform.\\nIn a demo, calledKairos,players interact with Jin, the purveyor of a ramen shop. Although he is an NPC, Jin replies to natural language queries realistically and consistent with the narrative backstory — all with the help of generative AI.Watch the demo, which is rendered in Unreal Engine 5 using the latest ray-tracing features andNVIDIA DLSS.\\n“With NVIDIA ACE for Games, Convai’s tools can achieve the latency and quality needed to make AI non-playable characters available to nearly every developer in a cost-efficient way,” said Purnendu Mukherjee, founder and CEO at Convai.\\nDeploy NVIDIA ACE for Gaming Models Locally or in the CloudThe neural networks enabling NVIDIA ACE for Games are optimized for different capabilities, with various size, performance and quality trade-offs. The ACE for Games foundry service will help developers fine-tune models for their games, then deploy viaNVIDIA DGX™ Cloud, GeForce RTX™ PCs or on premises for real-time inferencing.\\nThe models are optimized for latency — a critical requirement for immersive, responsive interactions in games.\\nGenerative AItoTransform the Gaming ExperienceGame developers and startups are already using NVIDIA generative AI technologies for their workflows.\\nLearn more about building onNVIDIA OmniverseusingNVIDIA ACEand other technology advancements atCOMPUTEX.'},\n",
       " {'title': 'NVIDIA Announces Upcoming Events for Financial Community',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-upcoming-events-for-financial-community-6892653',\n",
       "  'publish_date': '2023-05-25',\n",
       "  'summary': 'NVIDIA will present at the following events for the financial community:',\n",
       "  'content': \"NVIDIA will present at the following events for the financial community:\\nBofA Securities 2023 Global Technology ConferenceTuesday, June 6, 9:20 a.m. Pacific time\\nRosenblatt's 3rd Annual Technology Summit: The Age of AIWednesday, June 7, 9 a.m. Pacific time\\nNew Street Research’s Future of Transportation ConferenceMonday, June 12, 9 a.m. Pacific time\\nInterested parties can listen to the live audio webcast of NVIDIA presentations at financial events atinvestor.nvidia.com. Replays of the webcasts will be available for 90 days afterward.\"},\n",
       " {'title': 'NVIDIA Announces Financial Results for First Quarter Fiscal 2024',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2024',\n",
       "  'publish_date': '2023-05-24',\n",
       "  'summary': 'NVIDIA today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, down 13% from a year ago and up 19% from the previous quarter.',\n",
       "  'content': 'NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, down 13% from a year ago and up 19% from the previous quarter.\\nGAAP earnings per diluted share for the quarter were $0.82, up 28% from a year ago and up 44% from the previous quarter. Non-GAAP earnings per diluted share were $1.09, down 20% from a year ago and up 24% from the previous quarter.\\n“The computer industry is going through two simultaneous transitions — accelerated computing and generative AI,” said Jensen Huang, founder and CEO of NVIDIA.\\n“A trillion dollars of installed global data center infrastructure will transition from general purpose to accelerated computing as companies race to apply generative AI into every product, service and business process.\\n“Our entire data center family of products — H100, Grace CPU, Grace Hopper Superchip, NVLink, Quantum 400 InfiniBand and BlueField-3 DPU — is in production.\\xa0We are significantly increasing our supply to meet surging demand for them,” he said.\\nDuring the first quarter of fiscal 2024, NVIDIA returned to shareholders $99 million in cash dividends.\\nNVIDIA will pay its next quarterly cash dividend of $0.04 per share on June 30, 2023, to all shareholders of record on June 8, 2023.\\nQ1 Fiscal 2024 Summary\\n\\nOutlookNVIDIA’s outlook for the second quarter of fiscal 2024 is as follows:\\nHighlights\\nNVIDIA achieved progress since its previous earnings announcement in these areas:\\nData Center\\nGaming\\nProfessional Visualization\\nAutomotive\\nCFO CommentaryCommentary on the quarter by Colette Kress, NVIDIA’s executive vice president and chief financial officer, is available athttps://investor.nvidia.com/.\\nConference Call and Webcast InformationNVIDIA will conduct a conference call with analysts and investors to discuss its first quarter fiscal 2024 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA’s investor relations website,https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA’s conference call to discuss its financial results for its second quarter of fiscal 2024.\\nNon-GAAP MeasuresTo supplement NVIDIA’s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP income from operations, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA’s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude acquisition termination costs, stock-based compensation expense, acquisition-related and other costs, IP-related costs, legal settlement costs, other, losses from non-affiliated investments, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases of property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user’s overall understanding of the company’s historical financial performance. The presentation of the company’s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company’s financial results prepared in accordance with GAAP, and the company’s non-GAAP measures may be different from non-GAAP measures used by other companies.\\n\\n\\n\\n\\n'},\n",
       " {'title': 'NVIDIA Collaborates With Microsoft to Accelerate Enterprise-Ready Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-collaborates-with-microsoft-to-accelerate-enterprise-ready-generative-ai',\n",
       "  'publish_date': '2023-05-23',\n",
       "  'summary': 'NVIDIA today announced that it is integrating its NVIDIA AI Enterprise software into Microsoft’s Azure Machine Learning to help enterprises accelerate their AI initiatives.',\n",
       "  'content': 'Microsoft Build—NVIDIA today announced that it is integrating itsNVIDIA AI Enterprise softwareinto Microsoft’s Azure Machine Learning to help enterprises accelerate their AI initiatives.\\nThe integration will create a secure, enterprise-ready platform that enables Azure customers worldwide to quickly build, deploy and manage customized applications using the more than 100 NVIDIA AI frameworks and tools that come fully supported in NVIDIA AI Enterprise, the software layer of NVIDIA’s AI platform.\\n“With the coming wave of generative AI applications, enterprises are seeking secure accelerated tools and services that drive innovation,” said Manuvir Das, vice president of enterprise computing at NVIDIA. “The combination of NVIDIA AI Enterprise software and Azure Machine Learning will help enterprises speed up their AI initiatives with a straight, efficient path from development to production.”\\nNVIDIA AI Enterprise on Azure Machine Learning will also provide access to the highest-performance NVIDIA accelerated computing resources to speed the training and inference of AI models.\\n“Microsoft Azure Machine Learning users come to the platform expecting the highest performing, most secure development platform available,” said John Montgomery, corporate vice president of AI platform at Microsoft. “Our integration with NVIDIA AI Enterprise software allows us to meet that expectation, enabling enterprises and developers to easily access everything they need to train and deploy custom, secure large language models.”\\nWith Azure Machine Learning, developers can easily scale applications, from tests to massive deployments, while using Azure Machine Learning data encryption, access control and compliance certifications to meet security and compliance with their organizational policies requirements. NVIDIA AI Enterprise complements Azure Machine Learning with secure, production-ready AI capabilities and includes access to NVIDIA experts and support.\\nNVIDIA AI Enterprise includes over 100 frameworks, pretrained models and development tools, such asNVIDIA RAPIDS™ for accelerating data science workloads.NVIDIA Metropolisaccelerates vision AI model development, andNVIDIA Triton Inference Server™ supports enterprises in standardizing model deployment and execution.\\nAvailabilityThe NVIDIA AI Enterprise integration with Azure Machine Learning is available in alimited technical preview.\\nNVIDIA AI Enterprise is also available onAzure Marketplace, providing businesses worldwide with expanded options for fully secure and supported AI development and deployment.\\nAdditionally, theNVIDIA Omniverse Cloud™ platform-as-a-service is now available on Microsoft Azure as a private offer for enterprises. Omniverse Cloud provides developers and enterprises with a full-stack cloud environment to design, develop, deploy and manage industrialmetaverseapplications at scale.'},\n",
       " {'title': 'Dell Technologies and NVIDIA Introduce Project Helix for Secure, On-Premises Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/dell-technologies-and-nvidia-introduce-project-helix-for-secure-on-premises-generative-ai',\n",
       "  'publish_date': '2023-05-23',\n",
       "  'summary': 'Dell Technologies (NYSE: DELL) and NVIDIA (NASDAQ: NVDA) announce a joint initiative to make it easier for businesses to build and use generative AI models on premises to quickly and securely deliver better customer service, market intelligence, ...',\n",
       "  'content': '\\nProject Helix makes it easy for enterprises to build and deploy trustworthy generative AI\\nDell and NVIDIA infrastructure and software include built-in data security for on-premises generative AI applications\\nDell Technologies World—Dell Technologies (NYSE: DELL) and NVIDIA (NASDAQ: NVDA) announce a joint initiative to make it easier for businesses to build and use generative AI models on premises to quickly and securely deliver better customer service, market intelligence, enterprise search, and a range of other capabilities.\\nProject Helix will deliver a series of full-stack solutions with technical expertise and pre-built tools based on Dell and NVIDIA infrastructure and software. It includes a complete blueprint to help enterprises use their proprietary data and more easily deploy generative AI responsibly and accurately.\\n“Project Helix gives enterprises purpose-built AI models to more quickly and securely gain value from the immense amounts of data underused today,” said Jeff Clarke, vice chairman and co-chief operating officer, Dell Technologies. “With highly scalable and efficient infrastructure, enterprises can create a new wave of generative AI solutions that can reinvent their industries.”\\n“We are at a historic moment, when incredible advances in generative AI are intersecting with enterprise demand to do more with less,” said Jensen Huang, founder and CEO, NVIDIA. “With Dell Technologies, we’ve designed extremely scalable, highly efficient infrastructure that enables enterprises to transform their business by securely using their own data to build and operate generative AI applications.”\\nProject Helix simplifies enterprise generative AI deployments with a tested combination of optimized hardware and software, all available from Dell. This delivers the power to convert enterprise data into smarter, higher value outcomes, while maintaining data privacy. These solutions will help companies quickly deploy customized AI applications that drive trusted decisions from their own data to grow and scale their businesses.\\nBlueprint for On-Premises Generative AI\\nProject Helix will support the complete generative AI lifecycle – from infrastructure provisioning, modeling, training, fine-tuning, application development and deployment, to deploying inference and streamlining results. The validated designs help enterprises quickly build on-premises generative AI infrastructure at scale.\\nDell PowerEdge servers, such as thePowerEdge XE9680 and PowerEdge R760xa, are optimized to deliver performance for generative AI training and AI inferencing. The combination of Dell servers withNVIDIA® H100 Tensor Core GPUsandNVIDIA Networkingform the infrastructure backbone for these workloads. Customers can pair this infrastructure with resilient and scalable unstructured data storage, includingDell PowerScaleandDell ECS Enterprise Object Storage.\\nWith all Dell Validated Designs, customers can use the enterprise features of Dell server and storage software, with observability through Dell CloudIQ software.Project Helix also includesNVIDIA AI Enterprisesoftware to provide tools for customers as they move through the AI lifecycle. NVIDIA AI Enterprise includes more than 100 frameworks, pretrained models and development tools such as theNVIDIA NeMo™ large language model framework andNeMo Guardrailssoftware for building topical, safe and secure generative AI chatbots.\\nProject Helix includes security and privacy built into foundational components, such as Secured Component Verification. Protecting data on-premises reduces inherent risk and helps companies meet regulatory requirements.\\n“Companies are eager to explore the opportunities that generative AI tools enable for their organizations, but many aren’t sure how to get started,” said Bob O’Donnell, president and chief analyst, TECHnalysis Research. “By putting together a complete hardware and software solution from trusted brands,Dell Technologies and NVIDIA are offering enterprises a head start to building and refining AI-powered models that can leverage\\xa0their own company’s unique assets and create powerful, customized tools.”\\nAvailability\\nDell Validated Designs based on the Project Helix initiative will be available through traditional channels and APEX flexible consumption options, beginning in July 2023.\\nAdditional Resources\\nLearn moreabout AI at Dell Technologies.\\n'},\n",
       " {'title': 'NVIDIA Grace Drives Wave of New Energy-Efficient Arm Supercomputers',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-grace-drives-wave-of-new-energy-efficient-arm-supercomputers',\n",
       "  'publish_date': '2023-05-21',\n",
       "  'summary': 'NVIDIA today announced a supercomputer built on the NVIDIA Grace™ CPU Superchip, adding to a wave of new energy-efficient supercomputers based on the Arm® Neoverse™ platform.',\n",
       "  'content': 'ISC—NVIDIA today announced a supercomputer built on theNVIDIA Grace™ CPU Superchip, adding to a wave of newenergy-efficientsupercomputers based on the Arm®Neoverse™ platform.\\nThe Isambard 3 supercomputer to be based at the Bristol & Bath Science Park, in the U.K., will feature 384 Arm-based NVIDIA Grace CPU Superchips to power medical and scientific research, and is expected to deliver 6x the performance and energy efficiency of Isambard 2, placing it among Europe’s most energy-efficient systems.\\nIt will achieve about 2.7 petaflops of FP64 peak performance and consume less than 270 kilowatts of power, ranking it among the world’s three greenest non-accelerated supercomputers. The project is being led by the University of Bristol, as part of the research consortium the GW4 Alliance, together with the universities of Bath, Cardiff and Exeter.\\nIsambard 3 joins a growing wave of NVIDIA Arm-based supercomputers around the world, with additional systems that include GPUs being built at theSwiss National Supercomputing Centreand theLos Alamos National Laboratoryin the U.S.\\n“As climate change becomes an increasingly existential problem, it’s vital for computing to embrace energy-efficient technologies,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “NVIDIA is working alongside the Arm Neoverse ecosystem to provide a path forward for the creation of more energy-efficient supercomputing centers, driving important breakthroughs in scientific and industrial research.”\\n“From climate change to medicine, supercomputing is already enabling academic and industry leaders to take on some of the world’s biggest challenges,” said Mohamed Awad, senior vice president and general manager of infrastructure at Arm. “Expanding on important areas of research requires a level of performance and energy efficiency that Arm Neoverse uniquely delivers, and through our collaboration with NVIDIA, we’re proud to bring this to life in the Isambard 3 system.”\\nIsambard 3 to Supercharge Breakthroughs in Life Science, Medicine, MoreIsambard 3, to be built by Hewlett Packard Enterprise, will enable Europe’s scientific research community to supercharge breakthroughs in AI, life sciences, medical, astrophysics and biotech. It will be able to create detailed models of exceptionally complex structures, such as wind farms and fusion reactors, to help researchers unlock new advances in clean and green energy.\\nThe Arm-based NVIDIA Grace-powered system will also continue Isambard 2’s work of simulating molecular-level mechanisms to better understand Parkinson’s disease and find new treatments for osteoporosis and COVID-19. These compute-intensive applications benefit from the highest-performing cores, highest memory bandwidth and the optimal memory capacity per core provided by Grace.\\n“Isambard 3’s application performance efficiency of up to 6x its predecessor, which rivals many of the 50 fastest TOP500 systems, will provide scientists with a revolutionary new supercomputing platform to advance groundbreaking research,” said Simon McIntosh-Smith, principal investigator for the Isambard project and professor of HPC at the University of Bristol. “The Arm-based NVIDIA Grace CPU enables the breakthrough energy efficiency required to push the boundaries of scientific discovery and solve some of humanity’s most difficult challenges.”\\nOnce the system goes into production in spring 2024, Bristol expects the number of registered users to increase significantly beyond the current 800.\\nAccelerating Scientific DiscoveryNVIDIA’s accelerated computing platform comprisesNVIDIA H100 Tensor Core GPUs, NVIDIA Grace CPU Superchips,NVIDIA Grace Hopper™Superchips,NVIDIA Quantum-2 InfiniBand networkingand a full suite of NVIDIA AI and HPC software.\\nLearn more aboutNVIDIA’s accelerated computing platform for HPCatISC.'},\n",
       " {'title': 'NVIDIA, Jülich Supercomputing Centre and ParTec to Build Quantum Computing Lab',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-and-julich-supercomputing-centre-to-build-quantum-computing-lab',\n",
       "  'publish_date': '2023-05-21',\n",
       "  'summary': 'In a major step forward for the field of hybrid quantum-classical computing, NVIDIA today announced plans to build a new lab with the Jülich Supercomputing Centre (JSC) at Forschungszentrum Jülich (FZJ) that will feature a classical-quantum ...',\n",
       "  'content': 'ISC—In a major step forward for the field of hybrid quantum-classical computing, NVIDIA today announced plans to build a new lab with the Jülich Supercomputing Centre (JSC) at Forschungszentrum Jülich (FZJ) that will feature a classical-quantum supercomputer in partnership with ParTec AG, Munich, based on the NVIDIA®quantum computing platform.\\nFZJ, one of the largest interdisciplinary research centers in Europe, will host the lab as part of the Jülich UNified Infrastructure for Quantum Computing (JUNIQ) to run high-performance and low-latency quantum-classical computing workloads. JUNIQ is currently using the JUWELS booster system with 3,744NVIDIA A100 Tensor Core GPUsfor quantum computing simulations.\\nThe NVIDIA quantum computing platform enables tight integration of quantum and classical computing through the open-sourceCUDA®Quantumprogramming model and world-class simulation through theNVIDIA cuQuantumsoftware development kit.\\nJSC plans to use a phased approach to test the system and will use the NVIDIA CUDA Quantum programming model to program quantum processors and integrate them within the Jülich exascale modular supercomputing architecture.\\n“Unifying quantum computing and GPU supercomputing is a key part of enabling the scientific breakthroughs of tomorrow,” said Timothy Costa, director of HPC and quantum at NVIDIA. “NVIDIA’s collaboration with innovators such as the Jülich Supercomputing Centre and ParTec represents an important milestone for quantum-classical computing, making it accessible to countless new researchers and bringing the first quantum-accelerated supercomputer one step closer.”\\n“Hybrid quantum-classical systems are bringing quantum computing closer to reality to solve complex problems that classical computing alone is unable to do,” said Kristel Michielsen, head of the quantum information processing group at JSC. “By partnering with NVIDIA on the modular quantum computing laboratory, JSC’s researchers can make unprecedented strides in chemistry and material sciences that drive broader, transformative progress across science disciplines and industries.”\\n“ParTec has a long history as a driving force in developing the Modular Supercomputing Architecture with its world-class ParaStation Modulo software enabling hybrid, modular computing,” said Bernhard Frohwitter, CEO of ParTec AG. “Quantum computers will be an essential element of any future heterogeneous supercomputer. This development will open totally new possibilities.”\\nLearn more about the NVIDIA quantum computing platform atISC.'},\n",
       " {'title': 'NVIDIA, Rolls-Royce and Classiq Announce Quantum Computing Breakthrough for Computational Fluid Dynamics in Jet Engines',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-rolls-royce-and-classiq-announce-quantum-computing-breakthrough-for-computational-fluid-dynamics-in-jet-engines',\n",
       "  'publish_date': '2023-05-21',\n",
       "  'summary': 'NVIDIA, Rolls-Royce and Classiq, a quantum software company, today announced a quantum computing breakthrough aimed at bringing ever-increasing efficiency to jet engines.',\n",
       "  'content': 'ISC—NVIDIA, Rolls-Royce and Classiq, a quantum software company, today announced a quantum computing breakthrough aimed at bringing ever-increasing efficiency to jet engines.\\nUsing NVIDIA’s quantum computing platform, the companies have designed and simulated the world’s largest quantum computing circuit for computational fluid dynamics (CFD) — a circuit that measures 10 million layers deep with 39 qubits. By using GPUs, Rolls-Royce is preparing for a quantum future despite the limitations of today’s quantum computers, which only support circuits a few layers deep.\\nRolls-Royce plans to use the new circuit on its journey to quantum advantage in CFD for modeling the performance of jet engine designs in simulations that use both classical and quantum computing methods.\\nSuch breakthroughs are important to Rolls-Royce, a world leader in the aviation industry, in its work to build state-of-the-art jet engines that support the energy transition with more sustainable aviation.\\n“Designing jet engines, which are one of the most complicated devices on earth, is expensive and computationally challenging,” said Ian Buck, vice president of hyperscale and HPC at NVIDIA. “NVIDIA’s quantum computing platform gives Rolls-Royce a potential path to tackle these problems head-on while accelerating its research and future development of more efficient jet engines.”\\n“Applying both classical and quantum computing methods directly to the challenge of designing jet engines will help us accelerate our processes and perform more sophisticated calculations,” said Leigh Lapworth, computational science fellow at Rolls-Royce.\\nRolls-Royce and its partner, Israel-based Classiq, designed the circuit using Classiq’s synthesis engine and then simulated it usingNVIDIA®A100 Tensor Core GPUs. The speed and scale of the process was made possible byNVIDIA cuQuantum, a software development kit that includes optimized libraries and tools to speed up quantum computing workflows.\\nNVIDIA Grace Hopper Accelerates Quantum ComputingNVIDIA offers a unified computing platform for speeding breakthroughs in quantum research and development across disciplines. The NVIDIAGrace HopperSuperchip, which combines the groundbreaking performance of NVIDIA Hopper™ architecture GPUs with the versatility of theNVIDIA Grace CPUs, is ideally designed for giant-scale quantum simulation workloads.\\nAdditionally, its high-speed, low-latencyNVIDIA NVLink®-C2C interconnectmakes classical systems built with the superchip optimally suited to link to quantum processors, orQPUs. With a total 600GB of fast-accessible memory per node, Grace Hopper enables the quantum ecosystem to push these simulations to an even larger scale.\\nA strategic bridge to the quantum future, Grace Hopper powersDGX™ Quantum, the world’s first GPU-accelerated quantum computing system combining quantum computing with state-of-the-art classical computing. NVIDIA also provides developers withNVIDIA CUDA®Quantum, a robust open-source programming model that links GPUs and QPUs.\\nNVIDIA’s Quantum Ecosystem ExpandsA vast array of the world’s quantum computing research now runs on NVIDIA GPUs.\\nThe Jülich Supercomputer Centre, one of Europe’s largest facilities for quantum computing, also announced at ISC plans to build aquantum computing lab with NVIDIA, highlighting the growing importance of hybrid quantum-classical computing systems. The lab will also help developers advance the field of quantum computing with tools like CUDA Quantum.\\nAdditionally,ORCA Computingis the latest QPU builder to integrate CUDA Quantum, combining its photonic quantum computer with GPUs for machine learning.TensorFlow QuantumandTorchQuantum— two popular quantum machine learning frameworks — now also integrate cuQuantum. The majority of the world’s quantum computing software today supports GPU acceleration with the NVIDIA quantum platform.\\nLearn more about the NVIDIA quantum computing platform atISC.'},\n",
       " {'title': 'GeForce RTX 4060 Family Is Here: NVIDIA’s Revolutionary Ada Lovelace Architecture Comes to Core Gamers Everywhere, Starting at $299',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/geforce-rtx-4060-family-is-here-nvidias-revolutionary-ada-lovelace-architecture-comes-to-core-gamers-everywhere-starting-at-299',\n",
       "  'publish_date': '2023-05-18',\n",
       "  'summary': 'NVIDIA today announced the GeForce RTX™ 4060 family of GPUs, with two graphics cards that deliver all the advancements of the NVIDIA® Ada Lovelace architecture — including DLSS 3 neural rendering and third-generation ray-tracing technologies at high ...',\n",
       "  'content': 'NVIDIA today announced the GeForce RTX™ 4060 family of GPUs, with two graphics cards that deliver all the advancements of the NVIDIA®Ada Lovelace architecture — includingDLSS 3neural rendering and third-generation ray-tracing technologies at high frame rates — starting at just $299.\\nThe GeForce RTX 4060 Ti and GeForce RTX 4060 deliver unparalleled performance at fantastic value — bringing for the first time to the company’s popular 60-class twice the horsepower of the latest gaming consoles, including ray tracing for premium image quality on top games.\\n“The RTX 4060 family delivers PC gamers both great value and great performance at 1080p, whether they’re building a gaming battle box or an AI-assisted creation station,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “These GPUs deliver an incredible upgrade, starting at just $299, putting Ada Lovelace and DLSS 3 in the hands of millions more worldwide.”\\nDLSS Brings AI-Accelerated Performance to 300+ TitlesThe GeForce RTX 4060 family provides access to the300+ games and applications that now support DLSS, with eagerly anticipated titlesThe Lord of the Rings: GollumandDiablo IVto include DLSS 3. A DLSS 3 plug-in for Unreal Engine 5 is also coming soon.\\nDLSS 3 showcases the growing importance of AI in real-time games by creating new, high-quality frames for smoother gameplay. It massively increases performance in combination with DLSS Super Resolution, which uses AI to output higher-resolution frames from a lower-resolution input. Exceptional responsiveness is maintained throughNVIDIA Reflex, which reduces input lag.\\nThe Ultimate Graphics Cards for 1080p GamingThe GeForce RTX 4060 Ti is on average 2.6x faster than the RTX 2060 SUPER GPU and 1.7x faster than the GeForce RTX 3060 Ti GPU. For titles without frame generation, the RTX 4060 Ti is 1.6x faster than the RTX 2060 SUPER GPU.\\nThe RTX 4060 Ti’s memory subsystem features 32MB of L2 cache and 8GB or 16GB of ultra-high-speed GDDR6 memory. The RTX 4060 has 24MB of L2 cache with 8GB of GDDR6. The L2 cache reduces demands on the GPU’s memory interface, ultimately improving performance and power efficiency.\\nRay tracing performance has improved significantly from the previous generation, thanks toadvancementslikeShader Execution Reordering, cutting-edge Opacity Micromap and Displaced Micro-Mesh Engines. These innovations enable even the most demanding games to simultaneously implement multiple ray-tracing effects, and even full ray tracing, also known as path tracing, for unparalleled realism and immersion.\\nPerfect for Content CreatorsThe GeForce RTX 4060 family of GPUs comes backed by theNVIDIA Studio platform, which brings creators RTX acceleration and AI tools at a more accessible starting price. Serving livestreamers, video editors, 3D artists and others, the platform supercharges over 110 creative apps, provides lasting stability with NVIDIA Studio Drivers and includes a powerful suite of AI-powered Studio software, such asNVIDIA Omniverse™,CanvasandBroadcast.\\nCreators of many disciplines can benefit from new fourth-generation Tensor Cores, which provide a significant performance increase for AI tools compared with the last generation. Accelerated AI features allow creators to automate tedious tasks and apply advanced effects with ease.\\n3D modelers rendering high-resolution, ray-traced scenes can expect up to 45% faster performance than with the previous-generation GeForce RTX 3060 family. Adding AI-powered DLSS 3 — including within Omniverse, a hub for interconnecting existing 3D workflows to replace linear pipelines with live-sync creation and real-time collaboration — greatly accelerates the viewport in real-time 3D rendering applications, enabling a more fluid editing experience with full lighting, materials and physics.\\nBroadcasters can use the eighth-generation NVIDIA video encoder, called NVENC, with best-in-class AV1 hardware encoding, and benefit from 40% better encoding efficiency. Livestreams will appear as if bitrate was increased by 40% — a big boost in image quality for popular broadcast apps like OBS Studio. Broadcasters can also benefit from NVIDIA Broadcast and its set of AI effects that improve microphones and webcams, turning rooms into home studios.\\nVideo editors can benefit from a host of AI tools like auto-reframe, smart object selection and depth estimation, now available in top applications such as Adobe Premiere Pro and DaVinci Resolve, and export in AV1 for reduced file sizes.\\nGeForce RTX Offers a Graphics Card for Every Kind of UserWith this latest launch, the GeForce RTX 40 Series now has an option for every resolution and every user.\\nNVIDIA will celebrate the 4060 family’s launch with 100 streamers, and give away 460 of the new cards to members of the gaming community as part of its “Summer of RTX” event. Learn more on thesweepstakes webpage.\\nAvailabilityThe GeForce RTX 4060 Ti 8GB will be available starting Wednesday, May 24, at $399. The GeForce RTX 4060 Ti 16GB version will be available in July, starting at $499. GeForce RTX 4060 will also be available in July, starting at $299.\\nAn NVIDIA Founders Edition design of the GeForce RTX 4060 Ti 8GB will be available directly from NVIDIA.com and select retailers. Custom boards for the entire RTX 4060 family, including stock-clocked and factory-overclocked models, will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC, as well as from gaming system integrators and builders worldwide.'},\n",
       " {'title': 'ServiceNow and NVIDIA Announce Partnership to Build Generative AI Across Enterprise IT',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/servicenow-and-nvidia-announce-partnership-to-build-generative-ai-across-enterprise-it',\n",
       "  'publish_date': '2023-05-17',\n",
       "  'summary': 'ServiceNow and NVIDIA today announced a partnership to develop powerful, enterprise-grade generative AI capabilities that can transform business processes with faster, more intelligent workflow automation.',\n",
       "  'content': 'Knowledge 2023—ServiceNow and NVIDIA today announced a partnership to develop powerful, enterprise-grade generative AI capabilities that can transform business processes with faster, more intelligent workflow automation.\\nUsing NVIDIA software, services and accelerated infrastructure, ServiceNow is developing custom large language models trained on data specifically for itsServiceNow Platform, the intelligent platform for end-to-end digital transformation.\\nThis will expand ServiceNow’s already extensive AI functionality with new uses for generative AI across the enterprise — including for IT departments, customer service teams, employees and developers — to strengthen workflow automation and rapidly increase productivity.\\nServiceNow is also helping NVIDIA streamline its IT operations with these generative AI tools, using NVIDIA data to customizeNVIDIA®NeMo™models running on hybrid-cloud infrastructure consisting of NVIDIADGX™ Cloudand on-premisesNVIDIA DGX SuperPOD™AI supercomputers.\\n“IT is the nervous system of every modern enterprise in every industry,” said Jensen Huang, founder and CEO of NVIDIA. “Our collaboration to build super-specialized generative AI for enterprises will boost the capability and productivity of IT professionals worldwide using the ServiceNow platform.”\\n“As adoption of generative AI continues to accelerate, organizations are turning to trusted vendors with battle-tested, secure AI capabilities to boost productivity, gain a competitive edge, and keep data and IP secure,” said CJ Desai, president and chief operating officer of ServiceNow. “Together, NVIDIA and ServiceNow will help drive new levels of automation to fuel productivity and maximize business impact.\"\\nHarnessing Generative AI to Reshape Digital BusinessServiceNow and NVIDIA are exploring a number of generative AI use cases to simplify and improve productivity across the enterprise by providing high accuracy and higher value in IT.\\nThis includes developing intelligent virtual assistants and agents to help quickly resolve a broad range of user questions and support requests with purpose-built AI chatbots that use large language models and focus on defined IT tasks.\\nTo simplify the user experience, enterprises can customize chatbots with proprietary data to create a central generative AI resource that stays on topic while resolving many different requests.\\nThese generative AI use cases are also applicable to customer service agents, allowing for case prioritization with greater accuracy, saving time and improving outcomes. Customer service teams can use generative AI for automatic issue resolution, knowledge-base article generation based on customer case summaries, and chat summarization for faster hand-off, resolution and wrap-up.\\nIn addition, generative AI can improve the employee experience by helping identify growth opportunities. For example, delivering customized learning and development recommendations, like courses and mentors, based on natural language queries and information from an employee’s profile.\\nFull-Stack NVIDIA Generative AI Software and Infrastructure Fuel Rapid DevelopmentIn its generative AI research and development, ServiceNow is usingNVIDIA AI Foundationscloud services and theNVIDIA AI Enterprisesoftware platform, which includes the NVIDIA NeMo framework.\\nIncluded inNeMoare prompt tuning, supervised fine-tuning and knowledge retrieval tools to help developers build, customize and deploy language models for enterprise use cases.NeMo Guardrailssoftware is also included and enables developers to easily add topical, safety and security features for AI chatbots.'},\n",
       " {'title': 'NVIDIA Sets Conference Call for First-Quarter Financial Results',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-sets-conference-call-for-first-quarter-financial-results-6891941',\n",
       "  'publish_date': '2023-05-03',\n",
       "  'summary': 'NVIDIA will host a conference call on Wednesday, May 24, at 2 p.m. PT (5 p.m. ET) to discuss its financial results for the first quarter of fiscal year 2024, which ended April 30, 2023.',\n",
       "  'content': 'NVIDIA will host a conference call on Wednesday, May 24, at 2 p.m.\\xa0PT (5 p.m. ET) to discuss its financial results for the first quarter of fiscal year 2024, which ended April 30, 2023.\\nThe call will be webcast live (in listen-only mode) oninvestor.nvidia.com. The company’s prepared remarks will be followed by a question-and-answer session, which will be limited to questions from financial analysts and institutional investors.\\nAhead of the call, NVIDIA will provide written commentary on its first-quarter results from its CFO. This material will be posted toinvestor.nvidia.comimmediately after the company’s results are publicly announced at approximately 1:20 p.m. PT.\\nThe webcast will be recorded and available for replay until the company’s conference call to discuss financial results for its second quarter of fiscal year 2024.'},\n",
       " {'title': 'NVIDIA GeForce RTX 4070 Brings Power of Ada Lovelace Architecture and DLSS 3 to Millions More Gamers and Creators, Starting at $599',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-geforce-rtx-4070-brings-power-of-ada-lovelace-architecture-and-dlss-3-to-millions-more-gamers-and-creators-starting-at-599',\n",
       "  'publish_date': '2023-04-12',\n",
       "  'summary': 'NVIDIA today announced the GeForce RTX™ 4070 GPU, delivering all the advancements of the NVIDIA® Ada Lovelace architecture — including DLSS 3 neural rendering, real-time ray-tracing technologies and the ability to run most modern games at over 100 ...',\n",
       "  'content': 'NVIDIA today announced the GeForce RTX™ 4070 GPU, delivering all the advancements of the NVIDIA®Ada Lovelace architecture — including DLSS 3 neural rendering, real-time ray-tracing technologies and the ability to run most modern games at over 100 frames per second at 1440p resolution — starting at $599.\\nToday’s PC gamers increasingly want super-premium visual experiences, with 83% of GeForce RTX 40 Series desktop gamers enabling ray tracing and 79% turning on DLSS, a revolutionary breakthrough in AI-powered graphics that massively boosts performance, now in its third generation.\\n“Making sure games are ‘RTX ON’ with both ray tracing and DLSS has become a top priority for the majority of PC gamers,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “With prices starting at $599, the RTX 4070 is an amazing upgrade for gamers running on previous-generation GPUs.”\\nPlay at 100+ FPS at 1440p With DLSS 3The RTX 4070 allows the latest games such asA Plague Tale: Requiem,Dying Light 2,Microsoft Flight Simulator,Warhammer 40,000: Darktideand other demanding titles to run at over 100 frames per second, thanks to DLSS 3, as well as popular games such asBattlefield 2042,Call of Duty: Modern Warfare IIandMarvel’s Guardians of the Galaxythat support DLSS 2.\\nWith DLSS 3, AI-powered frame generation creates new, high-quality frames for smoother gameplay, rendering seven out of every eight pixels in a scene. Exceptional responsiveness is maintained throughNVIDIA Reflex, which reduces input lag between devices and monitors.\\nRay-tracing performance has also significantly improved, thanks to advancements like Shader Execution Reordering (SER), cutting-edge Opacity Micromap, and Displaced Micro-Mesh Engines. These innovations enable even the most demanding games to simultaneously implement multiple ray-tracing effects or even full ray tracing, also known as path tracing, for unparalleled realism and immersion.\\nThis potent combination of third-generation ray tracing and DLSS has resulted in a 16x leap in ray-tracing operations per pixel over the past five years. Adoption rates have soared, with over 400 RTX games and applications available, with adoption of DLSS 3 happening 7x faster than its predecessor.\\nClass-Leading Performance and FeaturesCompared to the RTX 2070 SUPER, the GeForce RTX 4070 is on average 2.6x faster with DLSS 3, and on average 1.4x faster than the GeForce RTX 3080 with DLSS 3.\\nDLSS 3 provides Ada Lovelace GPUs with a tremendous performance boost, but the GeForce RTX 4070 also excels in traditional games that do not include more advanced features such as ray tracing and DLSS.\\nIn these rasterized games, the GeForce RTX 4070 is on par with the GeForce RTX 3080 while running at nearly half the power — and offering an additional 2GB of memory. Additionally, the RTX 4070’s memory subsystem has been enhanced with 36MB of L2 cache and 12GB of ultra-high-speed GDDR6X memory.\\nPerfect for Content CreatorsThe GeForce RTX 4070 GPU comes backed by theNVIDIA Studio platform, which enables content creators to work faster and smarter with RTX acceleration and AI tools. Serving creators from livestreamers to video editors, the platform supercharges 100 creative apps, provides lasting stability with NVIDIA Studio Drivers and includes a powerful suite of Studio apps, such asNVIDIA Omniverse™,CanvasandBroadcast.\\n3D modelers rendering 4K scenes with the AI-powered DLSS 3 in NVIDIA Omniverse — a hub to interconnect existing 3D workflows, replacing linear pipelines with live-sync creation for real-time collaboration — can expect 2.8x faster performance than with last generation’s GeForce RTX 3070 Ti.\\nBroadcasters deploying the eighth-generation NVIDIA video encoder NVENC, with support for AV1, will enjoy 40% better efficiency. Livestreams will appear as if bitrate was increased by 40% — a big boost in image quality for popular broadcast apps like OBS Studio. AV1 is already supported in Discord with a full release coming to YouTube via OBS Studio soon.\\nVideo editors will see 20% faster video export speeds, thanks to the eighth-generation NVIDIA encoder on the GeForce RTX 4070, along with AV1 incorporated into top video editing apps.\\nAnd all creators can benefit from the new fourth-generation Tensor Cores for AI tools, providing a significant performance increase from the last generation.\\nWhere to BuyThe GeForce RTX 4070 will be available starting tomorrow at $599.\\nAn NVIDIA Founders Edition design will be available directly from NVIDIA.com and select retailers. Custom boards, including stock-clocked and factory-overclocked models, will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC.'},\n",
       " {'title': 'Ampere, NVIDIA Extend AICAN Gaming Platform Ecosystem',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/ampere-nvidia-extend-aican-gaming-platform-ecosystem',\n",
       "  'publish_date': '2023-03-30',\n",
       "  'summary': 'Ampere Computing and NVIDIA today announced the extension of the AICAN (pronounced “eye-CAN”) game-streaming platform to support more game environments and global partners using dense Arm + GPU platforms.',\n",
       "  'content': 'Ampere Computing and NVIDIA today announced the extension of the AICAN (pronounced “eye-CAN”) game-streaming platform to support more game environments and global partners using dense Arm + GPU platforms.\\nThe AICAN hardware streaming platform uses dual-socket Ampere®Altra®Max CPUs totaling an industry-leading 256 cores and up to four NVIDIA®A16 GPUs to enable the running, rendering and streaming of games to any remote streaming device.\\nThe platform, which launched last year in China to service Arm-compatible mobile games, has since expanded to support high-resolution games up to 4K and a growing list of titles, enabling a richer experience for gamers, game developers and cloud gaming service providers worldwide.\\nAICAN is now capable of supporting up to 160 concurrent sessions at 720p 30 frames per second, or up to 64 concurrent sessions at 1080p 60 fps. This ensures that the platform can handle the growing demand for high-resolution game streaming and accommodate more users simultaneously.\\nThe AICAN platform is optimized for use with NVIDIA GPUs that provide end-to-end rendering, in-line encoding and low-latency streaming. To support smooth, high-resolution streaming, excellent quality of service and dense concurrent users, AICAN demands faster networking options available throughNVIDIA ConnectX®SmartNICsandBlueField®DPUs. Dedicated engineering and quality-assurance teams make it easy for partners to quickly integrate the platform and keep up with the latest titles.\\nSoftware providers including Canonical have embraced the AICAN platform to offer out-of-the-box complete solutions through scalable products like Canonical Anbox Cloud, which enables service providers and platform suppliers to use the full breadth of available features using Android container hosting to provide dense cloud-native game streaming.\\nAICAN has also expanded to include new systems from manufacturers, such as Supermicro’s recently introduced Mt. Hamilton ARS-210M-NR Modular system, GIGABYTE G242-P35, Inspur Aoqin and Huaqin P6410. These platforms have been prequalified through the AICAN collaboration, offering service providers and end customers of game-streaming services the confidence to build, develop and play at scale.\\nAICAN represents a significant step forward for the gaming industry, providing a powerful, reliable and scalable platform for game developers, publishers and distributors. The platform’s expanded ecosystem, including full solution suppliers like Canonical and others, has made AICAN available worldwide through more server manufacturers than ever.'},\n",
       " {'title': 'NVIDIA and Google Cloud Deliver Powerful New Generative AI Platform, Built on the New L4 GPU and Vertex AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-and-google-cloud-deliver-powerful-new-generative-ai-platform-built-on-the-new-l4-gpu-and-vertex-ai',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'NVIDIA today announced Google Cloud is integrating the newly launched L4 GPU and Vertex AI to accelerate the work of companies building a rapidly expanding number of generative AI applications.',\n",
       "  'content': 'GTC—NVIDIA today announced Google Cloud is integrating the newly launched L4 GPU and Vertex AI to accelerate the work of companies building a rapidly expanding number of generative AI applications.\\nGoogle Cloud, with its announcement ofG2 virtual machinesavailable in private preview today, is the first cloud services provider to offerNVIDIA’s L4 Tensor Core GPU. Additionally, L4 GPUs will be available with optimized support on Vertex AI, which now supports building, tuning and deploying large generative AI models.\\nDevelopers can access the latest state-of-the-art technology available to help them get new applications up and running quickly and cost-efficiently. The NVIDIA L4 GPU is a universal GPU for every workload, with enhanced AI video capabilities that can deliver 120x more AI-powered video performance than CPUs, combined with 99% better energy efficiency.\\n“Surging interest in generative AI is inspiring a wave of companies to turn to cloud-based computing to support their business models,” said Jensen Huang, founder and CEO of NVIDIA. “We are working with Google Cloud to help ensure that the capabilities they require are easily available and able to help fuel the incredible new tools and applications they will create.”\\n“Generative AI represents a new era of computing — one that demands the speed, scalability and reliability we provide on Google Cloud,” said Amin Vahdat, vice president of Systems & Services Infrastructure at Google Cloud. “As our customers begin to explore the possibilities of generative AI, we’re proud to offer them NVIDIA’s latest L4 GPU innovation as part of our workload-optimized Compute Engine portfolio.”\\nHelping New Generative AI Applications Come to LifeGoogle Cloud provides the infrastructure for a wide variety of organizations offering generative AI applications, many of which are designed to help professionals do their work better and faster. Rapid inference is key to successfully running their applications.\\nGenerative AI is also driving a number of new apps that help people connect and have fun. WOMBO, which offers an AI-powered text to digital art app called Dream, has had early access to NVIDIA’s L4 inference platform on Google Cloud.\\n“WOMBO relies upon the latest AI technology for people to create immersive digital artwork from users’ prompts, letting them create high-quality, realistic art in any style with just an idea,” said Ben-Zion Benkhin, CEO at WOMBO. “NVIDIA’s L4 inference platform will enable us to offer a better, more efficient image-generation experience for users seeking to create and share unique artwork.”\\nDescript offers AI-powered editing features that let creators remove filler words, add captions and make social-media clips in a few clicks. Or they can use Descript’s generative-AI voice cloning to fix audio mistakes — even create entire voiceover tracks — just by typing.\\n“Descript uses NVIDIA TensorRT to optimize models to accelerate AI inferencing,” said Andrew Mason, CEO of Descript. “It allows users to replace their video backgrounds and enhance their speech to produce studio-quality content, without the studio.”\\nAvailabilityNVIDIA L4 GPUs are available in private preview on Google Cloud.Apply for access.\\nWatch Huang discuss the integration ofNVIDIA’s inference platform for generative AI into Google Cloudin his GTC keynote.'},\n",
       " {'title': 'Shutterstock Teams With NVIDIA to Build AI Foundation Models for Generative 3D Artist Tools',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/shutterstock-teams-with-nvidia-to-build-ai-foundation-models-for-generative-3d-artist-tools',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'Shutterstock, Inc., a leading creative platform for transformative brands and media companies, today announced it is partnering with NVIDIA to train custom 3D models to create generative 3D assets from simple text prompts.',\n",
       "  'content': 'GTC—Shutterstock, Inc. (NYSE: SSTK), a leading creative platform for transformative brands and media companies, today announced it is partnering with NVIDIA to train custom 3D models to create generative 3D assets from simple text prompts. Through a first of its kind collaboration, 3D models will be trained with Shutterstock assets usingNVIDIA Picasso generative AI cloud servicesto convert text into high-fidelity 3D content,\\xa0reducing creation time from hours to minutes.\\nWhen the models are introduced in the coming months on Shutterstock.com, the new NVIDIA-powered generative AI capabilities will be the latest addition toCreative Flow, an extensive toolkit designed to power the most seamless creative experience possible. The text-to-3D features will also be offered on Turbosquid.com and is planned to be introduced on theNVIDIA Omniverse™ platformfor building and operating 3D industrial metaverse applications.\\n“Our generative 3D partnership with NVIDIA will power the next generation of 3D contributor tools, greatly reducing the time it takes to create beautifully textured and structured 3D models,” said Shutterstock CEO Paul Hennessy. “This first of its kind partnership furthers our strategy of leveraging Shutterstock’s massive pool of metadata to bring new products, tools, and content to market. By combining our 3D content with NVIDIA’s foundation models, and utilizing our respective marketing and distribution platforms, we can capitalize on an extraordinarily large market opportunity.”\\nWith today’s professional software tools, building a high quality, detailed 3D model from scratch is often a challenging and time-consuming task for creators. In the case of a content being created for use as a digital twin, where absolute precision is required, the complexity of the job can take days or even longer depending on the specifics of the model. By creating custom models with the NVIDIA Picasso generative AI cloud service, Shutterstock will help 3D artists create object shapes, assist with object unwrapping, generate textures and materials and, for non-3D users, will even produce complete 3D models ready for use in a wide variety of applications and platforms.\\n”The transformative capabilities of generative AI make it possible for software makers and enterprises to build tools that use simple text prompts to create 3D assets for digital twins, simulation and design, saving artists enormous amounts of time and effort,” said NVIDIA Vice President of Developer Programs Greg Estes. “Training a custom Shutterstock model with the NVIDIA Picasso generative AI cloud services will give developers a tool that can automate much of the tedious work for artists, freeing them to spend more time exploring new concepts and refining their ideas.”\\nAs part of its responsible AI focus and in correlation with sales of the customized 3D models on Shutterstock’s platform, Shutterstock will compensate artists through itsContributor Fundfor the role their IP plays in training the generative technology.'},\n",
       " {'title': 'Adobe and NVIDIA Partner to Unlock the Power of Generative AI',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/adobe-and-nvidia-partner-to-unlock-the-power-of-generative-ai',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'Today, Adobe and NVIDIA, longstanding R&D partners, announced a new partnership to unlock the power of generative AI to further advance creative workflows.',\n",
       "  'content': 'GTC—Today, Adobe (Nasdaq:ADBE) and NVIDIA, longstanding R&D partners, announced a new partnership to unlock the power of generative AI to further advance creative workflows. Adobe and NVIDIA will co-develop a new generation of advanced generative AI models with a focus on deep integration into applications the world’s leading creators and marketers use.\\nSome of these models will be jointly developed and brought to market through Adobe’s Creative Cloud flagship products like Adobe Photoshop, Adobe Premiere Pro, and Adobe After Effects, as well as through the newNVIDIA Picassocloud service for broad reach to third-party developers. Priorities of the partnership include supporting commercial viability of the new technology and ensuring content transparency and Content Credentials powered by Adobe’sContent Authenticity Initiative.\\nPart of the NVIDIA AI Foundations cloud services for generative AIannounced today, NVIDIA Picasso lets users build and deploy generative AI-powered image, video, and 3D applications with advanced text-to-image, text-to-video, and text-to-3D capabilities to supercharge productivity for creativity, design, and digital simulation through simple cloud APIs.\\n“Adobe and NVIDIA have a long history of working closely together to advance the technology of creativity and marketing,” said Scott Belsky, Chief Strategy Officer and EVP, Design and Emerging Products, Adobe. “We’re thrilled to partner with them on ways that generative AI can give our customers more creative options, speed their work, and help scale content production.”\\n“Generative AI provides powerful new tools to empower unprecedented creativity,” said Greg Estes, VP, Corporate Marketing and Developer Programs, NVIDIA. “With NVIDIA Picasso and Adobe tools like Creative Cloud, we’ll be able to bring the transformational capabilities of generative AI to enterprises to help them explore more ideas to efficiently produce and scale incredible creative content and digital experiences.”\\nAdobe Firefly\\nEarlier today, Adobe introduced Adobe Firefly, Adobe’s new family of creative generative AI models, and unveiled the beta of its first model focused on the generation of images and text effects designed to be safe for commercial use. Firefly will bring even more precision, power, speed, and ease directly into Adobe Creative Cloud, Adobe Document Cloud, and Adobe Experience Cloud workflows that involve the creation and modification of content.\\nHosting some of Adobe Firefly’s models on NVIDIA Picasso will optimize performance and generate high-quality assets to meet customers’ expectations. (For more information on Firefly, including how it is trained and how it honors the role of creators, please seethis blog post.)\\nAdobe is\\xa0also\\xa0developing\\xa0new generative AI services to assist in the creation of video and 3D assets and to help marketers scale and personalize content for digital experiences through advancing end-to-end marketing workflows.\\nContent Authenticity Initiative\\nAdobe founded the Content Authenticity Initiative (CAI) to develop open industry standards for establishing attribution and Content Credentials. Through Content Credentials that CAI adds to content at the\\xa0point of capture, creation, edit, or generation, people will have a way to see when content was generated or modified using generative AI. Adobe and NVIDIA, along with 900 other members of the CAI, support Content Credentials so people can make informed decisions about the content they encounter.'},\n",
       " {'title': 'AWS and NVIDIA Collaborate on Next-Generation Infrastructure for Training Large Machine Learning Models and Building Generative AI Applications',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/aws-and-nvidia-collaborate-on-next-generation-infrastructure-for-training-large-machine-learning-models-and-building-generative-ai-applications',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': \"Amazon Web Services, Inc., an Amazon.com, Inc. company, and NVIDIA today announced a multi-part collaboration focused on building out the world's most scalable, on-demand AI infrastructure optimized for training increasingly complex large language ...\",\n",
       "  'content': 'GTC—Amazon Web Services, Inc. (AWS), an Amazon.com, Inc. company (NASDAQ: AMZN), and NVIDIA (NASDAQ: NVDA) today announced a multi-part collaboration focused on building out the world\\'s most scalable, on-demand artificial intelligence (AI) infrastructure optimized for training increasingly complex large language models (LLMs) and developing generative AI applications.\\nThe joint work features next-generation Amazon Elastic Compute Cloud (Amazon EC2) P5 instances powered by NVIDIA H100 Tensor Core GPUs and AWS’s state-of-the-art networking and scalability that will deliver up to 20 exaFLOPS of compute performance for building and training the largest deep learning models. P5 instances will be the first GPU-based instance to take advantage of AWS’s second-generation Elastic Fabric Adapter (EFA) networking, which provides 3,200 Gbps of low-latency, high bandwidth networking throughput, enabling customers to scale up to 20,000 H100 GPUs in EC2 UltraClusters for on-demand access to supercomputer-class performance for AI.\\n“AWS and NVIDIA have collaborated for more than 12 years to deliver large-scale, cost-effective GPU-based solutions on demand for various applications such as AI/ML, graphics, gaming, and HPC,” said Adam Selipsky, CEO at AWS. “AWS has unmatched experience delivering GPU-based instances that have pushed the scalability envelope with each successive generation, with many customers scaling machine learning training workloads to more than 10,000 GPUs today. With second-generation EFA, customers will be able to scale their P5 instances to over 20,000 NVIDIA H100 GPUs, bringing supercomputer capabilities on demand to customers ranging from startups to large enterprises.”\\n“Accelerated computing and AI have arrived, and just in time. Accelerated computing provides step-function speed-ups while driving down cost and power as enterprises strive to do more with less. Generative AI has awakened companies to reimagine their products and business models and to be the disruptor and not the disrupted,” said Jensen Huang, founder and CEO of NVIDIA. “AWS is a long-time partner and was the first cloud service provider to offer NVIDIA GPUs. We are thrilled to combine our expertise, scale, and reach to help customers harness accelerated computing and generative AI to engage the enormous opportunities ahead.”\\nNew Supercomputing ClustersNew P5 instances are built on more than a decade of collaboration between AWS and NVIDIA delivering the AI and HPC infrastructure and build on four previous collaborations across P2, P3, P3dn, and P4d(e) instances. P5 instances are the fifth generation of AWS offerings powered by NVIDIA GPUs and come almost 13 years after its initial deployment of NVIDIA GPUs, beginning with CG1 instances.\\nP5 instances are ideal for training and running inference for increasingly complex LLMs and computer vision models behind the most-demanding and compute-intensive generative AI applications, including question answering, code generation, video and image generation, speech recognition, and more.\\nSpecifically built for both enterprises and startups racing to bring AI-fueled innovation to market in a scalable and secure way, P5 instances feature eight NVIDIA H100 GPUs capable of 16 petaFLOPs of mixed-precision performance, 640 GB of high-bandwidth memory, and 3,200 Gbps networking connectivity (8x more than the previous generation) in a single EC2 instance. The increased performance of P5 instances accelerates the time-to-train machine learning (ML) models by up to 6x (reducing training time from days to hours), and the additional GPU memory helps customers train larger, more complex models. P5 instances are expected to lower the cost to train ML models by up to 40% over the previous generation, providing customers greater efficiency over less flexible cloud offerings or expensive on-premises systems.\\nAmazon EC2 P5 instances are deployed in hyperscale clusters called EC2 UltraClusters that are comprised of the highest performance compute, networking, and storage in the cloud. Each EC2 UltraCluster is one of the most powerful supercomputers in the world, enabling customers to run their most complex multi-node ML training and distributed HPC workloads. They feature petabit-scale non-blocking networking, powered by AWS EFA, a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at\\xa0 scale on AWS. EFA’s custom-built operating system (OS) bypass hardware interface and integration with NVIDIA GPUDirect RDMA enhances the performance of inter-instance communications by lowering latency and increasing bandwidth utilization, which is critical to scaling training of deep learning models across hundreds of P5 nodes. With P5 instances and EFA, ML applications can use NVIDIA Collective Communications Library (NCCL) to scale up to 20,000 H100 GPUs. As a result, customers get the application performance of on-premises HPC clusters with the on-demand elasticity and flexibility of AWS. On top of these cutting-edge computing capabilities, customers can use the industry’s broadest and deepest portfolio of services such as Amazon S3 for object storage, Amazon FSx for high-performance file systems, and Amazon SageMaker for building, training, and deploying deep learning applications. P5 instances will be available in the coming weeks in limited preview. To request access, visithttps://pages.awscloud.com/EC2-P5-Interest.html.\\nWith the new EC2 P5 instances, customers like Anthropic, Cohere, Hugging Face, Pinterest, and Stability AI will be able to build and train the largest ML models at scale. The collaboration through additional generations of EC2 instances will help startups, enterprises, and researchers seamlessly scale to meet their ML needs.\\nAnthropic builds reliable, interpretable, and steerable AI systems that will have many opportunities to create value commercially and for public benefit. “At Anthropic, we are working to build reliable, interpretable, and steerable AI systems. While the large, general AI systems of today can have significant benefits, they can also be unpredictable, unreliable, and opaque. Our goal is to make progress on these issues and deploy systems that people find useful,” said Tom Brown, co-founder of Anthropic. “Our organization is one of the few in the world that is building foundational models in deep learning research. These models are highly complex, and to develop and train these cutting-edge models, we need to distribute them efficiently across large clusters of GPUs. We are using Amazon EC2 P4 instances extensively today, and we are excited about the upcoming launch of P5 instances. We expect them to deliver substantial price-performance benefits over P4d instances, and they’ll be available at the massive scale required for building next-generation large language models and related products.”\\nCohere, a leading pioneer in language AI, empowers every developer and enterprise to build incredible products with world-leading natural language processing (NLP) technology while keeping their data private and secure. “Cohere leads the charge in helping every enterprise harness the power of language AI to explore, generate, search for, and act upon information in a natural and intuitive manner, deploying across multiple cloud platforms in the data environment that works best for each customer,” said Aidan Gomez, CEO at Cohere. “NVIDIA H100-powered Amazon EC2 P5 instances will unleash the ability of businesses to create, grow, and scale faster with its computing power combined with Cohere’s state-of-the-art LLM and generative AI capabilities.”\\nHugging Face is on a mission to democratize good machine learning. “As the fastest growing open source community for machine learning, we now provide over 150,000 pre-trained models and 25,000 datasets on our platform for NLP, computer vision, biology, reinforcement learning, and more,” said Julien Chaumond, CTO and co-founder at Hugging Face. “With significant advances in large language models and generative AI, we’re working with AWS to build and contribute the open source models of tomorrow. We’re looking forward to using Amazon EC2 P5 instances via Amazon SageMaker at scale in UltraClusters with EFA to accelerate the delivery of new foundation AI models for everyone.”\\nToday, more than 450 million people around the world use Pinterest as a visual inspiration platform to shop for products personalized to their taste, find ideas to do offline, and discover the most inspiring creators. “We use deep learning extensively across our platform for use-cases such as labeling and categorizing billions of photos that are uploaded to our platform, and visual search that provides our users the ability to go from inspiration to action,\" said David Chaiken, Chief Architect at Pinterest. \"We have built and deployed these use-cases by leveraging AWS GPU instances such as P3 and the latest P4d instances. We are looking forward to using Amazon EC2 P5 instances featuring H100 GPUs, EFA and Ultraclusters to accelerate our product development and bring new Empathetic AI-based experiences to our customers.”\\nAs the leader in multimodal, open-source AI model development and deployment, Stability AI collaborates with public- and private-sector partners to bring this next-generation infrastructure to a global audience. “At Stability AI, our goal is to maximize the accessibility of modern AI to inspire global creativity and innovation,” said Emad Mostaque, CEO of Stability AI. “We initially partnered with AWS in 2021 to build Stable Diffusion, a latent text-to-image diffusion model, using Amazon EC2 P4d instances that we employed at scale to accelerate model training time from months to weeks. As we work on our next generation of open-source generative AI models and expand into new modalities, we are excited to use Amazon EC2 P5 instances in second-generation EC2 UltraClusters. We expect P5 instances will further improve our model training time by up to 4x, enabling us to deliver breakthrough AI more quickly and at a lower cost.”\\nNew Server Designs for Scalable, Efficient AILeading up to the release of H100, NVIDIA and AWS engineering teams with expertise in thermal, electrical, and mechanical fields have collaborated to design servers to harness GPUs to deliver AI at scale, with a focus on energy efficiency in AWS infrastructure. GPUs are typically 20x more energy efficient than CPUs for certain AI workloads, with the H100 up to 300x more efficient for LLMs than CPUs.\\nThe joint work has included developing a system thermal design, integrated security and system management, security with the AWS Nitro hardware accelerated hypervisor, and NVIDIA GPUDirect™ optimizations for AWS custom-EFA network fabric.\\nBuilding on AWS and NVIDIA’s work focused on server optimization, the companies have begun collaborating on future server designs to increase the scaling efficiency with subsequent-generation system designs, cooling technologies, and network scalability.'},\n",
       " {'title': 'NVIDIA Expands Omniverse Cloud to Power Industrial Digitalization',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-expands-omniverse-cloud-to-power-industrial-digitalization',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'NVIDIA today announced that NVIDIA Omniverse™ Cloud, a platform-as-a-service that enables companies to unify digitalization across their core product and business processes, is now available to select enterprises.',\n",
       "  'content': 'GTC—NVIDIA today announced thatNVIDIA Omniverse™ Cloud, a platform-as-a-service that enables companies to unify digitalization across their core product and business processes, is now available to select enterprises.\\nNVIDIA hasselected Microsoft Azureas the first cloud service provider for Omniverse Cloud, giving enterprises access to the full-stack suite ofOmniversesoftware applications andNVIDIA OVX™ infrastructure, with the scale and security of Azure cloud services.\\nThe new subscription offering for Omniverse Cloud on Azure makes it easy for automotive teams — from design and engineering to smart factory to marketing — to digitalize their workflows, whetherconnecting 3D design toolsto accelerate vehicle development, building digital twins of automotive factories or running closed-loop simulations to test vehicle performance.\\n“Every manufactured object, from massive physical facilities to handheld consumer goods, will someday have a digital twin, created to build, operate and optimize the object,” said Jensen Huang, founder and CEO of NVIDIA. “NVIDIA Omniverse Cloud is the digital-to-physical operating system for industrial digitalization, arriving just in time for the trillions of dollars of new EV, battery and chip factories that are being built.”\\nOmniverse Cloud Delivers Ultimate Flexibility and ScalabilityThrough Omniverse Cloud, NVIDIA and Microsoft provide customers afull-stack cloud environmentand platform capabilities to design, develop, deploy and manage industrial metaverse applications. Omniverse Cloud also connects with the products that customers use from NVIDIA’s partner ecosystem.\\nPowered by NVIDIA OVX computing systems, Omniverse Cloud enables enterprise developers to customize foundation applications that are included with the platform-as-a-service:\\nAutomotive Makers Adopting Omniverse to Achieve DigitalizationOmniverse Cloud builds on the success of and experience with earlyOmniverse Enterprisecustomers, including BMW Group, Geely Lotus and Jaguar Land Rover.\\nBMW Group, which was the first carmaker to adopt Omniverse to build a fully digitalized smart factory, today announced that it willlaunch the current Omniverse Enterprise platform across its production network worldwide.\\n“NVIDIA Omniverse has given us an unprecedented ability to design, build and test complex manufacturing systems, which means we can plan and optimize a next-generation factory completely virtually before we build it in the physical world,” said Milan Nedeljković, board member for production at BMW AG. “This will save us time and resources, increase our sustainability efforts and improve operational efficiencies.”\\nGeely Lotus is adopting Omniverse Enterprise to builddigital twinsof factories to optimize manufacturing processes.\\nJaguar Land Rover is using Omniverse to generate synthetic data to train AI models, as well as validate perception and control algorithms through real-world driving scenarios. The vehicle maker has integrated Omniverse with its state-of-the-art vehicle dynamics models, virtual electronic control units, virtual automotive networks and cloud infrastructure, enabling teams to rapidly iterate software concepts.\\nAvailabilityOmniverse Cloud, powered byNVIDIA OVX computing systems, will be available starting with Microsoft Azure in the second half of the year.\\nOmniverse Cloud-based services will also be available from a network of leading service providers including WPP, the world’s largest marketing and communications company, which is building services to deliver sustainable and automated content supply chains for major brands worldwide.\\nTo learn more aboutNVIDIA Omniverse Cloud, watch theGTC keynote.Register free for GTCto attendOmniverse sessionswith NVIDIA and industry leaders.'},\n",
       " {'title': 'NVIDIA Redefines Workstations to Power New Era of AI, Design, Industrial Metaverse',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-redefines-workstations-to-power-new-era-of-ai-design-industrial-metaverse',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'NVIDIA today announced six new NVIDIA RTX™ Ada Lovelace architecture GPUs for laptops and desktops, which enable creators, engineers and data scientists to meet the demands of the new era of AI, design and the metaverse.',\n",
       "  'content': 'GTC—NVIDIA today announced six newNVIDIA RTX™ Ada Lovelace architectureGPUs for laptops and desktops, which enable creators, engineers and data scientists to meet the demands of the new era of AI, design and themetaverse.\\nUsing the new NVIDIA RTX GPUs withNVIDIA Omniverse™, a platform for building and operating metaverse applications, designers can simulate a concept before making it a reality, planners can visualize an entire factory before it is built and engineers can evaluate their designs in real time.\\nThe NVIDIA RTX 5000, RTX 4000, RTX 3500, RTX 3000 and RTX 2000 Ada Generationlaptop GPUsdeliver breakthrough performance and up to 2x the efficiency of the previous generation to tackle the most demanding workflows. For the desktop, theNVIDIA RTX 4000 Small Form Factor (SFF) Ada GenerationGPU features new RT Cores, Tensor Cores and CUDA®cores with 20GB of graphics memory to deliver incredible performance in a compact card.\\nThe latest NVIDIA RTX Ada Generation GPUs provide the accelerated computing power required for today’s highly collaborative content-creation, design and AI workflows. Anew generation of desktop workstationsthat combine high-end NVIDIA GPUs and smart networking with the latest Intel CPUs can drive innovation for the next wave of product and building designs, AI-augmented applications and industrial metaverse content.\\n“Running data-intensive applications like generative AI and real-time digital twins in the metaverse requires advanced computing power,” said Bob Pette, vice president of professional visualization at NVIDIA. “These new NVIDIA RTX GPUs provide the horsepower needed for creators, designers and engineers to accomplish their work from wherever they’re needed.”\\nCustomers Supporting NVIDIA RTX GPUsMany professionals are already using NVIDIA RTX GPUs to accelerate their workflows.\\n“General Motors is working to bring electric vehicles to more customers faster and at more price points, and virtual-reality tools are enabling us to test and make decisions at a quicker pace,” said Bryan Styles, director of immersive technology at General Motors. “The fidelity, frame rates and overall performance of the NVIDIA RTX 6000 Ada Generation GPU is well matched to the high demand we have from our product development workflows.”\\n“The NVIDIA RTX 6000 Ada Generation GPU is one step ahead of our evolving real-time pipeline for live-action filmmakers,” said Raphaël Goudin, virtual production supervisor at Versatile Media Ltd. “It’s adding efficiency, ease and, more importantly, creative power directly to filmmakers.”\\n“The NVIDIA RTX 6000 Ada Generation GPU is a game changer that lets us produce images quicker and accomplish things that previously weren’t even possible,” said Jon Ferguson, vice president of virtual design and construction at Layton Construction. “For the first time, we can start producing images with the primary question being ‘What would help this image?’ rather than ‘What can our computers handle?’”\\nNVIDIA RTX Laptops Deliver Creative Power to Professionals AnywhereNVIDIA’s new laptop GPUs deliver up to double the performance and power efficiency over the previous generation for mobile workstations.\\nThe new GPUs include the latest generations of NVIDIAMax-QandRTXtechnologies for optimal energy efficiency and photorealistic graphics, and are backed byNVIDIA Studiotechnologies for creators. Products with NVIDIA RTX GPUs benefit from RTX optimizations in over 110 creative apps, NVIDIA RTX Enterprise Drivers for the highest levels of stability and performance in creative apps, and exclusive AI-powered NVIDIA tools: Omniverse,CanvasandBroadcast.\\nProfessionals using these laptop GPUs can run advanced technologies likeDLSS 3to increase frame rates by up to 4x compared to the previous generation, andNVIDIA Omniverse Enterprisefor real-time collaboration and simulation.\\nNVIDIA RTX 4000 SFF Enables Enhanced Performance, ProductivityThe NVIDIA RTX 4000 SFF GPU offers a new level of performance and efficiency for mini-desktops,powering artists, designers and engineers who prefer small workstations.\\nBy delivering unprecedented rendering and visualization performance to compact workstations, the RTX 4000 SFF GPU enables users to enjoy a fluid experience in computer-aided design, graphic design, data analysis, AI applications and software development. Additionally, systems integrators developing specialized solutions — for example, in healthcare or large-scale displays — can benefit from the card’s combination of performance and compact size.\\n“The versatile NVIDIA RTX 4000 SFF Ada Generation GPU offers Genetec users performance increases of up to 80% and empowers them to decode, view and analyze more video streams,” said John Burger, product line manager for video appliances at Genetec. “As camera resolutions continue to increase and require additional resources to be decoded, the NVIDIA RTX 4000 SFF offers an ideal solution in a compact form factor for Genetec and its partners.”\\nNext-Generation RTX TechnologyThe new RTX desktop and laptop GPUs feature the Ada architecture’s latest technologies, including:\\nAvailabilityNext-generation desktop workstations featuring NVIDIA RTX GPUs will be available starting this month from global workstation manufacturing partners includingBOXX,HP Inc. andLenovo.\\nThe newNVIDIA RTX laptop GPUswill be available starting this month in mobile workstations from global workstation manufacturer partners. The new NVIDIA RTX 4000 SFF GPU will be available from global distribution partners such as Leadtek, PNY and Ryoyo Electro starting in April at an estimated price of $1,250 and from global workstation manufacturers later this year.\\nTo learn more aboutNVIDIA RTX, watch NVIDIA founder and CEOJensen Huang’s GTC 2023 keynote.Register free for GTCto attend sessions with NVIDIA and industry leaders.'},\n",
       " {'title': 'BYD, World’s Largest EV Maker, Partners With NVIDIA for Mainstream Software-Defined Vehicles Built on NVIDIA DRIVE',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/byd-worlds-largest-ev-maker-partners-with-nvidia-for-mainstream-software-defined-vehicles-built-on-nvidia-drive',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'NVIDIA today announced that BYD, the world’s leading manufacturer of new energy vehicles (NEVs), will extend its use of the NVIDIA DRIVE Orin™ centralized compute platform in a broader range of its NEVs.',\n",
       "  'content': '\\nGTC—NVIDIA today announced that BYD, the world’s leading manufacturer of new energy vehicles (NEVs), will extend its use of theNVIDIA DRIVE Orin™ centralized compute platform in a broader range of its NEVs. The enhanced partnership expands BYD’s use of DRIVE Orin across the multiple models in its next-generation Dynasty and Ocean series of vehicles, bringing safe and intelligent vehicles to market.\\n“NVIDIA DRIVE Orin has been enormously successful with global mobility leaders that are building the software-defined future,” said Rishi Dhall, vice president of automotive at NVIDIA. “Our ongoing collaboration with BYD is a testament to the industry’s confidence in DRIVE Orin as the centralized computer for today’s and tomorrow’s intelligent vehicles.”\\nNVIDIA and BYD share the belief that future cars will be programmable, evolving from being based on many embedded controllers to high-performance centralized computers — with functionalities delivered and enhanced through software updates over the life of the car. The compute horsepower from DRIVE Orin is critical for diverse, redundant sensor processing in real time, and provides automakers with the compute headroom to develop and support new software-driven services throughout the entire life of the vehicle.\\nSince entering production last year, DRIVE Orin — the highest performance automotive-grade processor on the market — has become the transportation industry’s AI engine of choice for the new generation of NEVs, robotaxis, shuttles and trucks. Designed to meet stringent industry safety standards, the scalable DRIVE Orin platform is capable of performing up to 254 trillion operations per second, enabling it to power AI cockpits, as well as automated driving functions — simultaneously running numerous deep neural networks to provide the ultimate safety and reliability.\\nBeyond selecting NVIDIA DRIVE Orin for its EV fleets, BYD announced earlier this year that it is working with NVIDIA to enhance the in-vehicle experience by bringing theNVIDIA GeForce NOW™ cloud gaming service to its vehicles.\\nWith industry-leading technologies such as the Blade Battery, DM-i super hybrid technology and the e-platform, BYD has sold over 3.7 million NEVs globally as of February 2023, while creating a better mobility experience for consumers.\\nTo learn more about the latest technology breakthroughs in automotive and other industries, watch NVIDIA founder and CEO Jensen Huang’sGTC keynote. Registerfree for GTC to attenda number of sessions with NVIDIA and today’s mobility leaders.'},\n",
       " {'title': 'NVIDIA Hopper GPUs Expand Reach as Demand for AI Grows',\n",
       "  'link': 'https://nvidianews.nvidia.com/news/nvidia-hopper-gpus-expand-reach-as-demand-for-ai-grows',\n",
       "  'publish_date': '2023-03-21',\n",
       "  'summary': 'NVIDIA and key partners today announced the availability of new products and services featuring the NVIDIA H100 Tensor Core GPU — the world’s most powerful GPU for AI — to address rapidly growing demand for generative AI training and inference.',\n",
       "  'content': 'GTC—NVIDIA and key partners today announced the availability of new products and services featuring theNVIDIA H100 Tensor Core GPU— the world’s most powerful GPU for AI — to address rapidly growing demand for generative AI training and inference.\\nOracle Cloud Infrastructure(OCI) announced the limited availability of new OCI Compute bare-metal GPU instances featuring H100 GPUs. Additionally, Amazon Web Services announced its forthcoming EC2 UltraClusters of Amazon EC2 P5 instances, which can scale in size up to 20,000 interconnected H100 GPUs. This followsMicrosoft Azure’s private preview announcement last week for its H100 virtual machine, ND H100 v5.\\nAdditionally, Meta has now deployed its H100-powered Grand Teton AI supercomputer internally for its AI production and research teams.\\nNVIDIA founder and CEO Jensen Huang announced during his GTC keynote today thatNVIDIA DGX™ H100AI supercomputers are in full production and will be coming soon to enterprises worldwide.\\n“Generative AI’s incredible potential is inspiring virtually every industry to reimagine its business strategies and the technology required to achieve them,” said Huang. “NVIDIA and our partners are moving fast to provide the world’s most powerful AI computing platform to those building applications that will fundamentally transform how we live, work and play.”\\nHopper Architecture Accelerates AIThe H100, based on the NVIDIA Hopper™ GPU computing architecture with its built-in Transformer Engine, is optimized for developing, training and deploying generative AI, large language models (LLMs) and recommender systems. This technology makes use of the H100’s FP8 precision and offers 9x faster AI training and up to 30x faster AI inference on LLMs versus the prior-generation A100. The H100 began shipping in the fall in individual and select board units from global manufacturers.\\nThe NVIDIA DGX H100 features eight H100 GPUs connected with NVIDIA NVLink® high-speed interconnects and integrated NVIDIA Quantum InfiniBand and Spectrum™ Ethernet networking. This platform provides 32 petaflops of compute performance at FP8 precision, with 2x faster networking than the prior generation, helping maximize energy efficiency in processing large AI workloads.\\nDGX H100 also features the complete NVIDIA AI software stack, enabling enterprises to seamlessly run and manage their AI workloads at scale. This offering includes the latest version ofNVIDIA AI Enterprise,announcedseparately today, as well asNVIDIA Base Command™, the operating system of the DGX data center, which coordinates AI training and operations across the NVIDIA DGX platform to simplify and streamline AI development.\\nAI Pioneers Adopt H100Several pioneers in generative AI are adopting H100 to accelerate their work:\\nDGX H100 Around the WorldInnovators worldwide are receiving the first wave of DGX H100 systems, including:\\nEcosystem Support“We are fully focused on AI innovation and AI-first products. NVIDIA H100 GPUs are state-of-the-art machine learning accelerators, giving us a significant competitive advantage within the machine learning industry for a wide variety of applications from model training to model inference.” —Eren Doğan, CEO of Anlatan\\n“AWS and NVIDIA have collaborated for more than 12 years to deliver large-scale, cost-effective GPU-based solutions on demand. AWS has unmatched experience delivering GPU-based instances that push the scalability envelope with each successive generation. Today, many customers scale machine learning training workloads to more than 10,000 GPUs. With second-generation EFA, customers can scale their P5 instances to more than 20,000 H100 GPUs, bringing on-demand supercomputer capabilities to any organization.” —David Brown, vice president of Amazon EC2 at AWS\\n“AI is at the core of everything we do at Google Cloud. NVIDIA H100 GPU and its powerful capabilities, coupled with our industry-leading AI products and services, will enable our customers to break new ground. We are excited to work with NVIDIA to accelerate enterprises in their effort to tap the power of generative AI.” —Amin Vahdat, vice president of Systems & Services Infrastructure at Google Cloud\\n“As we build new AI-powered experiences — like those based on generative AI — the underlying AI models become increasingly more sophisticated. Meta’s latest H100-powered Grand Teton AI supercomputer brings greater compute, memory capacity and bandwidth, further accelerating training and inference of Meta’s AI models, such as the open-sourced DLRM. As we move into the next computing platform, H100 also provides greater compute capabilities for researching Meta’s future content recommendation, generative AI and metaverse needs.” —Alexis Bjorlin, vice president of Infrastructure, AI Systems and Accelerated Platforms at Meta\\n“As the adoption of AI continues to accelerate, the way businesses operate and succeed is fundamentally changing. By bringing NVIDIA’s Hopper architecture to Microsoft Azure, we are able to offer unparalleled computing performance and functionality to enterprises looking to scale their AI capabilities.” —Scott Guthrie, executive vice president of the Cloud + AI group at Microsoft\\n“The computational power of the NVIDIA H100 Tensor Core GPU will be vital for enabling our efforts to push the frontier of AI training and inference. NVIDIA’s advancements unlock our research and alignment work on systems like GPT-4.” —Greg Brockman, president and co-founder of OpenAI\\n“OCI is bringing AI supercomputing capabilities at scale to thousands of organizations of all sizes. Our strong collaboration with NVIDIA is providing great value to customers, and we’re excited by the power of H100.” —Greg Pavlik, CTO and senior vice president at Oracle Cloud Infrastructure\\n“As the world’s leading open-source generative AI model company, Stability AI is committed to providing consumers and enterprises with the world’s best tools for multimodal creation. Harnessing the power of the NVIDIA H100 provides unprecedented computing power to fuel the creativity and research capabilities of the surging numbers of those looking to benefit from the transformative powers of generative AI. It will unlock our video, 3D and other models that uniquely benefit from the higher interconnect and advanced architecture for exabytes of data.” —Emad Mostaque, founder and CEO of\\xa0Stability AI\\n“Twelve Labs is excited to leverage Oracle Cloud Infrastructure Compute bare-metal instances powered by NVIDIA H100 GPUs to continue leading the effort in bringing video foundation models to market.” —Jae Lee, CEO of Twelve Labs\\nAvailabilityNVIDIA DGX H100 supercomputers are in full production and orderable from NVIDIA partners worldwide. Customers can trial DGX H100 today withNVIDIA DGX Cloud. Pricing is available from NVIDIA DGX partners worldwide.\\nNVIDIA H100 in the cloud is available now fromAzurein private preview,Oracle Cloud Infrastructurein limited availability, and generally available fromCirrascaleandCoreWeave. AWS announced H100 will be available in the coming weeks in limited preview. Google Cloud along with NVIDIA’s cloud partnersLambda,PaperspaceandVultrplan to offer H100.\\nServers and systems featuring NVIDIA H100 GPUs are available from leading server makers including Atos, Cisco, Dell Technologies, GIGABYTE, Hewlett Packard Enterprise, Lenovo and Supermicro.\\nPricing and other details are available directly from NVIDIA partners.\\nWatch Huang discuss the NVIDIA Hopper architecture in hisGTC keynote.'}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5fb1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "url_with_page = url+\"&page={page}\".format(page=page)\n",
    "url_with_page\n",
    "\n",
    "response = requests.get(url_with_page, headers={\"User-Agent\": \"Mozilla/5.0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d4a4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encuentra las páginas de navegación\n",
    "# y extrae los números de página\n",
    "def get_page_numbers(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"Extracts page numbers from the pagination section of the soup object.\n",
    "    This function looks for a div with class \"paging\" and retrieves all links\n",
    "    that contain the text \"for page\" in their title attribute. It returns a list\n",
    "    of page numbers as strings.\n",
    "    \n",
    "    Note: This function assumes that the pagination links are structured in a specific way, such as\n",
    "        for the NVIDIA News website, where the links contain the text \"for page\" in their title attribute.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of page numbers as strings extracted from the pagination links.\n",
    "    \"\"\"\n",
    "    paging_links = soup.find(\"div\", class_=\"paging\").find_all('a')\n",
    "    page_numbers = [a.text for a in paging_links if \"for page\" in a.get(\"title\", \"\")]\n",
    "    return page_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "559296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all(\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4b05e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article: Tag) -> dict:\n",
    "    \"\"\"Extracts information from an article Tag object.\n",
    "    This function retrieves the title, link, publish_date, and summary from a given article Tag.\n",
    "    It returns a dictionary containing these details.\n",
    "    Args:\n",
    "        article (Tag): A BeautifulSoup Tag object representing an article.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the article's title, link, publish_date, and summary.\n",
    "    Note: This function assumes that the article Tag has a specific structure, such as\n",
    "        containing an <h3> tag for the title, an <a> tag for the link, a <span> tag for the date,\n",
    "        and a <div> tag for the description.\n",
    "    \"\"\"\n",
    "\n",
    "    title = article.find(\n",
    "        \"h3\", class_=\"index-item-text-title\").get_text(strip=True)\n",
    "    link = BASE_URL + article.find(\"a\")[\"href\"]\n",
    "    date = article.find(\n",
    "        \"span\", class_=\"index-item-text-info-date\").get_text(strip=True)\n",
    "    date = dt.datetime.strptime(date, \"%B %d, %Y\").strftime('%Y-%m-%d')\n",
    "    summary = article.find(\n",
    "        \"div\", class_=\"index-item-text-description\").get_text(strip=True)\n",
    "\n",
    "    return {\"title\": title, \"link\": link, \"publish_date\": date, \"summary\": summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "364099c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_article = get_article_info(articles[0])\n",
    "link = c_article[\"link\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fde30f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(link, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "soup_article = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4621c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_page(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extracts the main content of a page from a BeautifulSoup object.\n",
    "    This function retrieves the text content from a div with class \"article-body\".\n",
    "    It returns the text as a string, with leading and trailing whitespace removed.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML content.\n",
    "\n",
    "    Returns:\n",
    "        str: The text content of the article body, stripped of leading and trailing whitespace.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        paragraphs = soup.find(\"div\", class_=\"article-body\").find_all(\"p\")\n",
    "        texto = [paragraphs[p].get_text(strip=True) for p in  range(len(paragraphs))]\n",
    "        return \"\\n\".join(texto)\n",
    "    except AttributeError:\n",
    "        return \"No content available\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d9aef23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0].get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA Brings Business Intelligence to Chatbots, Copilots and Summarization Tools With Enterprise-Grade Generative AI Microservice',\n",
       " 'link': 'https://nvidianews.nvidia.com/news/nemo-retriever-generative-ai-microservice',\n",
       " 'publish_date': '2023-11-28',\n",
       " 'summary': 'AWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.',\n",
       " 'text': 'AWS re:Invent—NVIDIA today announced a generative AI microservice that lets enterprises connect custom large language models to enterprise data to deliver highly accurate responses for their AI applications.\\nNVIDIA NeMo™ Retriever— a new offering in theNVIDIA NeMofamily of frameworks and tools for building, customizing and deploying generative AI models — helps organizations enhance their generative AI applications with enterprise-graderetrieval-augmented generation(RAG) capabilities.\\nAs a semantic-retrieval microservice, NeMo Retriever helps generative AI applications provide more accurate responses through NVIDIA-optimized algorithms. Developers using the microservice can connect their AI applications to business data wherever it resides across clouds and data centers. It adds NVIDIA-optimized RAG capabilities toAI foundriesand is part of theNVIDIA AI Enterprisesoftware platform, available inAWS Marketplace.\\nCadence, Dropbox, SAP and ServiceNow are among the pioneers working with NVIDIA to build production-ready RAG capabilities into their custom generative AI applications and services.\\n“Generative AI applications with RAG capabilities are the next killer app of the enterprise,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NeMo Retriever, developers can create customized generative AI chatbots, copilots and summarization tools that can access their business data to transform productivity with accurate and valuable generative AI intelligence.”\\nGlobal Leaders Enhance LLM Accuracy With NeMo RetrieverElectronic systems design leader Cadence serves companies across hyperscale computing, 5G communications, automotive, mobile, aerospace, consumer and healthcare markets. It is working with NVIDIA to develop RAG features for generative AI applications in industrial electronics design.\\n“Generative AI introduces innovative approaches to address customer needs, such as tools to uncover potential flaws early in the design process,” said Anirudh Devgan, president and CEO of Cadence. “Our researchers are working with NVIDIA to use NeMo Retriever to further boost the accuracy and relevance of generative AI applications to reveal issues and help customers get high-quality products to market faster.”\\nCracking the Code for Accurate Generative AI ApplicationsUnlike open-source RAG toolkits, NeMo Retriever supports production-ready generative AI with commercially viable models, API stability, security patches and enterprise support.\\nNVIDIA-optimized algorithms power the highest accuracy results in Retriever’s embedding models. The optimized embedding models capture relationships between words, enabling LLMs to process and analyze textual data.\\nUsing NeMo Retriever, enterprises can connect their LLMs to multiple data sources and knowledge bases, so that users can easily interact with data and receive accurate, up-to-date answers using simple, conversational prompts. Businesses using Retriever-powered applications can allow users to securely gain access to information spanning numerous data modalities, such as text, PDFs, images and videos.\\nEnterprises can use NeMo Retriever to achieve more accurate results with less training, speeding time to market and supporting energy efficiency in the development of generative AI applications.\\nReliable, Simple, Secure Deployment With NVIDIA AI EnterpriseCompanies can deploy NeMo Retriever-powered applications to run during inference on NVIDIA-accelerated computing on virtually any data center or cloud. NVIDIA AI Enterprise supports accelerated, high-performance inference with NVIDIA NeMo,NVIDIA Triton Inference Server™,NVIDIA TensorRT™,NVIDIA TensorRT-LLMand otherNVIDIA AIsoftware.\\nTo maximize inference performance, developers can run their models onNVIDIA GH200 Grace Hopper Superchips with TensorRT-LLM software.\\nAvailabilityDevelopers can sign up forearly access to NVIDIA NeMo Retriever.'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "c_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b4930139",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = art.find(\"span\", class_=\"index-item-text-info-date\").get_text(strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9fbadbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/news/infosys-and-nvidia'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art.find(\"a\")[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a139e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
